{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32e3c585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber\n",
      "  Obtaining dependency information for pdfplumber from https://files.pythonhosted.org/packages/53/5c/4523bfce8ba473b0e33931f9638f69c3573b3b72b0c63c73d779848d182f/pdfplumber-0.10.4-py3-none-any.whl.metadata\n",
      "  Downloading pdfplumber-0.10.4-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting pdfminer.six==20221105 (from pdfplumber)\n",
      "  Obtaining dependency information for pdfminer.six==20221105 from https://files.pythonhosted.org/packages/46/68/b3fb5f073bcd3df9143a3520289c147351bfa3c1b096d44081f38fd1c247/pdfminer.six-20221105-py3-none-any.whl.metadata\n",
      "  Downloading pdfminer.six-20221105-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: Pillow>=9.1 in /Users/nishithranjanbiswas/anaconda3/lib/python3.11/site-packages (from pdfplumber) (9.4.0)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Obtaining dependency information for pypdfium2>=4.18.0 from https://files.pythonhosted.org/packages/db/12/47a850cd8cc312d1a8e4a48c313a3e1d6b0d905fe2ae76548f9a6a8efb38/pypdfium2-4.27.0-py3-none-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading pypdfium2-4.27.0-py3-none-macosx_11_0_arm64.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /Users/nishithranjanbiswas/anaconda3/lib/python3.11/site-packages (from pdfminer.six==20221105->pdfplumber) (2.0.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /Users/nishithranjanbiswas/anaconda3/lib/python3.11/site-packages (from pdfminer.six==20221105->pdfplumber) (41.0.3)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/nishithranjanbiswas/anaconda3/lib/python3.11/site-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /Users/nishithranjanbiswas/anaconda3/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (2.21)\n",
      "Downloading pdfplumber-0.10.4-py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.7/54.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pypdfium2-4.27.0-py3-none-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
      "Successfully installed pdfminer.six-20221105 pdfplumber-0.10.4 pypdfium2-4.27.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e94ac19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "filename = 'docs/collobert11a.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "193859a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pdfplumber.open(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ebf6ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf.pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad65d7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Journal of Machine Learning Research 12 (2011) 2493-2537 Submitted 1/10; Revised 11/10; Published 8/11\n",
      "Natural Language Processing (Almost) from Scratch\n",
      "Ronan Collobert∗ RONAN@COLLOBERT.COM\n",
      "Jason Weston† JWESTON@GOOGLE.COM\n",
      "Le´on Bottou‡ LEON@BOTTOU.ORG\n",
      "Michael Karlen MICHAEL.KARLEN@GMAIL.COM\n",
      "Koray Kavukcuoglu§ KORAY@CS.NYU.EDU\n",
      "Pavel Kuksa¶ PKUKSA@CS.RUTGERS.EDU\n",
      "NEC Laboratories America\n",
      "4 Independence Way\n",
      "Princeton, NJ 08540\n",
      "Editor: Michael Collins\n",
      "Abstract\n",
      "We propose a unified neural network architecture and learning algorithm that can be applied to var-\n",
      "ious natural language processing tasks including part-of-speech tagging, chunking, named entity\n",
      "recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific\n",
      "engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made\n",
      "input features carefully optimized for each task, our system learns internal representations on the\n",
      "basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for\n",
      "building a freely available tagging system with good performance and minimal computational re-\n",
      "quirements.\n",
      "Keywords: natural language processing, neural networks\n",
      "1. Introduction\n",
      "Will a computer program ever be able to convert a piece of English text into a programmer friendly\n",
      "data structure that describes the meaning of the natural language text? Unfortunately, no consensus\n",
      "has emerged about the form or the existence of such a data structure. Until such fundamental\n",
      "Articial Intelligence problems are resolved, computer scientists must settle for the reduced objective\n",
      "of extracting simpler representations that describe limited aspects of the textual information.\n",
      "These simpler representations are often motivated by specific applications (for instance, bag-\n",
      "of-words variants for information retrieval), or by our belief that they capture something more gen-\n",
      "eral about natural language. They can describe syntactic information (e.g., part-of-speech tagging,\n",
      "chunking, and parsing) or semantic information (e.g., word-sense disambiguation, semantic role\n",
      "labeling, named entity extraction, and anaphora resolution). Text corpora have been manually an-\n",
      "notated with such data structures in order to compare the performance of various systems. The\n",
      "availability of standard benchmarks has stimulated research in Natural Language Processing (NLP)\n",
      "∗. Ronan Collobert is now with the Idiap Research Institute, Switzerland.\n",
      "†. Jason Weston is now with Google, New York, NY.\n",
      "‡. Le´on Bottou is now with Microsoft, Redmond, WA.\n",
      "§. Koray Kavukcuoglu is also with New York University, New York, NY.\n",
      "¶. Pavel Kuksa is also with Rutgers University, New Brunswick, NJ.\n",
      "(cid:13)c 2011 Ronan Collobert, Jason Weston, Le´on Bottou, Michael Karlen, Koray Kavukcuoglu and Pavel Kuksa.\n"
     ]
    }
   ],
   "source": [
    "text = pdf.pages[0].extract_text(x_tolerance=1)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f490ffbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLLOBERT, WESTON, BOTTOU, KARLEN, KAVUKCUOGLU AND KUKSA\n",
      "and effective systems have been designed for all these tasks. Such systems are often viewed as\n",
      "software components for constructing real-world NLP solutions.\n",
      "The overwhelming majority of these state-of-the-art systems address their single benchmark\n",
      "task by applying linear statistical models to ad-hoc features. In other words, the researchers them-\n",
      "selves discover intermediate representations by engineering task-specific features. These features\n",
      "are often derived from the output of preexisting systems, leading to complex runtime dependencies.\n",
      "This approach is effective because researchers leverage a large body of linguistic knowledge. On\n",
      "the other hand, there is a great temptation to optimize the performance of a system for a specific\n",
      "benchmark. Although such performance improvements can be very useful in practice, they teach us\n",
      "little about the means to progress toward the broader goals of natural language understanding and\n",
      "the elusive goals of Artificial Intelligence.\n",
      "In this contribution, we try to excel on multiple benchmarks while avoiding task-specific engi-\n",
      "neering. Instead we use a single learning system able to discover adequate internal representations.\n",
      "In fact we view the benchmarks as indirect measurements of the relevance of the internal represen-\n",
      "tations discovered by the learning procedure, and we posit that these intermediate representations\n",
      "are more general than any of the benchmarks. Our desire to avoid task-specific engineered features\n",
      "prevented us from using a large body of linguistic knowledge. Instead we reach good performance\n",
      "levels in most of the tasks by transferring intermediate representations discovered on large unlabeled\n",
      "data sets. We call this approach “almost from scratch” to emphasize the reduced (but still important)\n",
      "reliance on a priori NLP knowledge.\n",
      "The paper is organized as follows. Section 2 describes the benchmark tasks of interest. Sec-\n",
      "tion 3 describes the unified model and reports benchmark results obtained with supervised training.\n",
      "Section 4 leverages large unlabeled data sets (∼ 852 million words) to train the model on a language\n",
      "modeling task. Performance improvements are then demonstrated by transferring the unsupervised\n",
      "internal representations into the supervised benchmark models. Section 5 investigates multitask\n",
      "supervised training. Section 6 then evaluates how much further improvement can be achieved by\n",
      "incorporating standard NLP task-specific engineering into our systems. Drifting away from our ini-\n",
      "tial goals gives us the opportunity to construct an all-purpose tagger that is simultaneously accurate,\n",
      "practical, and fast. We then conclude with a short discussion section.\n",
      "2. The Benchmark Tasks\n",
      "In this section, we briefly introduce four standard NLP tasks on which we will benchmark our\n",
      "architectures within this paper: Part-Of-Speech tagging (POS), chunking (CHUNK), Named Entity\n",
      "Recognition (NER) and Semantic Role Labeling (SRL). For each of them, we consider a standard\n",
      "experimental setup and give an overview of state-of-the-art systems on this setup. The experimental\n",
      "setups are summarized in Table 1, while state-of-the-art systems are reported in Table 2.\n",
      "2.1 Part-Of-Speech Tagging\n",
      "POS aims at labeling each word with a unique tag that indicates its syntactic role, for example, plural\n",
      "noun, adverb, . . . A standard benchmark setup is described in detail by Toutanova et al. (2003).\n",
      "Sections 0–18 of Wall Street Journal (WSJ) data are used for training, while sections 19–21 are for\n",
      "validation and sections 22–24 for testing.\n",
      "The best POS classifiers are based on classifiers trained on windows of text, which are then fed\n",
      "to a bidirectional decoding algorithm during inference. Features include preceding and following\n",
      "2494\n"
     ]
    }
   ],
   "source": [
    "text = pdf.pages[1].extract_text(x_tolerance=1)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ee4e11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [page.extract_text(x_tolerance=1) for page in pdf.pages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "619745b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Journal of Machine Learning Research 12 (2011) 2493-2537 Submitted 1/10; Revised 11/10; Published 8/11\n",
      "Natural Language Processing (Almost) from Scratch\n",
      "Ronan Collobert∗ RONAN@COLLOBERT.COM\n",
      "Jason Weston† JWESTON@GOOGLE.COM\n",
      "Le´on Bottou‡ LEON@BOTTOU.ORG\n",
      "Michael Karlen MICHAEL.KARLEN@GMAIL.COM\n",
      "Koray Kavukcuoglu§ KORAY@CS.NYU.EDU\n",
      "Pavel Kuksa¶ PKUKSA@CS.RUTGERS.EDU\n",
      "NEC Laboratories America\n",
      "4 Independence Way\n",
      "Princeton, NJ 08540\n",
      "Editor: Michael Collins\n",
      "Abstract\n",
      "We propose a unified neural network architecture and learning algorithm that can be applied to var-\n",
      "ious natural language processing tasks including part-of-speech tagging, chunking, named entity\n",
      "recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific\n",
      "engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made\n",
      "input features carefully optimized for each task, our system learns internal representations on the\n",
      "basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for\n",
      "building a freely available tagging system with good performance and minimal computational re-\n",
      "quirements.\n",
      "Keywords: natural language processing, neural networks\n",
      "1. Introduction\n",
      "Will a computer program ever be able to convert a piece of English text into a programmer friendly\n",
      "data structure that describes the meaning of the natural language text? Unfortunately, no consensus\n",
      "has emerged about the form or the existence of such a data structure. Until such fundamental\n",
      "Articial Intelligence problems are resolved, computer scientists must settle for the reduced objective\n",
      "of extracting simpler representations that describe limited aspects of the textual information.\n",
      "These simpler representations are often motivated by specific applications (for instance, bag-\n",
      "of-words variants for information retrieval), or by our belief that they capture something more gen-\n",
      "eral about natural language. They can describe syntactic information (e.g., part-of-speech tagging,\n",
      "chunking, and parsing) or semantic information (e.g., word-sense disambiguation, semantic role\n",
      "labeling, named entity extraction, and anaphora resolution). Text corpora have been manually an-\n",
      "notated with such data structures in order to compare the performance of various systems. The\n",
      "availability of standard benchmarks has stimulated research in Natural Language Processing (NLP)\n",
      "∗. Ronan Collobert is now with the Idiap Research Institute, Switzerland.\n",
      "†. Jason Weston is now with Google, New York, NY.\n",
      "‡. Le´on Bottou is now with Microsoft, Redmond, WA.\n",
      "§. Koray Kavukcuoglu is also with New York University, New York, NY.\n",
      "¶. Pavel Kuksa is also with Rutgers University, New Brunswick, NJ.\n",
      "(cid:13)c 2011 Ronan Collobert, Jason Weston, Le´on Bottou, Michael Karlen, Koray Kavukcuoglu and Pavel Kuksa.  \n",
      "\n",
      "COLLOBERT, WESTON, BOTTOU, KARLEN, KAVUKCUOGLU AND KUKSA\n",
      "and effective systems have been designed for all these tasks. Such systems are often viewed as\n",
      "software components for constructing real-world NLP solutions.\n",
      "The overwhelming majority of these state-of-the-art systems address their single benchmark\n",
      "task by applying linear statistical models to ad-hoc features. In other words, the researchers them-\n",
      "selves discover intermediate representations by engineering task-specific features. These features\n",
      "are often derived from the output of preexisting systems, leading to complex runtime dependencies.\n",
      "This approach is effective because researchers leverage a large body of linguistic knowledge. On\n",
      "the other hand, there is a great temptation to optimize the performance of a system for a specific\n",
      "benchmark. Although such performance improvements can be very useful in practice, they teach us\n",
      "little about the means to progress toward the broader goals of natural language understanding and\n",
      "the elusive goals of Artificial Intelligence.\n",
      "In this contribution, we try to excel on multiple benchmarks while avoiding task-specific engi-\n",
      "neering. Instead we use a single learning system able to discover adequate internal representations.\n",
      "In fact we view the benchmarks as indirect measurements of the relevance of the internal represen-\n",
      "tations discovered by the learning procedure, and we posit that these intermediate representations\n",
      "are more general than any of the benchmarks. Our desire to avoid task-specific engineered features\n",
      "prevented us from using a large body of linguistic knowledge. Instead we reach good performance\n",
      "levels in most of the tasks by transferring intermediate representations discovered on large unlabeled\n",
      "data sets. We call this approach “almost from scratch” to emphasize the reduced (but still important)\n",
      "reliance on a priori NLP knowledge.\n",
      "The paper is organized as follows. Section 2 describes the benchmark tasks of interest. Sec-\n",
      "tion 3 describes the unified model and reports benchmark results obtained with supervised training.\n",
      "Section 4 leverages large unlabeled data sets (∼ 852 million words) to train the model on a language\n",
      "modeling task. Performance improvements are then demonstrated by transferring the unsupervised\n",
      "internal representations into the supervised benchmark models. Section 5 investigates multitask\n",
      "supervised training. Section 6 then evaluates how much further improvement can be achieved by\n",
      "incorporating standard NLP task-specific engineering into our systems. Drifting away from our ini-\n",
      "tial goals gives us the opportunity to construct an all-purpose tagger that is simultaneously accurate,\n",
      "practical, and fast. We then conclude with a short discussion section.\n",
      "2. The Benchmark Tasks\n",
      "In this section, we briefly introduce four standard NLP tasks on which we will benchmark our\n",
      "architectures within this paper: Part-Of-Speech tagging (POS), chunking (CHUNK), Named Entity\n",
      "Recognition (NER) and Semantic Role Labeling (SRL). For each of them, we consider a standard\n",
      "experimental setup and give an overview of state-of-the-art systems on this setup. The experimental\n",
      "setups are summarized in Table 1, while state-of-the-art systems are reported in Table 2.\n",
      "2.1 Part-Of-Speech Tagging\n",
      "POS aims at labeling each word with a unique tag that indicates its syntactic role, for example, plural\n",
      "noun, adverb, . . . A standard benchmark setup is described in detail by Toutanova et al. (2003).\n",
      "Sections 0–18 of Wall Street Journal (WSJ) data are used for training, while sections 19–21 are for\n",
      "validation and sections 22–24 for testing.\n",
      "The best POS classifiers are based on classifiers trained on windows of text, which are then fed\n",
      "to a bidirectional decoding algorithm during inference. Features include preceding and following\n",
      "2494  \n",
      "\n",
      "NATURAL LANGUAGE PROCESSING (ALMOST) FROM SCRATCH\n",
      "Task Benchmark Data set Training set Test set\n",
      "(#tokens) (#tokens) (#tags)\n",
      "POS Toutanova et al. (2003) WSJ sections 0–18 sections 22–24 ( 45 )\n",
      "( 912,344 ) ( 129,654 )\n",
      "Chunking CoNLL 2000 WSJ sections 15–18 section 20 ( 42 )\n",
      "( 211,727 ) ( 47,377 ) (IOBES)\n",
      "NER CoNLL 2003 Reuters “eng.train” “eng.testb” ( 17 )\n",
      "( 203,621 ) ( 46,435 ) (IOBES)\n",
      "SRL CoNLL 2005 WSJ sections 2–21 section 23 ( 186 )\n",
      "( 950,028 ) + 3 Brown sections (IOBES)\n",
      "( 63,843 )\n",
      "Table 1: Experimental setup: for each task, we report the standard benchmark we used, the data set\n",
      "it relates to, as well as training and test information.\n",
      "System Accuracy System F1\n",
      "Shen et al. (2007) 97.33% Shen and Sarkar (2005) 95.23%\n",
      "Toutanova et al. (2003) 97.24% Sha and Pereira (2003) 94.29%\n",
      "Gime´nez and Ma`rquez (2004) 97.16% Kudo and Matsumoto (2001) 93.91%\n",
      "(a) POS (b) CHUNK\n",
      "System F1 System F1\n",
      "Ando and Zhang (2005) 89.31% Koomen et al. (2005) 77.92%\n",
      "Florian et al. (2003) 88.76% Pradhan et al. (2005) 77.30%\n",
      "Kudo and Matsumoto (2001) 88.31% Haghighi et al. (2005) 77.04%\n",
      "(c) NER (d) SRL\n",
      "Table 2: State-of-the-art systems on four NLP tasks. Performance is reported in per-word accuracy\n",
      "for POS, and F1 score for CHUNK, NER and SRL. Systems in bold will be referred as\n",
      "benchmark systems in the rest of the paper (see Section 2.6).\n",
      "tag context as well as multiple words (bigrams, trigrams. . . ) context, and handcrafted features to\n",
      "deal with unknown words. Toutanova et al. (2003), who use maximum entropy classifiers and\n",
      "inference in a bidirectional dependency network (Heckerman et al., 2001), reach 97.24% per-word\n",
      "accuracy. Gime´nez and Ma`rquez (2004) proposed a SVM approach also trained on text windows,\n",
      "with bidirectional inference achieved with two Viterbi decoders (left-to-right and right-to-left). They\n",
      "obtained 97.16% per-word accuracy. More recently, Shen et al. (2007) pushed the state-of-the-art up\n",
      "to 97.33%, with a new learning algorithm they call guided learning, also for bidirectional sequence\n",
      "classification.\n",
      "2495  \n",
      "\n",
      "COLLOBERT, WESTON, BOTTOU, KARLEN, KAVUKCUOGLU AND KUKSA\n",
      "2.2 Chunking\n",
      "Also called shallow parsing, chunking aims at labeling segments of a sentence with syntactic con-\n",
      "stituents such as noun or verb phrases (NP or VP). Each word is assigned only one unique tag, often\n",
      "encoded as a begin-chunk (e.g., B-NP) or inside-chunk tag (e.g., I-NP). Chunking is often evaluated\n",
      "using the CoNLL 2000 shared task.1 Sections 15–18 of WSJ data are used for training and section\n",
      "20 for testing. Validation is achieved by splitting the training set.\n",
      "Kudoh and Matsumoto (2000) won the CoNLL 2000 challenge on chunking with a F1-score\n",
      "of 93.48%. Their system was based on Support Vector Machines (SVMs). Each SVM was trained\n",
      "in a pairwise classification manner, and fed with a window around the word of interest containing\n",
      "POS and words as features, as well as surrounding tags. They perform dynamic programming at\n",
      "test time. Later, they improved their results up to 93.91% (Kudo and Matsumoto, 2001) using an\n",
      "ensemble of classifiers trained with different tagging conventions (see Section 3.3.3).\n",
      "Since then, a certain number of systems based on second-order random fields were reported\n",
      "(Sha and Pereira, 2003; McDonald et al., 2005; Sun et al., 2008), all reporting around 94.3% F1\n",
      "score. These systems use features composed of words, POS tags, and tags.\n",
      "More recently, Shen and Sarkar (2005) obtained 95.23% using a voting classifier scheme, where\n",
      "each classifier is trained on different tag representations2 (IOB, IOE, . . . ). They use POS features\n",
      "coming from an external tagger, as well carefully hand-crafted specialization features which again\n",
      "change the data representation by concatenating some (carefully chosen) chunk tags or some words\n",
      "with their POS representation. They then build trigrams over these features, which are finally passed\n",
      "through a Viterbi decoder a test time.\n",
      "2.3 Named Entity Recognition\n",
      "NER labels atomic elements in the sentence into categories such as “PERSON” or “LOCATION”.\n",
      "As in the chunking task, each word is assigned a tag prefixed by an indicator of the beginning or the\n",
      "inside of an entity. The CoNLL 2003 setup3 is a NER benchmark data set based on Reuters data.\n",
      "The contest provides training, validation and testing sets.\n",
      "Florian et al. (2003) presented the best system at the NER CoNLL 2003 challenge, with 88.76%\n",
      "F1 score. They used a combination of various machine-learning classifiers. Features they picked\n",
      "included words, POS tags, CHUNK tags, prefixes and suffixes, a large gazetteer (not provided by\n",
      "the challenge), as well as the output of two other NER classifiers trained on richer data sets. Chieu\n",
      "(2003), the second best performer of CoNLL 2003 (88.31% F1), also used an external gazetteer\n",
      "(their performance goes down to 86.84% with no gazetteer) and several hand-chosen features.\n",
      "Later, Ando and Zhang (2005) reached 89.31% F1 with a semi-supervised approach. They\n",
      "trained jointly a linear model on NER with a linear model on two auxiliary unsupervised tasks.\n",
      "They also performed Viterbi decoding at test time. The unlabeled corpus was 27M words taken\n",
      "from Reuters. Features included words, POS tags, suffixes and prefixes or CHUNK tags, but overall\n",
      "were less specialized than CoNLL 2003 challengers.\n",
      "1. See http://www.cnts.ua.ac.be/conll2000/chunking.\n",
      "2. See Table 3 for tagging scheme details.\n",
      "3. See http://www.cnts.ua.ac.be/conll2003/ner.\n",
      "2496  \n",
      "\n",
      "NATURAL LANGUAGE PROCESSING (ALMOST) FROM SCRATCH\n",
      "2.4 Semantic Role Labeling\n",
      "SRL aims at giving a semantic role to a syntactic constituent of a sentence. In the PropBank\n",
      "(Palmer et al., 2005) formalism one assigns roles ARG0-5 to words that are arguments of a verb\n",
      "(or more technically, a predicate) in the sentence, for example, the following sentence might be\n",
      "tagged “[John] [ate] [the apple] ”, where “ate” is the predicate. The precise arguments\n",
      "ARG0 REL ARG1\n",
      "depend on a verb’s frame and if there are multiple verbs in a sentence some words might have multi-\n",
      "ple tags. In addition to the ARG0-5 tags, there there are several modifier tags such as ARGM-LOC\n",
      "(locational) and ARGM-TMP (temporal) that operate in a similar way for all verbs. We picked\n",
      "CoNLL 20054 as our SRL benchmark. It takes sections 2–21 of WSJ data as training set, and sec-\n",
      "tion 24 as validation set. A test set composed of section 23 of WSJ concatenated with 3 sections\n",
      "from the Brown corpus is also provided by the challenge.\n",
      "State-of-the-art SRL systems consist of several stages: producing a parse tree, identifying which\n",
      "parse tree nodes represent the arguments of a given verb, and finally classifying these nodes to\n",
      "compute the corresponding SRL tags. This entails extracting numerous base features from the parse\n",
      "tree and feeding them into statistical models. Feature categories commonly used by these system\n",
      "include (Gildea and Jurafsky, 2002; Pradhan et al., 2004):\n",
      "• the parts of speech and syntactic labels of words and nodes in the tree;\n",
      "• the node’s position (left or right) in relation to the verb;\n",
      "• the syntactic path to the verb in the parse tree;\n",
      "• whether a node in the parse tree is part of a noun or verb phrase;\n",
      "• the voice of the sentence: active or passive;\n",
      "• the node’s head word; and\n",
      "• the verb sub-categorization.\n",
      "Pradhan et al. (2004) take these base features and define additional features, notably the part-of-\n",
      "speech tag of the head word, the predicted named entity class of the argument, features providing\n",
      "word sense disambiguation for the verb (they add 25 variants of 12 new feature types overall). This\n",
      "system is close to the state-of-the-art in performance. Pradhan et al. (2005) obtain 77.30% F1 with a\n",
      "system based on SVM classifiers and simultaneously using the two parse trees provided for the SRL\n",
      "task. In the same spirit, Haghighi et al. (2005) use log-linear models on each tree node, re-ranked\n",
      "globally with a dynamic algorithm. Their system reaches 77.04% using the five top Charniak parse\n",
      "trees.\n",
      "Koomen et al. (2005) hold the state-of-the-art with Winnow-like (Littlestone, 1988) classifiers,\n",
      "followed by a decoding stage based on an integer program that enforces specific constraints on SRL\n",
      "tags. They reach 77.92% F1 on CoNLL 2005, thanks to the five top parse trees produced by the\n",
      "Charniak (2000) parser (only the first one was provided by the contest) as well as the Collins (1999)\n",
      "parse tree.\n",
      "4. See http://www.lsi.upc.edu/˜srlconll.\n",
      "2497  \n",
      "\n",
      "COLLOBERT, WESTON, BOTTOU, KARLEN, KAVUKCUOGLU AND KUKSA\n",
      "2.5 Evaluation\n",
      "In our experiments, we strictly followed the standard evaluation procedure of each CoNLL chal-\n",
      "lenges for NER, CHUNK and SRL. In particular, we chose the hyper-parameters of our model\n",
      "according to a simple validation procedure (see Remark 8 later in Section 3.5), performed over the\n",
      "validation set available for each task (see Section 2). All these three tasks are evaluated by comput-\n",
      "ing the F1 scores over chunks produced by our models. The POS task is evaluated by computing\n",
      "the per-word accuracy, as it is the case for the standard benchmark we refer to (Toutanova et al.,\n",
      "2003). We used the conlleval script5 for evaluating POS,6 NER and CHUNK. For SRL, we used\n",
      "the srl-eval.pl script included in the srlconll package.7\n",
      "2.6 Discussion\n",
      "When participating in an (open) challenge, it is legitimate to increase generalization by all means.\n",
      "It is thus not surprising to see many top CoNLL systems using external labeled data, like additional\n",
      "NER classifiers for the NER architecture of Florian et al. (2003) or additional parse trees for SRL\n",
      "systems (Koomen et al., 2005). Combining multiple systems or tweaking carefully features is also\n",
      "a common approach, like in the chunking top system (Shen and Sarkar, 2005).\n",
      "However, when comparing systems, we do not learn anything of the quality of each system if\n",
      "they were trained with different labeled data. For that reason, we will refer to benchmark systems,\n",
      "that is, top existing systems which avoid usage of external data and have been well-established in\n",
      "the NLP field: Toutanova et al. (2003) for POS and Sha and Pereira (2003) for chunking. For NER\n",
      "we consider Ando and Zhang (2005) as they were using additional unlabeled data only. We picked\n",
      "Koomen et al. (2005) for SRL, keeping in mind they use 4 additional parse trees not provided by\n",
      "the challenge. These benchmark systems will serve as baseline references in our experiments. We\n",
      "marked them in bold in Table 2.\n",
      "We note that for the four tasks we are considering in this work, it can be seen that for the\n",
      "more complex tasks (with corresponding lower accuracies), the best systems proposed have more\n",
      "engineered features relative to the best systems on the simpler tasks. That is, the POS task is one of\n",
      "the simplest of our four tasks, and only has relatively few engineered features, whereas SRL is the\n",
      "most complex, and many kinds of features have been designed for it. This clearly has implications\n",
      "for as yet unsolved NLP tasks requiring more sophisticated semantic understanding than the ones\n",
      "considered here.\n",
      "3. The Networks\n",
      "All the NLP tasks above can be seen as tasks assigning labels to words. The traditional NLP ap-\n",
      "proach is: extract from the sentence a rich set of hand-designed features which are then fed to a\n",
      "standard classification algorithm, for example, a Support Vector Machine (SVM), often with a lin-\n",
      "ear kernel. The choice of features is a completely empirical process, mainly based first on linguistic\n",
      "intuition, and then trial and error, and the feature selection is task dependent, implying additional\n",
      "research for each new NLP task. Complex tasks like SRL then require a large number of possibly\n",
      "5. Available at http://www.cnts.ua.ac.be/conll2000/chunking/conlleval.txt.\n",
      "6. We used the “-r” option of the conlleval script to get the per-word accuracy, for POS only.\n",
      "7. Available at http://www.lsi.upc.es/˜srlconll/srlconll-1.1.tgz.\n",
      "2498  \n",
      "\n",
      "NATURAL LANGUAGE PROCESSING (ALMOST) FROM SCRATCH\n",
      "Input Window\n",
      "word of interest\n",
      "Text cat sat on the mat\n",
      "Feature 1 w1 w1 . . . w1\n",
      ". 1 2 N\n",
      ".\n",
      ".\n",
      "Feature K wK wK . . . wK\n",
      "1 2 N\n",
      "Lookup Table\n",
      "xxxxxxxxxx\n",
      "LT\n",
      ".\n",
      ".\n",
      ".W 1\n",
      "xxxxxxxxxx xxxxxxxxxx xxxxxxxxxx xxxxxxxxxx\n",
      "d\n",
      "xxxxLT W K xxxx xxxx xxxx xxxx\n",
      "concat\n",
      "Linear\n",
      "M 1 × ·\n",
      "xxxxxxxxxxxxxnxx1 hx uxxxxxxxxxxxxxx\n",
      "HardTanh\n",
      "Linear\n",
      "M 2 × ·\n",
      "n2 hu = #tags\n",
      "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "Figure 1: Window approach network.\n",
      "complex features (e.g., extracted from a parse tree) which can impact the computational cost which\n",
      "might be important for large-scale applications or applications requiring real-time response.\n",
      "Instead, we advocate a radically different approach: as input we will try to pre-process our\n",
      "features as little as possible and then use a multilayer neural network (NN) architecture, trained in\n",
      "an end-to-end fashion. The architecture takes the input sentence and learns several layers of feature\n",
      "extraction that process the inputs. The features computed by the deep layers of the network are\n",
      "automatically trained by backpropagation to be relevant to the task. We describe in this section a\n",
      "general multilayer architecture suitable for all our NLP tasks, which is generalizable to other NLP\n",
      "tasks as well.\n",
      "Our architecture is summarized in Figure 1 and Figure 2. The first layer extracts features for\n",
      "each word. The second layer extracts features from a window of words or from the whole sentence,\n",
      "treating it as a sequence with local and global structure (i.e., it is not treated like a bag of words).\n",
      "The following layers are standard NN layers.\n",
      "3.1 Notations\n",
      "We consider a neural network fq (·), with parameters q . Any feed-forward neural network with L\n",
      "layers, can be seen as a composition of functions f l (·), corresponding to each layer l:\n",
      "q\n",
      "fq (·) = f q L( f q L−1(. . . f q 1(·) . . .)) .\n",
      "2499  \n",
      "\n",
      "COLLOBERT, WESTON, BOTTOU, KARLEN, KAVUKCUOGLU AND KUKSA\n",
      "Input Sentence\n",
      "Text The cat sat on the mat\n",
      "w1 w1 w1\n",
      "Feature 1 1 2 . . . N\n",
      ".\n",
      ".\n",
      ".\n",
      "wK wK wK\n",
      "Feature K 1 2 . . . N\n",
      "Lookup Table\n",
      "xxxxxxxxxx\n",
      "LT\n",
      ". .\n",
      ".W 1\n",
      "xxxxxxxxxx xxxxxxxxxx xxxxxxxxxx xxxxxxxxxx xxxxxxxxxx xxxxxxxxxx xxxxxxxxxx xxxxxxxxxx\n",
      "d\n",
      "xxxxxLT\n",
      "W K\n",
      "xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx xxxxx\n",
      "Convolution\n",
      "M 1 × ·\n",
      "xxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxx\n",
      "n1\n",
      "hu\n",
      "Max Over Time\n",
      "max(·)\n",
      "n1\n",
      "hu\n",
      "Linear\n",
      "M 2 × ·\n",
      "xxxxxxxxxxxxxxxxxxxxxxxnxxxx2 hxx uxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "HardTanh\n",
      "Linear\n",
      "M 3 × ·\n",
      "n3 hu = #tags\n",
      "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "Figure 2: Sentence approach network.\n",
      "In the following, we will describe each layer we use in our networks shown in Figure 1 and Figure 2.\n",
      "We adopt few notations. Given a matrix A we denote [A] the coefficient at row i and column j\n",
      "i, j\n",
      "in the matrix. We also denote hAidwin the vector obtained by concatenating the d column vectors\n",
      "i win\n",
      "around the ith column vector of matrix A ∈ Rd1×d2:\n",
      "hAidwin T = [A] . . . [A] , . . . , [A] . . . [A] .\n",
      "i 1,i−dwin/2 d1,i−dwin/2 1,i+dwin/2 d1,i+dwin/2\n",
      "h i (cid:16) (cid:17)\n",
      "2500\n",
      "Padding Padding  \n",
      "\n",
      "NATURAL LANGUAGE PROCESSING (ALMOST) FROM SCRATCH\n",
      "As a special case, hAi1 represents the ith column of matrix A. For a vector v, we denote [v] the\n",
      "i i\n",
      "scalar at index i in the vector. Finally, a sequence of element {x , x , . . . , x } is written [x]T . The ith\n",
      "1 2 T 1\n",
      "element of the sequence is [x] .\n",
      "i\n",
      "3.2 Transforming Words into Feature Vectors\n",
      "One of the key points of our architecture is its ability to perform well with the use of (almost8)\n",
      "raw words. The ability for our method to learn good word representations is thus crucial to our\n",
      "approach. For efficiency, words are fed to our architecture as indices taken from a finite dictionary\n",
      "D\n",
      ". Obviously, a simple index does not carry much useful information about the word. However,\n",
      "the first layer of our network maps each of these word indices into a feature vector, by a lookup\n",
      "table operation. Given a task of interest, a relevant representation of each word is then given by\n",
      "the corresponding lookup table feature vector, which is trained by backpropagation, starting from\n",
      "a random initialization.9 We will see in Section 4 that we can learn very good word representa-\n",
      "tions from unlabeled corpora. Our architecture allow us to take advantage of better trained word\n",
      "representations, by simply initializing the word lookup table with these representations (instead of\n",
      "randomly).\n",
      "D\n",
      "More formally, for each word w ∈ , an internal d -dimensional feature vector representation\n",
      "wrd\n",
      "is given by the lookup table layer LT (·):\n",
      "W\n",
      "LT (w) = hW i1 ,\n",
      "W w\n",
      "where W ∈ Rdwrd×|D| is a matrix of parameters to be learned, hW i1 ∈ Rdwrd is the wth column of W\n",
      "w\n",
      "and d is the word vector size (a hyper-parameter to be chosen by the user). Given a sentence or\n",
      "wrd\n",
      "any sequence of T words [w]T in D , the lookup table layer applies the same operation for each word\n",
      "1\n",
      "in the sequence, producing the following output matrix:\n",
      "LT ([w]T ) = hW i1 hW i1 . . . hW i1 . (1)\n",
      "W 1 [w] [w] [w]\n",
      "1 2 T\n",
      "(cid:16) (cid:17)\n",
      "This matrix can then be fed to further neural network layers, as we will see below.\n",
      "3.2.1 EXTENDING TO ANY DISCRETE FEATURES\n",
      "One might want to provide features other than words if one suspects that these features are helpful\n",
      "for the task of interest. For example, for the NER task, one could provide a feature which says if a\n",
      "word is in a gazetteer or not. Another common practice is to introduce some basic pre-processing,\n",
      "such as word-stemming or dealing with upper and lower case. In this latter option, the word would\n",
      "be then represented by three discrete features: its lower case stemmed root, its lower case ending,\n",
      "and a capitalization feature.\n",
      "Generally speaking, we can consider a word as represented by K discrete features w ∈\n",
      "D1\n",
      "×\n",
      "· · · × DK, where Dk is the dictionary for the kth feature. We associate to each feature a lookup table\n",
      "LT\n",
      "Wk\n",
      "(·), with parameters W k ∈ Rd wk rd×|Dk| where d wk\n",
      "rd\n",
      "∈ N is a user-specified vector size. Given a\n",
      "8. We did some pre-processing, namely lowercasing and encoding capitalization as another feature. With enough (un-\n",
      "labeled) training data, presumably we could learn a model without this processing. Ideally, an even more raw input\n",
      "would be to learn from letter sequences rather than words, however we felt that this was beyond the scope of this\n",
      "work.\n",
      "9. As any other neural network layer.\n",
      "2501  \n",
      "\n",
      "COLLOBERT, WESTON, BOTTOU, KARLEN, KAVUKCUOGLU AND KUKSA\n",
      "word w, a feature vector of dimension d = (cid:229) dk is then obtained by concatenating all lookup\n",
      "wrd k wrd\n",
      "table outputs:\n",
      "LT (w ) hW 1i1\n",
      "W1 1 w1\n",
      ". .\n",
      "LT W1,...,WK (w) =  . .  =  . .  .\n",
      "LT (w ) hW Ki1\n",
      " WK K   wK \n",
      "The matrix output of the lookup table\n",
      "la\n",
      "yer for a\n",
      "seque\n",
      "nce\n",
      "o\n",
      "f words\n",
      "[w]T\n",
      "is then similar to (1), but\n",
      "1\n",
      "where extra rows have been added for each discrete feature:\n",
      "hW 1i1 . . . hW 1i1\n",
      "[w1]\n",
      "1\n",
      "[w1]\n",
      "T\n",
      "LT W1,...,WK ([w]T 1 ) =  . . . . . .  . (2)\n",
      "hW Ki1 . . . hW Ki1\n",
      " [wK] 1 [wK] T \n",
      " \n",
      "These vector features in the lookup table effectively learn features for words in the dictionary. Now,\n",
      "we want to use these trainable features as input to further layers of trainable feature extractors, that\n",
      "can represent groups of words and then finally sentences.\n",
      "3.3 Extracting Higher Level Features from Word Feature Vectors\n",
      "Feature vectors produced by the lookup table layer need to be combined in subsequent layers of\n",
      "the neural network to produce a tag decision for each word in the sentence. Producing tags for\n",
      "each element in variable length sequences (here, a sentence is a sequence of words) is a standard\n",
      "problem in machine-learning. We consider two common approaches which tag one word at the\n",
      "time: a window approach, and a (convolutional) sentence approach.\n",
      "3.3.1 WINDOW APPROACH\n",
      "A window approach assumes the tag of a word depends mainly on its neighboring words. Given a\n",
      "word to tag, we consider a fixed size k (a hyper-parameter) window of words around this word.\n",
      "sz\n",
      "Each word in the window is first passed through the lookup table layer (1) or (2), producing a matrix\n",
      "of word features of fixed size d × k . This matrix can be viewed as a d k -dimensional vector\n",
      "wrd sz wrd sz\n",
      "by concatenating each column vector, which can be fed to further neural network layers. More\n",
      "formally, the word feature window given by the first network layer can be written as:\n",
      "hW i1\n",
      "[w]\n",
      "t−dwin/2\n",
      ".\n",
      ".\n",
      " . \n",
      "f q 1 = hLT W ([w]T 1 )i tdwin =  hW i1 [w]  . (3)\n",
      " . t \n",
      " . . \n",
      " \n",
      " hW i1 \n",
      " [w] \n",
      " t+dwin/2 \n",
      " \n",
      "Linear Layer. The fixed size vector f 1 can be fed to one or several standard neural network layers\n",
      "q\n",
      "which perform affine transformations over their inputs:\n",
      "f l = W l f l−1 + bl , (4)\n",
      "q q\n",
      "where W l ∈ Rnl hu×nl h− u1 and bl ∈ Rnl hu are the parameters to be trained. The hyper-parameter nl is\n",
      "hu\n",
      "usually called the number of hidden units of the lth layer.\n",
      "2502  \n",
      "\n",
      "NATURAL LANGUAGE PROCESSING (ALMOST) FROM SCRATCH\n",
      "HardTanh Layer. Several linear layers are often stacked, interleaved with a non-linearity func-\n",
      "tion, to extract highly non-linear features. If no non-linearity is introduced, our network would be a\n",
      "simple linear model. We chose a “hard” version of the hyperbolic tangent as non-linearity. It has the\n",
      "advantage of being slightly cheaper to compute (compared to the exact hyperbolic tangent), while\n",
      "leaving the generalization performance unchanged (Collobert, 2004). The corresponding layer l\n",
      "applies a HardTanh over its input vector:\n",
      "f l = HardTanh( f l−1 ) ,\n",
      "q q\n",
      "i i\n",
      "h i h i\n",
      "where\n",
      "−1 if x < −1\n",
      "HardTanh(x) = x if − 1 <= x <= 1 . (5)\n",
      "\n",
      "1 if x > 1\n",
      "\n",
      "Scoring. Finally, the output size of the last layer L of our network is equal to the number\n",
      "of possible tags for the task of interest. Each output can be then interpreted as a score of the\n",
      "corresponding tag (given the input of the network), thanks to a carefully chosen cost function that\n",
      "we will describe later in this section.\n",
      "Remark 1 (Border Effects) The feature window (3) is not well defined for words near the begin-\n",
      "ning or the end of a sentence. To circumvent this problem, we augment the sentence with a special\n",
      "“PADDING” word replicated d /2 times at the beginning and the end. This is akin to the use of\n",
      "win\n",
      "“start” and “stop” symbols in sequence models.\n",
      "3.3.2 SENTENCE APPROACH\n",
      "We will see in the experimental section that a window approach performs well for most natural\n",
      "language processing tasks we are interested in. However this approach fails with SRL, where the tag\n",
      "of a word depends on a verb (or, more correctly, predicate) chosen beforehand in the sentence. If the\n",
      "verb falls outside the window, one cannot expect this word to be tagged correctly. In this particular\n",
      "case, tagging a word requires the consideration of the whole sentence. When using neural networks,\n",
      "the natural choice to tackle this problem becomes a convolutional approach, first introduced by\n",
      "Waibel et al. (1989) and also called Time Delay Neural Networks (TDNNs) in the literature.\n",
      "We describe in detail our convolutional network below. It successively takes the complete sen-\n",
      "tence, passes it through the lookup table layer (1), produces local features around each word of the\n",
      "sentence thanks to convolutional layers, combines these feature into a global feature vector which\n",
      "can then be fed to standard affine layers (4). In the semantic role labeling case, this operation is\n",
      "performed for each word in the sentence, and for each verb in the sentence. It is thus necessary to\n",
      "encode in the network architecture which verb we are considering in the sentence, and which word\n",
      "we want to tag. For that purpose, each word at position i in the sentence is augmented with two\n",
      "features in the way described in Section 3.2.1. These features encode the relative distances i − pos\n",
      "v\n",
      "and i − pos with respect to the chosen verb at position pos , and the word to tag at position pos\n",
      "w v w\n",
      "respectively.\n",
      "Convolutional Layer. A convolutional layer can be seen as a generalization of a window ap-\n",
      "proach: given a sequence represented by columns in a matrix f l−1 (in our lookup table matrix (1)),\n",
      "q\n",
      "a matrix-vector operation as in (4) is applied to each window of successive windows in the sequence.\n",
      "2503  \n",
      "\n",
      "COLLOBERT, WESTON, BOTTOU, KARLEN, KAVUKCUOGLU AND KUKSA\n",
      "70 70\n",
      "x\n",
      "60 x 60 xxxx\n",
      "50 xxx 50 xxxx\n",
      "xxx xxxx\n",
      "40 40\n",
      "xxx xx xxxx\n",
      "30 xxxx 30 xx x xxxx\n",
      "20 xxxxxx xx 20 xxxx xxxx\n",
      "xxxxxx xxxx xxxx xxxxx\n",
      "10 10\n",
      "xxxxxxxxxxxxxx x xxxxxx xxxxx\n",
      "0 xxxxxxxxxxxxxxxxxxxx xx 0 xx xxxxxxxxxxxxxxxxxxxxxxxx\n",
      "xThxx eprx opc oxh\n",
      "sa en\n",
      "dgax els soxwouxx ldalloxx wexxx ecuto tx ivesrxeporex txerx co isf esxxoptxx ionla stxx eranx dlexssxoftenxx. x xThxx eprx opoc sxh\n",
      "ea\n",
      "dngax els soxwouxx ldalloxx wexxx ecuto tx ivesrxeporex txerx co isf esxxoptxx ionla stxx eranx dlexssxoftenxx. x\n",
      "Figure 3: Number of features chosen at each word position by the Max layer. We consider a sen-\n",
      "tence approach network (Figure 2) trained for SRL. The number of “local” features output\n",
      "by the convolution layer is 300 per word. By applying a Max over the sentence, we ob-\n",
      "tain 300 features for the whole sentence. It is interesting to see that the network catches\n",
      "features mostly around the verb of interest (here “report”) and word of interest (“pro-\n",
      "posed” (left) or “often” (right)).\n",
      "Using previous notations, the tth output column of the lth layer can be computed as:\n",
      "h f l i1 = W l h f l−1idwin + bl ∀t , (6)\n",
      "q t q t\n",
      "where the weight matrix W l is the same across all windows t in the sequence. Convolutional layers\n",
      "extract local features around each window of the given sequence. As for standard affine layers (4),\n",
      "convolutional layers are often stacked to extract higher level features. In this case, each layer must\n",
      "be followed by a non-linearity (5) or the network would be equivalent to one convolutional layer.\n",
      "Max Layer. The size of the output (6) depends on the number of words in the sentence fed\n",
      "to the network. Local feature vectors extracted by the convolutional layers have to be combined\n",
      "to obtain a global feature vector, with a fixed size independent of the sentence length, in order to\n",
      "apply subsequent standard affine layers. Traditional convolutional networks often apply an average\n",
      "(possibly weighted) or a max operation over the “time” t of the sequence (6). (Here, “time” just\n",
      "means the position in the sentence, this term stems from the use of convolutional layers in, for\n",
      "example, speech data where the sequence occurs over time.) The average operation does not make\n",
      "much sense in our case, as in general most words in the sentence do not have any influence on the\n",
      "semantic role of a given word to tag. Instead, we used a max approach, which forces the network to\n",
      "capture the most useful local features produced by the convolutional layers (see Figure 3), for the\n",
      "task at hand. Given a matrix f l−1 output by a convolutional layer l − 1, the Max layer l outputs a\n",
      "q\n",
      "vector f l :\n",
      "q\n",
      "f l = max f l−1 1 ≤ i ≤ nl−1 . (7)\n",
      "q q hu\n",
      "i t i,t\n",
      "h i h i\n",
      "This fixed sized global feature vector can be then fed to standard affine network layers (4). As in\n",
      "the window approach, we then finally produce one score per possible tag for the given task.\n",
      "Remark 2 The same border effects arise in the convolution operation (6) as in the window ap-\n",
      "proach (3). We again work around this problem by padding the sentences with a special word.\n",
      "2504  \n",
      "\n",
      "NATURAL LANGUAGE PROCESSING (ALMOST) FROM SCRATCH\n",
      "Scheme Begin Inside End Single Other\n",
      "IOB B-X I-X I-X B-X O\n",
      "IOE I-X I-X E-X E-X O\n",
      "IOBES B-X I-X E-X S-X O\n",
      "Table 3: Various tagging schemes. Each word in a segment labeled “X” is tagged with a prefixed\n",
      "label, depending of the word position in the segment (begin, inside, end). Single word\n",
      "segment labeling is also output. Words not in a labeled segment are labeled “O”. Variants\n",
      "of the IOB (and IOE) scheme exist, where the prefix B (or E) is replaced by I for all\n",
      "segments not contiguous with another segment having the same label “X”.\n",
      "3.3.3 TAGGING SCHEMES\n",
      "As explained earlier, the network output layers compute scores for all the possible tags for the task of\n",
      "interest. In the window approach, these tags apply to the word located in the center of the window.\n",
      "In the (convolutional) sentence approach, these tags apply to the word designated by additional\n",
      "markers in the network input.\n",
      "The POS task indeed consists of marking the syntactic role of each word. However, the re-\n",
      "maining three tasks associate labels with segments of a sentence. This is usually achieved by using\n",
      "special tagging schemes to identify the segment boundaries, as shown in Table 3. Several such\n",
      "schemes have been defined (IOB, IOE, IOBES, . . . ) without clear conclusion as to which scheme\n",
      "is better in general. State-of-the-art performance is sometimes obtained by combining classifiers\n",
      "trained with different tagging schemes (e.g., Kudo and Matsumoto, 2001).\n",
      "The ground truth for the NER, CHUNK, and SRL tasks is provided using two different tagging\n",
      "schemes. In order to eliminate this additional source of variations, we have decided to use the most\n",
      "expressive IOBES tagging scheme for all tasks. For instance, in the CHUNK task, we describe\n",
      "noun phrases using four different tags. Tag “S-NP” is used to mark a noun phrase containing a\n",
      "single word. Otherwise tags “B-NP”, “I-NP”, and “E-NP” are used to mark the first, intermediate\n",
      "and last words of the noun phrase. An additional tag “O” marks words that are not members of a\n",
      "chunk. During testing, these tags are then converted to the original IOB tagging scheme and fed to\n",
      "the standard performance evaluation scripts mentioned in Section 2.5.\n",
      "3.4 Training\n",
      "All our neural networks are trained by maximizing a likelihood over the training data, using stochas-\n",
      "tic gradient ascent. If we denote q to be all the trainable parameters of the network, which are trained\n",
      "using a training set T we want to maximize the following log-likelihood with respect to q :\n",
      "q 7→ (cid:229) log p(y | x, q ) , (8)\n",
      "(x,y)∈T\n",
      "where x corresponds to either a training word window or a sentence and its associated features, and\n",
      "y represents the corresponding tag. The probability p(·) is computed from the outputs of the neural\n",
      "network. We will see in this section two ways of interpreting neural network outputs as probabilities.\n",
      "2505  \n",
      "\n",
      "COLLOBERT, WESTON, BOTTOU, KARLEN, KAVUKCUOGLU AND KUKSA\n",
      "3.4.1 WORD-LEVEL LOG-LIKELIHOOD\n",
      "In this approach, each word in a sentence is considered independently. Given an input example\n",
      "x, the network with parameters q outputs a score f (x) , for the ith tag with respect to the task of\n",
      "q i\n",
      "interest. To simplify the notation, we drop x from now, and we write instead f . This score can be\n",
      "(cid:2) (cid:3) q i\n",
      "interpreted as a conditional tag probability p(i | x, q ) by applying a softmax (Bridle, 1990) operation\n",
      "(cid:2) (cid:3)\n",
      "over all the tags:\n",
      "p(i | x, q ) =\n",
      "e[ fq ]\n",
      "i . (9)\n",
      "(cid:229) e[ fq ] j\n",
      "j\n",
      "Defining the log-add operation as\n",
      "logadd z = log((cid:229) ezi ) , (10)\n",
      "i\n",
      "i i\n",
      "we can express the log-likelihood for one training example (x, y) as follows:\n",
      "log p(y | x, q ) = [ f q ] y − logadd [ f q ] j . (11)\n",
      "j\n",
      "While this training criterion, often referred as cross-entropy is widely used for classification prob-\n",
      "lems, it might not be ideal in our case, where there is often a correlation between the tag of a word\n",
      "in a sentence and its neighboring tags. We now describe another common approach for neural\n",
      "networks which enforces dependencies between the predicted tags in a sentence.\n",
      "3.4.2 SENTENCE-LEVEL LOG-LIKELIHOOD\n",
      "In tasks like chunking, NER or SRL we know that there are dependencies between word tags in a\n",
      "sentence: not only are tags organized in chunks, but some tags cannot follow other tags. Training\n",
      "using a word-level approach discards this kind of labeling information. We consider a training\n",
      "scheme which takes into account the sentence structure: given the predictions of all tags by our\n",
      "network for all words in a sentence, and given a score for going from one tag to another tag, we\n",
      "want to encourage valid paths of tags during training, while discouraging all other paths.\n",
      "We consider the matrix of scores f ([x]T ) output by the network. As before, we drop the input\n",
      "q 1\n",
      "[x]T for notation simplification. The element f of the matrix is the score output by the network\n",
      "1 q i,t\n",
      "with parameters q , for the sentence [x]T and for the ith tag, at the tth word. We introduce a transition\n",
      "1 (cid:2) (cid:3)\n",
      "score [A] for jumping from i to j tags in successive words, and an initial score [A] for starting\n",
      "i, j i,0\n",
      "from the ith tag. As the transition scores are going to be trained (as are all network parameters q ),\n",
      "we define q ˜ = q ∪ {[A] ∀i, j}. The score of a sentence [x]T along a path of tags [i]T is then given\n",
      "i, j 1 1\n",
      "by the sum of transition scores and network scores:\n",
      "T\n",
      "s([x]T 1 , [i]T 1 , q ˜ ) = (cid:229) [A] [i] t−1,[i]\n",
      "t\n",
      "+ [ f q ] [i] t,t . (12)\n",
      "t=1\n",
      "(cid:16) (cid:17)\n",
      "Exactly as for the word-level likelihood (11), where we were normalizing with respect to all tags\n",
      "using a softmax (9), we normalize this score over all possible tag paths [ j]T using a softmax, and\n",
      "1\n",
      "we interpret the resulting ratio as a conditional tag path probability. Taking the log, the conditional\n",
      "probability of the true path [y]T is therefore given by:\n",
      "1\n",
      "log p([y]T | [x]T , q ˜ ) = s([x]T , [y]T , q ˜ ) − logadd s([x]T , [ j]T , q ˜ ) . (13)\n",
      "1 1 1 1 1 1\n",
      "∀[ j]T\n",
      "1\n",
      "2506  \n",
      "\n",
      "NATURAL LANGUAGE PROCESSING (ALMOST) FROM SCRATCH\n",
      "While the number of terms in the logadd operation (11) was equal to the number of tags, it grows\n",
      "exponentially with the length of the sentence in (13). Fortunately, one can compute it in linear\n",
      "time with the following standard recursion over t (see Rabiner, 1989), taking advantage of the\n",
      "associativity and distributivity on the semi-ring10 (R ∪ {−¥ }, logadd, +):\n",
      "D\n",
      "d (k) = logadd s([x]t , [ j]t , q ˜ )\n",
      "t 1 1\n",
      "{[ j]t ∩[ j] =k}\n",
      "1 t\n",
      "= logadd logadd s([x]t 1, [ j]t 1−1, q ˜ ) + [A] [ j] ,k + [ f q ] k,t\n",
      "i {[ j]t 1 ∩[ j] t−1=i∩[ j] t=k} t−1 (14)\n",
      "= logadd d t−1(i) + [A] i,k + [ f q ] k,t\n",
      "i\n",
      "= [ f q ] k,t + logadd d t−1(i) + [A] i,k ∀k ,\n",
      "i\n",
      "(cid:16) (cid:17)\n",
      "followed by the termination\n",
      "logadd s([x]T , [ j]T , q ˜ ) = logadd d (i) . (15)\n",
      "1 1 T\n",
      "∀[ j]T i\n",
      "1\n",
      "We can now maximize in (8) the log-likelihood (13) over all the training pairs ([x]T , [y]T ).\n",
      "1 1\n",
      "At inference time, given a sentence [x]T to tag, we have to find the best tag path which minimizes\n",
      "1\n",
      "the sentence score (12). In other words, we must find\n",
      "argmax s([x]T , [ j]T , q ˜ ) .\n",
      "1 1\n",
      "[ j]T\n",
      "1\n",
      "The Viterbi algorithm is the natural choice for this inference. It corresponds to performing the\n",
      "recursion (14) and (15), but where the logadd is replaced by a max, and then tracking back the\n",
      "optimal path through each max.\n",
      "Remark 3 (Graph Transformer Networks) Our approach is a particular case of the discrimina-\n",
      "tive forward training for graph transformer networks (GTNs) (Bottou et al., 1997; Le Cun et al.,\n",
      "1998). The log-likelihood (13) can be viewed as the difference between the forward score con-\n",
      "strained over the valid paths (in our case there is only the labeled path) and the unconstrained\n",
      "forward score (15).\n",
      "Remark 4 (Conditional Random Fields) An important feature of equation (12) is the absence of\n",
      "normalization. Summing the exponentials e\n",
      "[ fq ]\n",
      "i,t over all possible tags does not necessarily yield\n",
      "the unity. If this was the case, the scores could be viewed as the logarithms of conditional transition\n",
      "probabilities, and our model would be subject to the label-bias problem that motivates Conditional\n",
      "Random Fields (CRFs) (Lafferty et al., 2001). The denormalized scores should instead be likened to\n",
      "the potential functions of a CRF. In fact, a CRF maximizes the same likelihood (13) using a linear\n",
      "model instead of a nonlinear neural network. CRFs have been widely used in the NLP world, such\n",
      "as for POS tagging (Lafferty et al., 2001), chunking (Sha and Pereira, 2003), NER (McCallum and\n",
      "Li, 2003) or SRL (Cohn and Blunsom, 2005). Compared to such CRFs, we take advantage of the\n",
      "nonlinear network to learn appropriate features for each task of interest.\n",
      "10. In other words, read logadd as ⊕ and + as ⊗.\n",
      "2507  \n",
      "\n",
      "COLLOBERT, WESTON, BOTTOU, KARLEN, KAVUKCUOGLU AND KUKSA\n",
      "3.4.3 STOCHASTIC GRADIENT\n",
      "Maximizing (8) with stochastic gradient (Bottou, 1991) is achieved by iteratively selecting a random\n",
      "example (x, y) and making a gradient step:\n",
      "¶ log p(y | x, q )\n",
      "q ←− q + l , (16)\n",
      "¶q\n",
      "where l is a chosen learning rate. Our neural networks described in Figure 1 and Figure 2 are a\n",
      "succession of layers that correspond to successive composition of functions. The neural network\n",
      "is finally composed with the word-level log-likelihood (11), or successively composed in the re-\n",
      "cursion (14) if using the sentence-level log-likelihood (13). Thus, an analytical formulation of the\n",
      "derivative (16) can be computed, by applying the differentiation chain rule through the network, and\n",
      "through the word-level log-likelihood (11) or through the recurrence (14).\n",
      "Remark 5 (Differentiability) Our cost functions are differentiable almost everywhere.\n",
      "Non-differentiable points arise because we use a “hard” transfer function (5) and because we use a\n",
      "“max” layer (7) in the sentence approach network. Fortunately, stochastic gradient still converges\n",
      "to a meaningful local minimum despite such minor differentiability problems (Bottou, 1991, 1998).\n",
      "Stochastic gradient iterations that hit a non-differentiability are simply skipped.\n",
      "Remark 6 (Modular Approach) The well known “back-propagation” algorithm (LeCun, 1985;\n",
      "Rumelhart et al., 1986) computes gradients using the chain rule. The chain rule can also be used\n",
      "in a modular implementation.11 Our modules correspond to the boxes in Figure 1 and Figure 2.\n",
      "Given derivatives with respect to its outputs, each module can independently compute derivatives\n",
      "with respect to its inputs and with respect to its trainable parameters, as proposed by Bottou and\n",
      "Gallinari (1991). This allows us to easily build variants of our networks. For details about gradient\n",
      "computations, see Appendix A.\n",
      "Remark 7 (Tricks) Many tricks have been reported for training neural networks (LeCun et al.,\n",
      "1998). Which ones to choose is often confusing. We employed only two of them: the initialization\n",
      "and update of the parameters of each network layer were done according to the “fan-in” of the\n",
      "layer, that is the number of inputs used to compute each output of this layer (Plaut and Hinton,\n",
      "1987). The fan-in for the lookup table (1), the lth linear layer (4) and the convolution layer (6)\n",
      "are respectively 1, nl−1 and d × nl−1. The initial parameters of the network were drawn from a\n",
      "hu win hu\n",
      "centered uniform distribution, with a variance equal to the inverse of the square-root of the fan-in.\n",
      "The learning rate in (16) was divided by the fan-in, but stays fixed during the training.\n",
      "3.5 Supervised Benchmark Results\n",
      "For POS, chunking and NER tasks, we report results with the window architecture12 described\n",
      "in Section 3.3.1. The SRL task was trained using the sentence approach (Section 3.3.2). Results\n",
      "are reported in Table 4, in per-word accuracy (PWA) for POS, and F1 score for all the other tasks.\n",
      "We performed experiments both with the word-level log-likelihood (WLL) and with the sentence-\n",
      "level log-likelihood (SLL). The hyper-parameters of our networks are reported in Table 5. All our\n",
      "11. See http://torch5.sf.net.\n",
      "12. We found that training these tasks with the more complex sentence approach was computationally expensive and\n",
      "offered little performance benefits. Results discussed in Section 5 provide more insight about this decision.\n",
      "2508  \n",
      "\n",
      "NATURAL LANGUAGE PROCESSING (ALMOST) FROM SCRATCH\n",
      "Approach POS Chunking NER SRL\n",
      "(PWA) (F1) (F1) (F1)\n",
      "Benchmark Systems 97.24 94.29 89.31 77.92\n",
      "NN+WLL 96.31 89.13 79.53 55.40\n",
      "NN+SLL 96.37 90.33 81.47 70.99\n",
      "Table 4: Comparison in generalization performance of benchmark NLP systems with a vanilla neu-\n",
      "ral network (NN) approach, on POS, chunking, NER and SRL tasks. We report results with\n",
      "both the word-level log-likelihood (WLL) and the sentence-level log-likelihood (SLL).\n",
      "Generalization performance is reported in per-word accuracy rate (PWA) for POS and F1\n",
      "score for other tasks. The NN results are behind the benchmark results, in Section 4 we\n",
      "show how to improve these models using unlabeled data.\n",
      "Task Window/Conv. size Word dim. Caps dim. Hidden units Learning rate\n",
      "POS d = 5 d0 = 50 d1 = 5 n1 = 300 l = 0.01\n",
      "win hu\n",
      "CHUNK ” ” ” ” ”\n",
      "NER ” ” ” ” ”\n",
      "n1 = 300\n",
      "SRL ” ” ” hu ”\n",
      "n2 = 500\n",
      "hu\n",
      "Table 5: Hyper-parameters of our networks. They were chosen by a minimal validation (see Re-\n",
      "mark 8), preferring identical parameters for most tasks. We report for each task the window\n",
      "size (or convolution size), word feature dimension, capital feature dimension, number of\n",
      "hidden units and learning rate.\n",
      "networks were fed with two raw text features: lower case words, and a capital letter feature. We\n",
      "chose to consider lower case words to limit the number of words in the dictionary. However, to keep\n",
      "some upper case information lost by this transformation, we added a “caps” feature which tells if\n",
      "each word was in lowercase, was all uppercase, had first letter capital, or had at least one non-initial\n",
      "capital letter. Additionally, all occurrences of sequences of numbers within a word are replaced with\n",
      "the string “NUMBER”, so for example both the words “PS1” and “PS2” would map to the single\n",
      "word “psNUMBER”. We used a dictionary containing the 100,000 most common words in WSJ\n",
      "(case insensitive). Words outside this dictionary were replaced by a single special “RARE” word.\n",
      "Results show that neural networks “out-of-the-box” are behind baseline benchmark systems.\n",
      "Although the initial performance of our networks falls short from the performance of the CoNLL\n",
      "challenge winners, it compares honorably with the performance of most competitors. The training\n",
      "criterion which takes into account the sentence structure (SLL) seems to boost the performance for\n",
      "the Chunking, NER and SRL tasks, with little advantage for POS. This result is in line with existing\n",
      "NLP studies comparing sentence-level and word-level likelihoods (Liang et al., 2008). The capacity\n",
      "of our network architectures lies mainly in the word lookup table, which contains 50 × 100, 000\n",
      "parameters to train. In the WSJ data, 15% of the most common words appear about 90% of the time.\n",
      "Many words appear only a few times. It is thus very difficult to train properly their corresponding\n",
      "2509  \n",
      "\n",
      "COLLOBERT, WESTON, BOTTOU, KARLEN, KAVUKCUOGLU AND KUKSA\n",
      "FRANCE JESUS XBOX REDDISH SCRATCHED MEGABITS\n",
      "454 1973 6909 11724 29869 87025\n",
      "PERSUADE THICKETS DECADENT WIDESCREEN ODD PPA\n",
      "FAW SAVARY DIVO ANTICA ANCHIETA UDDIN\n",
      "BLACKSTOCK SYMPATHETIC VERUS SHABBY EMIGRATION BIOLOGICALLY\n",
      "GIORGI JFK OXIDE AWE MARKING KAYAK\n",
      "SHAHEED KHWARAZM URBINA THUD HEUER MCLARENS\n",
      "RUMELIA STATIONERY EPOS OCCUPANT SAMBHAJI GLADWIN\n",
      "PLANUM ILIAS EGLINTON REVISED WORSHIPPERS CENTRALLY\n",
      "GOA’ULD GSNUMBER EDGING LEAVENED RITSUKO INDONESIA\n",
      "COLLATION OPERATOR FRG PANDIONIDAE LIFELESS MONEO\n",
      "BACHA W.J. NAMSOS SHIRT MAHAN NILGIRIS\n",
      "Table 6: Word embeddings in the word lookup table of a SRL neural network trained from scratch,\n",
      "with a dictionary of size 100, 000. For each column the queried word is followed by its\n",
      "index in the dictionary (higher means more rare) and its 10 nearest neighbors (arbitrarily\n",
      "using the Euclidean metric).\n",
      "50 dimensional feature vectors in the lookup table. Ideally, we would like semantically similar\n",
      "words to be close in the embedding space represented by the word lookup table: by continuity of\n",
      "the neural network function, tags produced on semantically similar sentences would be similar. We\n",
      "show in Table 6 that it is not the case: neighboring words in the embedding space do not seem to be\n",
      "semantically related.\n",
      "We will focus in the next section on improving these word embeddings by leveraging unlabeled\n",
      "data. We will see our approach results in a performance boost for all tasks.\n",
      "Remark 8 (Architectures) In all our experiments in this paper, we tuned the hyper-parameters by\n",
      "trying only a few different architectures by validation. In practice, the choice of hyperparameters\n",
      "such as the number of hidden units, provided they are large enough, has a limited impact on the\n",
      "generalization performance. In Figure 4, we report the F1 score for each task on the validation set,\n",
      "with respect to the number of hidden units. Considering the variance related to the network initial-\n",
      "ization, we chose the smallest network achieving “reasonable” performance, rather than picking\n",
      "the network achieving the top performance obtained on a single run.\n",
      "Remark 9 (Training Time) Training our network is quite computationally expensive. Chunking\n",
      "and NER take about one hour to train, POS takes few hours, and SRL takes about three days.\n",
      "Training could be faster with a larger learning rate, but we preferred to stick to a small one which\n",
      "works, rather than finding the optimal one for speed. Second order methods (LeCun et al., 1998)\n",
      "could be another speedup technique.\n",
      "4. Lots of Unlabeled Data\n",
      "We would like to obtain word embeddings carrying more syntactic and semantic information than\n",
      "shown in Table 6. Since most of the trainable parameters of our system are associated with the\n",
      "word embeddings, these poor results suggest that we should use considerably more training data.\n",
      "2510  \n",
      "\n",
      "NATURAL LANGUAGE PROCESSING (ALMOST) FROM SCRATCH\n",
      "96.5 91.5 86.5 69\n",
      "68.5\n",
      "91 86\n",
      "96 68\n",
      "90.5 85.5\n",
      "67.5\n",
      "95.5 90 85 67\n",
      "100 300 500 700 900 100 300 500 700 900 100 300 500 700 900 100 300 500 700 900\n",
      "(a) POS (b) CHUNK (c) NER (d) SRL\n",
      "Figure 4: F1 score on the validation set (y-axis) versus number of hidden units (x-axis) for different\n",
      "tasks trained with the sentence-level likelihood (SLL), as in Table 4. For SRL, we vary in\n",
      "this graph only the number of hidden units in the second layer. The scale is adapted for\n",
      "each task. We show the standard deviation (obtained over 5 runs with different random\n",
      "initialization), for the architecture we picked (300 hidden units for POS, CHUNK and\n",
      "NER, 500 for SRL).\n",
      "Following our NLP from scratch philosophy, we now describe how to dramatically improve these\n",
      "embeddings using large unlabeled data sets. We then use these improved embeddings to initialize\n",
      "the word lookup tables of the networks described in Section 3.5.\n",
      "4.1 Data Sets\n",
      "Our first English corpus is the entire English Wikipedia.13 We have removed all paragraphs con-\n",
      "taining non-roman characters and all MediaWiki markups. The resulting text was tokenized using\n",
      "the Penn Treebank tokenizer script.14 The resulting data set contains about 631 million words. As\n",
      "in our previous experiments, we use a dictionary containing the 100,000 most common words in\n",
      "WSJ, with the same processing of capitals and numbers. Again, words outside the dictionary were\n",
      "replaced by the special “RARE” word.\n",
      "Our second English corpus is composed by adding an extra 221 million words extracted from\n",
      "the Reuters RCV1 (Lewis et al., 2004) data set.15 We also extended the dictionary to 130, 000 words\n",
      "by adding the 30, 000 most common words in Reuters. This is useful in order to determine whether\n",
      "improvements can be achieved by further increasing the unlabeled data set size.\n",
      "4.2 Ranking Criterion versus Entropy Criterion\n",
      "We used these unlabeled data sets to train language models that compute scores describing the\n",
      "acceptability of a piece of text. These language models are again large neural networks using the\n",
      "window approach described in Section 3.3.1 and in Figure 1. As in the previous section, most of the\n",
      "trainable parameters are located in the lookup tables.\n",
      "Similar language models were already proposed by Bengio and Ducharme (2001) and Schwenk\n",
      "and Gauvain (2002). Their goal was to estimate the probability of a word given the previous words\n",
      "in a sentence. Estimating conditional probabilities suggests a cross-entropy criterion similar to those\n",
      "described in Section 3.4.1. Because the dictionary size is large, computing the normalization term\n",
      "13. Available at http://download.wikimedia.org. We took the November 2007 version.\n",
      "14. Available at http://www.cis.upenn.edu/˜treebank/tokenization.html.\n",
      "15. Now available at http://trec.nist.gov/data/reuters/reuters.html.\n",
      "2511  \n",
      "\n",
      "COLLOBERT, WESTON, BOTTOU, KARLEN, KAVUKCUOGLU AND KUKSA\n",
      "can be extremely demanding, and sophisticated approximations are required. More importantly for\n",
      "us, neither work leads to significant word embeddings being reported.\n",
      "Shannon (1951) has estimated the entropy of the English language between 0.6 and 1.3 bits per\n",
      "character by asking human subjects to guess upcoming characters. Cover and King (1978) give\n",
      "a lower bound of 1.25 bits per character using a subtle gambling approach. Meanwhile, using a\n",
      "simple word trigram model, Brown et al. (1992b) reach 1.75 bits per character. Teahan and Cleary\n",
      "(1996) obtain entropies as low as 1.46 bits per character using variable length character n-grams.\n",
      "The human subjects rely of course on all their knowledge of the language and of the world. Can we\n",
      "learn the grammatical structure of the English language and the nature of the world by leveraging\n",
      "the 0.2 bits per character that separate human subjects from simple n-gram models? Since such tasks\n",
      "certainly require high capacity models, obtaining sufficiently small confidence intervals on the test\n",
      "set entropy may require prohibitively large training sets.16 The entropy criterion lacks dynamical\n",
      "range because its numerical value is largely determined by the most frequent phrases. In order to\n",
      "learn syntax, rare but legal phrases are no less significant than common phrases.\n",
      "It is therefore desirable to define alternative training criteria. We propose here to use a pairwise\n",
      "ranking approach (Cohen et al., 1998). We seek a network that computes a higher score when\n",
      "given a legal phrase than when given an incorrect phrase. Because the ranking literature often deals\n",
      "with information retrieval applications, many authors define complex ranking criteria that give more\n",
      "weight to the ordering of the best ranking instances (see Burges et al., 2007; Cle´menc¸on and Vayatis,\n",
      "2007). However, in our case, we do not want to emphasize the most common phrase over the rare\n",
      "but legal phrases. Therefore we use a simple pairwise criterion.\n",
      "We consider a window approach network, as described in Section 3.3.1 and Figure 1, with\n",
      "parameters q which outputs a score f (x) given a window of text x = [w]dwin. We minimize the\n",
      "q 1\n",
      "ranking criterion with respect to q :\n",
      "q 7→ (cid:229) (cid:229) max 0 , 1 − fq (x) + fq (x(w)) , (17)\n",
      "x∈X w∈D\n",
      "n o\n",
      "X D\n",
      "where is the set of all possible text windows with d words coming from our training corpus,\n",
      "win\n",
      "is the dictionary of words, and x(w) denotes the text window obtained by replacing the central word\n",
      "of text window [w]dwin by the word w.\n",
      "1\n",
      "Okanohara and Tsujii (2007) use a related approach to avoiding the entropy criteria using a\n",
      "binary classification approach (correct/incorrect phrase). Their work focuses on using a kernel\n",
      "classifier, and not on learning word embeddings as we do here. Smith and Eisner (2005) also\n",
      "propose a contrastive criterion which estimates the likelihood of the data conditioned to a “negative”\n",
      "neighborhood. They consider various data neighborhoods, including sentences of length d drawn\n",
      "win\n",
      "from\n",
      "Ddwin.\n",
      "Their goal was however to perform well on some tagging task on fully unsupervised\n",
      "data, rather than obtaining generic word embeddings useful for other tasks.\n",
      "4.3 Training Language Models\n",
      "The language model network was trained by stochastic gradient minimization of the ranking crite-\n",
      "rion (17), sampling a sentence-word pair (s, w) at each iteration.\n",
      "16. However, Klein and Manning (2002) describe a rare example of realistic unsupervised grammar induction using a\n",
      "cross-entropy approach on binary-branching parsing trees, that is, by forcing the system to generate a hierarchical\n",
      "representation.\n",
      "2512  \n",
      "\n",
      "NATURAL LANGUAGE PROCESSING (ALMOST) FROM SCRATCH\n",
      "Since training times for such large scale systems are counted in weeks, it is not feasible to\n",
      "try many combinations of hyperparameters. It also makes sense to speed up the training time by\n",
      "initializing new networks with the embeddings computed by earlier networks. In particular, we\n",
      "found it expedient to train a succession of networks using increasingly large dictionaries, each\n",
      "network being initialized with the embeddings of the previous network. Successive dictionary sizes\n",
      "and switching times are chosen arbitrarily. Bengio et al. (2009) provides a more detailed discussion\n",
      "of this, the (as yet, poorly understood) “curriculum” process.\n",
      "For the purposes of model selection we use the process of “breeding”. The idea of breeding\n",
      "is instead of trying a full grid search of possible values (which we did not have enough computing\n",
      "power for) to search for the parameters in analogy to breeding biological cell lines. Within each line,\n",
      "child networks are initialized with the embeddings of their parents and trained on increasingly rich\n",
      "data sets with sometimes different parameters. That is, suppose we have k processors, which is much\n",
      "less than the possible set of parameters one would like to try. One chooses k initial parameter choices\n",
      "from the large set, and trains these on the k processors. In our case, possible parameters to adjust\n",
      "are: the learning rate l , the word embedding dimensions d, number of hidden units n1 and input\n",
      "hu\n",
      "window size d . One then trains each of these models in an online fashion for a certain amount\n",
      "win\n",
      "of time (i.e., a few days), and then selects the best ones using the validation set error rate. That is,\n",
      "breeding decisions were made on the basis of the value of the ranking criterion (17) estimated on\n",
      "a validation set composed of one million words held out from the Wikipedia corpus. In the next\n",
      "breeding iteration, one then chooses another set of k parameters from the possible grid of values\n",
      "that permute slightly the most successful candidates from the previous round. As many of these\n",
      "parameter choices can share weights, we can effectively continue online training retaining some of\n",
      "the learning from the previous iterations.\n",
      "Very long training times make such strategies necessary for the foreseeable future: if we had\n",
      "been given computers ten times faster, we probably would have found uses for data sets ten times\n",
      "bigger. However, we should say we believe that although we ended up with a particular choice of\n",
      "parameters, many other choices are almost equally as good, although perhaps there are others that\n",
      "are better as we could not do a full grid search.\n",
      "In the following subsections, we report results obtained with two trained language models. The\n",
      "results achieved by these two models are representative of those achieved by networks trained on\n",
      "the full corpora.\n",
      "• Language model LM1 has a window size d = 11 and a hidden layer with n1 = 100 units.\n",
      "win hu\n",
      "The embedding layers were dimensioned like those of the supervised networks (Table 5).\n",
      "Model LM1 was trained on our first English corpus (Wikipedia) using successive dictionaries\n",
      "composed of the 5000, 10, 000, 30, 000, 50, 000 and finally 100, 000 most common WSJ\n",
      "words. The total training time was about four weeks.\n",
      "• Language model LM2 has the same dimensions. It was initialized with the embeddings of\n",
      "LM1, and trained for an additional three weeks on our second English corpus\n",
      "(Wikipedia+Reuters) using a dictionary size of 130,000 words.\n",
      "4.4 Embeddings\n",
      "Both networks produce much more appealing word embeddings than in Section 3.5. Table 7 shows\n",
      "the ten nearest neighbors of a few randomly chosen query words for the LM1 model. The syntactic\n",
      "2513  \n",
      "\n",
      "COLLOBERT, WESTON, BOTTOU, KARLEN, KAVUKCUOGLU AND KUKSA\n",
      "FRANCE JESUS XBOX REDDISH SCRATCHED MEGABITS\n",
      "454 1973 6909 11724 29869 87025\n",
      "AUSTRIA GOD AMIGA GREENISH NAILED OCTETS\n",
      "BELGIUM SATI PLAYSTATION BLUISH SMASHED MB/S\n",
      "GERMANY CHRIST MSX PINKISH PUNCHED BIT/S\n",
      "ITALY SATAN IPOD PURPLISH POPPED BAUD\n",
      "GREECE KALI SEGA BROWNISH CRIMPED CARATS\n",
      "SWEDEN INDRA PSNUMBER GREYISH SCRAPED KBIT/S\n",
      "NORWAY VISHNU HD GRAYISH SCREWED MEGAHERTZ\n",
      "EUROPE ANANDA DREAMCAST WHITISH SECTIONED MEGAPIXELS\n",
      "HUNGARY PARVATI GEFORCE SILVERY SLASHED GBIT/S\n",
      "SWITZERLAND GRACE CAPCOM YELLOWISH RIPPED AMPERES\n",
      "Table 7: Word embeddings in the word lookup table of the language model neural network LM1\n",
      "trained with a dictionary of size 100, 000. For each column the queried word is followed\n",
      "by its index in the dictionary (higher means more rare) and its 10 nearest neighbors (using\n",
      "the Euclidean metric, which was chosen arbitrarily).\n",
      "and semantic properties of the neighbors are clearly related to those of the query word. These\n",
      "results are far more satisfactory than those reported in Table 7 for embeddings obtained using purely\n",
      "supervised training of the benchmark NLP tasks.\n",
      "4.5 Semi-supervised Benchmark Results\n",
      "Semi-supervised learning has been the object of much attention during the last few years (see\n",
      "Chapelle et al., 2006). Previous semi-supervised approaches for NLP can be roughly categorized as\n",
      "follows:\n",
      "• Ad-hoc approaches such as Rosenfeld and Feldman (2007) for relation extraction.\n",
      "• Self-training approaches, such as Ueffing et al. (2007) for machine translation, and McClosky\n",
      "et al. (2006) for parsing. These methods augment the labeled training set with examples from\n",
      "the unlabeled data set using the labels predicted by the model itself. Transductive approaches,\n",
      "such as Joachims (1999) for text classification can be viewed as a refined form of self-training.\n",
      "• Parameter sharing approaches such as Ando and Zhang (2005); Suzuki and Isozaki (2008).\n",
      "Ando and Zhang propose a multi-task approach where they jointly train models sharing cer-\n",
      "tain parameters. They train POS and NER models together with a language model (trained on\n",
      "15 million words) consisting of predicting words given the surrounding tokens. Suzuki and\n",
      "Isozaki embed a generative model (Hidden Markov Model) inside a CRF for POS, Chunking\n",
      "and NER. The generative model is trained on one billion words. These approaches should\n",
      "be seen as a linear counterpart of our work. Using multilayer models vastly expands the\n",
      "parameter sharing opportunities (see Section 5).\n",
      "Our approach simply consists of initializing the word lookup tables of the supervised networks\n",
      "with the embeddings computed by the language models. Supervised training is then performed as\n",
      "in Section 3.5. In particular the supervised training stage is free to modify the lookup tables. This\n",
      "sequential approach is computationally convenient because it separates the lengthy training of the\n",
      "2514  \n",
      "\n",
      "NATURAL LANGUAGE PROCESSING (ALMOST) FROM SCRATCH\n",
      "Approach POS CHUNK NER SRL\n",
      "(PWA) (F1) (F1) (F1)\n",
      "Benchmark Systems 97.24 94.29 89.31 77.92\n",
      "NN+WLL 96.31 89.13 79.53 55.40\n",
      "NN+SLL 96.37 90.33 81.47 70.99\n",
      "NN+WLL+LM1 97.05 91.91 85.68 58.18\n",
      "NN+SLL+LM1 97.10 93.65 87.58 73.84\n",
      "NN+WLL+LM2 97.14 92.04 86.96 58.34\n",
      "NN+SLL+LM2 97.20 93.63 88.67 74.15\n",
      "Table 8: Comparison in generalization performance of benchmark NLP systems with our (NN) ap-\n",
      "proach on POS, chunking, NER and SRL tasks. We report results with both the word-level\n",
      "log-likelihood (WLL) and the sentence-level log-likelihood (SLL). We report with (LMn)\n",
      "performance of the networks trained from the language model embeddings (Table 7). Gen-\n",
      "eralization performance is reported in per-word accuracy (PWA) for POS and F1 score for\n",
      "other tasks.\n",
      "language models from the relatively fast training of the supervised networks. Once the language\n",
      "models are trained, we can perform multiple experiments on the supervised networks in a rela-\n",
      "tively short time. Note that our procedure is clearly linked to the (semi-supervised) deep learning\n",
      "procedures of Hinton et al. (2006), Bengio et al. (2007) and Weston et al. (2008).\n",
      "Table 8 clearly shows that this simple initialization significantly boosts the generalization per-\n",
      "formance of the supervised networks for each task. It is worth mentioning the larger language\n",
      "model led to even better performance. This suggests that we could still take advantage of even\n",
      "bigger unlabeled data sets.\n",
      "4.6 Ranking and Language\n",
      "There is a large agreement in the NLP community that syntax is a necessary prerequisite for se-\n",
      "mantic role labeling (Gildea and Palmer, 2002). This is why state-of-the-art semantic role labeling\n",
      "systems thoroughly exploit multiple parse trees. The parsers themselves (Charniak, 2000; Collins,\n",
      "1999) contain considerable prior information about syntax (one can think of this as a kind of in-\n",
      "formed pre-processing).\n",
      "Our system does not use such parse trees because we attempt to learn this information from the\n",
      "unlabeled data set. It is therefore legitimate to question whether our ranking criterion (17) has the\n",
      "conceptual capability to capture such a rich hierarchical information. At first glance, the ranking\n",
      "task appears unrelated to the induction of probabilistic grammars that underly standard parsing\n",
      "algorithms. The lack of hierarchical representation seems a fatal flaw (Chomsky, 1956).\n",
      "However, ranking is closely related to an alternative description of the language structure: op-\n",
      "erator grammars (Harris, 1968). Instead of directly studying the structure of a sentence, Harris\n",
      "defines an algebraic structure on the space of all sentences. Starting from a couple of elementary\n",
      "sentence forms, sentences are described by the successive application of sentence transformation\n",
      "operators. The sentence structure is revealed as a side effect of the successive transformations.\n",
      "Sentence transformations can also have a semantic interpretation.\n",
      "2515  \n",
      "\n",
      "COLLOBERT, WESTON, BOTTOU, KARLEN, KAVUKCUOGLU AND KUKSA\n",
      "In the spirit of structural linguistics, Harris describes procedures to discover sentence trans-\n",
      "formation operators by leveraging the statistical regularities of the language. Such procedures are\n",
      "obviously useful for machine learning approaches. In particular, he proposes a test to decide whether\n",
      "two sentences forms are semantically related by a transformation operator. He first defines a ranking\n",
      "criterion (Harris, 1968, Section 4.1):\n",
      "“Starting for convenience with very short sentence forms, say ABC, we choose a\n",
      "particular word choice for all the classes, say B C , except one, in this case A; for every\n",
      "q q\n",
      "pair of members A , A of that word class we ask how the sentence formed with one\n",
      "i j\n",
      "of the members, that is, A B C compares as to acceptability with the sentence formed\n",
      "i q q\n",
      "with the other member, that is, A B C .”\n",
      "j q q\n",
      "These gradings are then used to compare sentence forms:\n",
      "“It now turns out that, given the graded n-tuples of words for a particular sentence\n",
      "form, we can find other sentences forms of the same word classes in which the same\n",
      "n-tuples of words produce the same grading of sentences.”\n",
      "This is an indication that these two sentence forms exploit common words with the same syntac-\n",
      "tic function and possibly the same meaning. This observation forms the empirical basis for the\n",
      "construction of operator grammars that describe real-world natural languages such as English.\n",
      "Therefore there are solid reasons to believe that the ranking criterion (17) has the conceptual\n",
      "potential to capture strong syntactic and semantic information. On the other hand, the structure\n",
      "of our language models is probably too restrictive for such goals, and our current approach only\n",
      "exploits the word embeddings discovered during training.\n",
      "5. Multi-Task Learning\n",
      "It is generally accepted that features trained for one task can be useful for related tasks. This idea\n",
      "was already exploited in the previous section when certain language model features, namely the\n",
      "word embeddings, were used to initialize the supervised networks.\n",
      "Multi-task learning (MTL) leverages this idea in a more systematic way. Models for all tasks\n",
      "of interests are jointly trained with an additional linkage between their trainable parameters in the\n",
      "hope of improving the generalization error. This linkage can take the form of a regularization\n",
      "term in the joint cost function that biases the models towards common representations. A much\n",
      "simpler approach consists in having the models share certain parameters defined a priori. Multi-\n",
      "task learning has a long history in machine learning and neural networks. Caruana (1997) gives a\n",
      "good overview of these past efforts.\n",
      "5.1 Joint Decoding versus Joint Training\n",
      "Multitask approaches do not necessarily involve joint training. For instance, modern speech recog-\n",
      "nition systems use Bayes rule to combine the outputs of an acoustic model trained on speech data\n",
      "and a language model trained on phonetic or textual corpora (Jelinek, 1976). This joint decoding\n",
      "approach has been successfully applied to structurally more complex NLP tasks. Sutton and McCal-\n",
      "lum (2005b) obtain improved results by combining the predictions of independently trained CRF\n",
      "models using a joint decoding process at test time that requires more sophisticated probabilistic\n",
      "2516  \n",
      "\n",
      "NATURAL LANGUAGE PROCESSING (ALMOST) FROM SCRATCH\n",
      "inference techniques. On the other hand, Sutton and McCallum (2005a) obtain results somewhat\n",
      "below the state-of-the-art using joint decoding for SRL and syntactic parsing. Musillo and Merlo\n",
      "(2006) also describe a negative result at the same joint task.\n",
      "Joint decoding invariably works by considering additional probabilistic dependency paths be-\n",
      "tween the models. Therefore it defines an implicit supermodel that describes all the tasks in the\n",
      "same probabilistic framework. Separately training a submodel only makes sense when the train-\n",
      "ing data blocks these additional dependency paths (in the sense of d-separation, Pearl, 1988). This\n",
      "implies that, without joint training, the additional dependency paths cannot directly involve unob-\n",
      "served variables. Therefore, the natural idea of discovering common internal representations across\n",
      "tasks requires joint training.\n",
      "Joint training is relatively straightforward when the training sets for the individual tasks con-\n",
      "tain the same patterns with different labels. It is then sufficient to train a model that computes\n",
      "multiple outputs for each pattern (Suddarth and Holden, 1991). Using this scheme, Sutton et al.\n",
      "(2007) demonstrate improvements on POS tagging and noun-phrase chunking using jointly trained\n",
      "CRFs. However the joint labeling requirement is a limitation because such data is not often avail-\n",
      "able. Miller et al. (2000) achieves performance improvements by jointly training NER, parsing,\n",
      "and relation extraction in a statistical parsing model. The joint labeling requirement problem was\n",
      "weakened using a predictor to fill in the missing annotations.\n",
      "Ando and Zhang (2005) propose a setup that works around the joint labeling requirements. They\n",
      "define linear models of the form f (x) = w⊤ F (x) + v⊤ YQ (x) where f is the classifier for the i-th\n",
      "i i i i\n",
      "task with parameters w and v . Notations F (x) and Y (x) represent engineered features for the pat-\n",
      "i i\n",
      "tern x. Matrix Q maps the Y (x) features into a low dimensional subspace common across all tasks.\n",
      "Each task is trained using its own examples without a joint labeling requirement. The learning pro-\n",
      "cedure alternates the optimization of w and v for each task, and the optimization of Q to minimize\n",
      "i i\n",
      "the average loss for all examples in all tasks. The authors also consider auxiliary unsupervised tasks\n",
      "for predicting substructures. They report excellent results on several tasks, including POS and NER.\n",
      "5.2 Multi-Task Benchmark Results\n",
      "Table 9 reports results obtained by jointly trained models for the POS, CHUNK, NER and SRL tasks\n",
      "using the same setup as Section 4.5. We trained jointly POS, CHUNK and NER using the window\n",
      "approach network. As we mentioned earlier, SRL can be trained only with the sentence approach\n",
      "network, due to long-range dependencies related to the verb predicate. We thus performed additional\n",
      "experiments, where all four tasks were trained using the sentence approach network. In both cases,\n",
      "all models share the lookup table parameters (2). The parameters of the first linear layers (4) were\n",
      "shared in the window approach case (see Figure 5), and the first the convolution layer parameters (6)\n",
      "were shared in the sentence approach networks.\n",
      "For the window approach, best results were obtained by enlarging the first hidden layer size to\n",
      "n1 = 500 (chosen by validation) in order to account for its shared responsibilities. We used the\n",
      "hu\n",
      "same architecture as SRL for the sentence approach network. The word embedding dimension was\n",
      "kept constant d0 = 50 in order to reuse the language models of Section 4.5.\n",
      "Training was achieved by minimizing the loss averaged across all tasks. This is easily achieved\n",
      "with stochastic gradient by alternatively picking examples for each task and applying (16) to all the\n",
      "parameters of the corresponding model, including the shared parameters. Note that this gives each\n",
      "task equal weight. Since each task uses the training sets described in Table 1, it is worth noticing\n",
      "2517  \n",
      "\n",
      "COLLOBERT, WESTON, BOTTOU, KARLEN, KAVUKCUOGLU AND KUKSA\n",
      "Lookup Table Lookup Table\n",
      "xxxxxxxxxx xxxxxxxxxx xxxxxxxxxx xxxxxxxxxx xxxxxxxxxx xxxxxxxxxx\n",
      "LT\n",
      "xxxxxxxxxx\n",
      ".\n",
      ".\n",
      ".W 1\n",
      "xxxxxxxxxx xxxxxxxxxx xxxxxxxxxx xxxxxxxxxx xxxxxxxxxx\n",
      "xxxxx xxxxx xxxxx xxxxx xxxxx xxxxxLT xxxxx\n",
      "W K\n",
      "xxxxx xxxxx xxxxx xxxxx xxxxx\n",
      "Linear Linear\n",
      "M 1 × ·\n",
      "n1\n",
      "hu\n",
      "n1\n",
      "hu\n",
      "HardTanh HardTanh\n",
      "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "Linear Linear\n",
      "M 2 × · M 2 × ·\n",
      "(t1) (t2)\n",
      "n2 hu,(t1) = #tags n2 hu,(t2) = #tags\n",
      "Task 1 Task 2\n",
      "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "Figure 5: Example of multitasking with NN. Task 1 and Task 2 are two tasks trained with the\n",
      "window approach architecture presented in Figure 1. Lookup tables as well as the first\n",
      "hidden layer are shared. The last layer is task specific. The principle is the same with\n",
      "more than two tasks.\n",
      "that examples can come from quite different data sets. The generalization performance for each\n",
      "task was measured using the traditional testing data specified in Table 1. Fortunately, none of the\n",
      "training and test sets overlap across tasks.\n",
      "It is worth mentioning that MTL can produce a single unified network that performs well for\n",
      "all these tasks using the sentence approach. However this unified network only leads to marginal\n",
      "improvements over using a separate network for each task: the most important MTL task appears to\n",
      "be the unsupervised learning of the word embeddings. As explained before, simple computational\n",
      "considerations led us to train the POS, Chunking, and NER tasks using the window approach. The\n",
      "baseline results in Table 9 also show that using the sentence approach for the POS, Chunking, and\n",
      "NER tasks yields no performance improvement (or degradation) over the window approach. The\n",
      "next section shows we can leverage known correlations between tasks in more direct manner.\n",
      "6. The Temptation\n",
      "Results so far have been obtained by staying (almost17) true to our from scratch philosophy. We\n",
      "have so far avoided specializing our architecture for any task, disregarding a lot of useful a priori\n",
      "17. We did some basic preprocessing of the raw input words as described in Section 3.5, hence the “almost” in the title of\n",
      "this article. A completely from scratch approach would presumably not know anything about words at all and would\n",
      "work from letters only (or, taken to a further extreme, from speech or optical character recognition, as humans do).\n",
      "2518  \n",
      "\n",
      "NATURAL LANGUAGE PROCESSING (ALMOST) FROM SCRATCH\n",
      "Approach POS CHUNK NER SRL\n",
      "(PWA) (F1) (F1) (F1)\n",
      "Benchmark Systems 97.24 94.29 89.31 77.92\n",
      "Window Approach\n",
      "NN+SLL+LM2 97.20 93.63 88.67 –\n",
      "NN+SLL+LM2+MTL 97.22 94.10 88.62 –\n",
      "Sentence Approach\n",
      "NN+SLL+LM2 97.12 93.37 88.78 74.15\n",
      "NN+SLL+LM2+MTL 97.22 93.75 88.27 74.29\n",
      "Table 9: Effect of multi-tasking on our neural architectures. We trained POS, CHUNK NER in a\n",
      "MTL way, both for the window and sentence network approaches. SRL was only included\n",
      "in the sentence approach joint training. As a baseline, we show previous results of our\n",
      "window approach system, as well as additional results for our sentence approach system,\n",
      "when trained separately on each task. Benchmark system performance is also given for\n",
      "comparison.\n",
      "Approach POS CHUNK NER SRL\n",
      "(PWA) (F1) (F1)\n",
      "Benchmark Systems 97.24 94.29 89.31 77.92\n",
      "NN+SLL+LM2 97.20 93.63 88.67 74.15\n",
      "NN+SLL+LM2+Suffix2 97.29 – – –\n",
      "NN+SLL+LM2+Gazetteer – – 89.59 –\n",
      "NN+SLL+LM2+POS – 94.32 88.67 –\n",
      "NN+SLL+LM2+CHUNK – – – 74.72\n",
      "Table 10: Comparison in generalization performance of benchmark NLP systems with our neural\n",
      "networks (NNs) using increasing task-specific engineering. We report results obtained\n",
      "with a network trained without the extra task-specific features (Section 5) and with the\n",
      "extra task-specific features described in Section 6. The POS network was trained with\n",
      "two character word suffixes; the NER network was trained using the small CoNLL 2003\n",
      "gazetteer; the CHUNK and NER networks were trained with additional POS features;\n",
      "and finally, the SRL network was trained with additional CHUNK features.\n",
      "NLP knowledge. We have shown that, thanks to large unlabeled data sets, our generic neural net-\n",
      "works can still achieve close to state-of-the-art performance by discovering useful features. This\n",
      "section explores what happens when we increase the level of task-specific engineering in our sys-\n",
      "tems by incorporating some common techniques from the NLP literature. We often obtain further\n",
      "improvements. These figures are useful to quantify how far we went by leveraging large data sets\n",
      "instead of relying on a priori knowledge.\n",
      "2519  \n",
      "\n",
      "COLLOBERT, WESTON, BOTTOU, KARLEN, KAVUKCUOGLU AND KUKSA\n",
      "6.1 Suffix Features\n",
      "Word suffixes in many western languages are strong predictors of the syntactic function of the word\n",
      "and therefore can benefit the POS system. For instance, Ratnaparkhi (1996) uses inputs representing\n",
      "word suffixes and prefixes up to four characters. We achieve this in the POS task by adding discrete\n",
      "word features (Section 3.2.1) representing the last two characters of every word. The size of the\n",
      "suffix dictionary was 455. This led to a small improvement of the POS performance (Table 10,\n",
      "row NN+SLL+LM2+Suffix2). We also tried suffixes obtained with the Porter (1980) stemmer and\n",
      "obtained the same performance as when using two character suffixes.\n",
      "6.2 Gazetteers\n",
      "State-of-the-art NER systems often use a large dictionary containing well known named entities\n",
      "(e.g., Florian et al., 2003). We restricted ourselves to the gazetteer provided by the CoNLL chal-\n",
      "lenge, containing 8, 000 locations, person names, organizations, and miscellaneous entities. We\n",
      "trained a NER network with 4 additional word features indicating (feature “on” or “off”) whether\n",
      "the word is found in the gazetteer under one of these four categories. The gazetteer includes not\n",
      "only words, but also chunks of words. If a sentence chunk is found in the gazetteer, then all words in\n",
      "the chunk have their corresponding gazetteer feature turned to “on”. The resulting system displays\n",
      "a clear performance improvement (Table 10, row NN+SLL+LM2+Gazetteer), slightly outperforming\n",
      "the baseline. A plausible explanation of this large boost over the network using only the language\n",
      "model is that gazetteers include word chunks, while we use only the word representation of our\n",
      "language model. For example, “united” and “bicycle” seen separately are likely to be non-entities,\n",
      "while “united bicycle” might be an entity, but catching it would require higher level representations\n",
      "of our language model.\n",
      "6.3 Cascading\n",
      "When one considers related tasks, it is reasonable to assume that tags obtained for one task can be\n",
      "useful for taking decisions in the other tasks. Conventional NLP systems often use features obtained\n",
      "from the output of other preexisting NLP systems. For instance, Shen and Sarkar (2005) describe a\n",
      "chunking system that uses POS tags as input; Florian et al. (2003) describes a NER system whose\n",
      "inputs include POS and CHUNK tags, as well as the output of two other NER classifiers. State-of-\n",
      "the-art SRL systems exploit parse trees (Gildea and Palmer, 2002; Punyakanok et al., 2005), related\n",
      "to CHUNK tags, and built using POS tags (Charniak, 2000; Collins, 1999).\n",
      "Table 10 reports results obtained for the CHUNK and NER tasks by adding discrete word fea-\n",
      "tures (Section 3.2.1) representing the POS tags. In order to facilitate comparisons, instead of using\n",
      "the more accurate tags from our POS network, we use for each task the POS tags provided by the\n",
      "corresponding CoNLL challenge. We also report results obtained for the SRL task by adding word\n",
      "features representing the CHUNK tags (also provided by the CoNLL challenge). We consistently\n",
      "obtain moderate improvements.\n",
      "6.4 Ensembles\n",
      "Constructing ensembles of classifiers is a proven way to trade computational efficiency for general-\n",
      "ization performance (Bell et al., 2007). Therefore it is not surprising that many NLP systems achieve\n",
      "state-of-the-art performance by combining the outputs of multiple classifiers. For instance, Kudo\n",
      "2520  \n",
      "\n",
      "NATURAL LANGUAGE PROCESSING (ALMOST) FROM SCRATCH\n",
      "Approach POS CHUNK NER\n",
      "(PWA) (F1) (F1)\n",
      "Benchmark Systems 97.24 94.29 89.31\n",
      "NN+SLL+LM2+POS worst 97.29 93.99 89.35\n",
      "NN+SLL+LM2+POS mean 97.31 94.17 89.65\n",
      "NN+SLL+LM2+POS best 97.35 94.32 89.86\n",
      "NN+SLL+LM2+POS voting ensemble 97.37 94.34 89.70\n",
      "NN+SLL+LM2+POS joined ensemble 97.30 94.35 89.67\n",
      "Table 11: Comparison in generalization performance for POS, CHUNK and NER tasks of the net-\n",
      "works obtained using by combining ten training runs with different initialization.\n",
      "and Matsumoto (2001) use an ensemble of classifiers trained with different tagging conventions (see\n",
      "Section 3.3.3). Winning a challenge is of course a legitimate objective. Yet it is often difficult to\n",
      "figure out which ideas are most responsible for the state-of-the-art performance of a large ensemble.\n",
      "Because neural networks are nonconvex, training runs with different initial parameters usually\n",
      "give different solutions. Table 11 reports results obtained for the CHUNK and NER task after ten\n",
      "training runs with random initial parameters. Voting the ten network outputs on a per tag basis\n",
      "(“voting ensemble”) leads to a small improvement over the average network performance. We\n",
      "have also tried a more sophisticated ensemble approach: the ten network output scores (before\n",
      "sentence-level likelihood) were combined with an additional linear layer (4) and then fed to a new\n",
      "sentence-level likelihood (13). The parameters of the combining layers were then trained on the\n",
      "existing training set, while keeping the ten networks fixed (“joined ensemble”). This approach did\n",
      "not improve on simple voting.\n",
      "These ensembles come of course at the expense of a ten fold increase of the running time. On\n",
      "the other hand, multiple training times could be improved using smart sampling strategies (Neal,\n",
      "1996).\n",
      "We can also observe that the performance variability among the ten networks is not very large.\n",
      "The local minima found by the training algorithm are usually good local minima, thanks to the\n",
      "oversized parameter space and to the noise induced by the stochastic gradient procedure (LeCun\n",
      "et al., 1998). In order to reduce the variance in our experimental results, we always use the same\n",
      "initial parameters for networks trained on the same task (except of course for the results reported in\n",
      "Table 11.)\n",
      "6.5 Parsing\n",
      "Gildea and Palmer (2002) and Punyakanok et al. (2005) offer several arguments suggesting that\n",
      "syntactic parsing is a necessary prerequisite for the SRL task. The CoNLL 2005 SRL benchmark\n",
      "task provides parse trees computed using both the Charniak (2000) and Collins (1999) parsers.\n",
      "State-of-the-art systems often exploit additional parse trees such as the k top ranking parse trees\n",
      "(Koomen et al., 2005; Haghighi et al., 2005).\n",
      "In contrast our SRL networks so far do not use parse trees at all. They rely instead on internal\n",
      "representations transferred from a language model trained with an objective function that captures\n",
      "2521  \n",
      "\n",
      "COLLOBERT, WESTON, BOTTOU, KARLEN, KAVUKCUOGLU AND KUKSA\n",
      "S\n",
      "NP NP VP\n",
      "NP PP\n",
      "LEVEL 0 The luxury auto maker last year sold\n",
      "b-np i-np i-np e-np b-np e-np s-vp NP\n",
      "1,214 cars in\n",
      "b-np e-np s-pp\n",
      "the U.S.\n",
      "b-np e-np\n",
      "S\n",
      "VP\n",
      "The luxury auto maker last year\n",
      "LEVEL 1 o o o o o o sold 1,214 cars PP\n",
      "b-vp i-vp e-vp\n",
      "in the U.S.\n",
      "b-pp i-pp e-pp\n",
      "S\n",
      "VP\n",
      "LEVEL 2 The luxury auto maker last year\n",
      "o o o o o o\n",
      "sold 1,214 cars in the U.S.\n",
      "b-vp i-vp i-vp i-vp i-vp e-vp\n",
      "Figure 6: Charniak parse tree for the sentence “The luxury auto maker last year sold 1,214 cars\n",
      "in the U.S.”. Level 0 is the original tree. Levels 1 to 4 are obtained by successively\n",
      "collapsing terminal tree branches. For each level, words receive tags describing the seg-\n",
      "ment associated with the corresponding leaf. All words receive tag “O” at level 3 in this\n",
      "example.\n",
      "a lot of syntactic information (see Section 4.6). It is therefore legitimate to question whether this\n",
      "approach is an acceptable lightweight replacement for parse trees.\n",
      "We answer this question by providing parse tree information as additional input features to our\n",
      "system.18 We have limited ourselves to the Charniak parse tree provided with the CoNLL 2005 data.\n",
      "Considering that a node in a syntactic parse tree assigns a label to a segment of the parsed sentence,\n",
      "we propose a way to feed (partially) this labeled segmentation to our network, through additional\n",
      "lookup tables. Each of these lookup tables encode labeled segments of each parse tree level (up to\n",
      "a certain depth). The labeled segments are fed to the network following a IOBES tagging scheme\n",
      "(see Sections 3.3.3 and 3.2.1). As there are 40 different phrase labels in WSJ, each additional tree-\n",
      "related lookup tables has 161 entries (40 × 4 + 1) corresponding to the IBES segment tags, plus the\n",
      "extra O tag.\n",
      "We call level 0 the information associated with the leaves of the original Charniak parse tree.\n",
      "The lookup table for level 0 encodes the corresponding IOBES phrase tags for each words. We\n",
      "obtain levels 1 to 4 by repeatedly trimming the leaves as shown in Figure 6. We labeled “O” words\n",
      "belonging to the root node “S”, or all words of the sentence if the root itself has been trimmed.\n",
      "Experiments were performed using the LM2 language model using the same network archi-\n",
      "tectures (see Table 5) and using additional lookup tables of dimension 5 for each parse tree level.\n",
      "Table 12 reports the performance improvements obtained by providing increasing levels of parse\n",
      "18. In a more recent work (Collobert, 2011), we propose an extension of this approach for the generation of full syntactic\n",
      "parse trees, using a recurrent version of our architecture.\n",
      "2522  \n",
      "\n",
      "NATURAL LANGUAGE PROCESSING (ALMOST) FROM SCRATCH\n",
      "Approach SRL\n",
      "(valid) (test)\n",
      "Benchmark System (six parse trees) 77.35 77.92\n",
      "Benchmark System (top Charniak parse tree only) 74.76 –\n",
      "NN+SLL+LM2 72.29 74.15\n",
      "NN+SLL+LM2+Charniak (level 0 only) 74.44 75.65\n",
      "NN+SLL+LM2+Charniak (levels 0 & 1) 74.50 75.81\n",
      "NN+SLL+LM2+Charniak (levels 0 to 2) 75.09 76.05\n",
      "NN+SLL+LM2+Charniak (levels 0 to 3) 75.12 75.89\n",
      "NN+SLL+LM2+Charniak (levels 0 to 4) 75.42 76.06\n",
      "NN+SLL+LM2+CHUNK – 74.72\n",
      "NN+SLL+LM2+PT0 – 75.49\n",
      "Table 12: Generalization performance on the SRL task of our NN architecture compared with the\n",
      "benchmark system. We show performance of our system fed with different levels of depth\n",
      "of the Charniak parse tree. We report previous results of our architecture with no parse\n",
      "tree as a baseline. Koomen et al. (2005) report test and validation performance using six\n",
      "parse trees, as well as validation performance using only the top Charniak parse tree. For\n",
      "comparison purposes, we hence also report validation performance. Finally, we report\n",
      "our performance with the CHUNK feature, and compare it against a level 0 feature PT0\n",
      "obtained by our network.\n",
      "tree information. Level 0 alone increases the F1 score by almost 1.5%. Additional levels yield\n",
      "diminishing returns. The top performance reaches 76.06% F1 score. This is not too far from the\n",
      "state-of-the-art system which we note uses six parse trees instead of one. Koomen et al. (2005) also\n",
      "report a 74.76% F1 score on the validation set using only the Charniak parse tree. Using the first\n",
      "three parse tree levels, we reach this performance on the validation set. These results corroborate\n",
      "findings in the NLP literature (Gildea and Palmer, 2002; Punyakanok et al., 2005) showing that\n",
      "parsing is important for the SRL task.\n",
      "We also reported in Table 12 our previous performance obtained with the CHUNK feature (see\n",
      "Table 10). It is surprising to observe that adding chunking features into the semantic role labeling\n",
      "network performs significantly worse than adding features describing the level 0 of the Charniak\n",
      "parse tree (Table 12). Indeed, if we ignore the label prefixes “BIES” defining the segmentation,\n",
      "the parse tree leaves (at level 0) and the chunking have identical labeling. However, the parse trees\n",
      "identify leaf sentence segments that are often smaller than those identified by the chunking tags,\n",
      "as shown by Hollingshead et al. (2005).19 Instead of relying on Charniak parser, we chose to train\n",
      "a second chunking network to identify the segments delimited by the leaves of the Penn Treebank\n",
      "parse trees (level 0). Our network achieved 92.25% F1 score on this task (we call it PT0), while we\n",
      "evaluated Charniak performance as 91.94% on the same task. As shown in Table 12, feeding our\n",
      "19. As in Hollingshead et al. (2005), consider the sentence and chunk labels “(NP They) (VP are starting to buy) (NP\n",
      "growth stocks)”. The parse tree can be written as “(S (NP They) (VP are (VP starting (S (VP to (VP buy (NP growth\n",
      "stocks)))))))”. The tree leaves segmentation is thus given by “(NP They) (VP are) (VP starting) (VP to) (VP buy) (NP\n",
      "growth stocks)”.\n",
      "2523  \n",
      "\n",
      "COLLOBERT, WESTON, BOTTOU, KARLEN, KAVUKCUOGLU AND KUKSA\n",
      "own PT0 prediction into the SRL system gives similar performance to using Charniak predictions,\n",
      "and is consistently better than the CHUNK feature.\n",
      "6.6 Word Representations\n",
      "We have described how we induced useful word embeddings by applying our architecture to a\n",
      "language modeling task trained using a large amount of unlabeled text data. These embeddings\n",
      "improve the generalization performance on all tasks (Section 4.) The literature describes other ways\n",
      "to induce word representations. Mnih and Hinton (2007) proposed a related language model ap-\n",
      "proach inspired from Restricted Boltzmann Machines. However, word representations are perhaps\n",
      "more commonly inferred from n-gram language modeling rather than purely continuous language\n",
      "models. One popular approach is the Brown clustering algorithm (Brown et al., 1992a), which\n",
      "builds hierarchical word clusters by maximizing the bigram’s mutual information. The induced\n",
      "word representation has been used with success in a wide variety of NLP tasks, including POS\n",
      "(Schu¨tze, 1995), NER (Miller et al., 2004; Ratinov and Roth, 2009), or parsing (Koo et al., 2008).\n",
      "Other related approaches exist, like phrase clustering (Lin and Wu, 2009) which has been shown to\n",
      "work well for NER. Finally, Huang and Yates (2009) have recently proposed a smoothed language\n",
      "modeling approach based on a Hidden Markov Model, with success on POS and Chunking tasks.\n",
      "While a comparison of all these word representations is beyond the scope of this paper, it is\n",
      "rather fair to question the quality of our word embeddings compared to a popular NLP approach.\n",
      "In this section, we report a comparison of our word embeddings against Brown clusters, when used\n",
      "as features into our neural network architecture. We report as baseline previous results where our\n",
      "word embeddings are fine-tuned for each task. We also report performance when our embeddings\n",
      "are kept fixed during task-specific training. Since convex machine learning algorithms are common\n",
      "practice in NLP, we finally report performances for the convex version of our architecture.\n",
      "For the convex experiments, we considered the linear version of our neural networks (instead of\n",
      "having several linear layers interleaved with a non-linearity). While we always picked the sentence\n",
      "approach for SRL, we had to consider the window approach in this particular convex setup, as the\n",
      "sentence approach network (see Figure 2) includes a Max layer. Having only one linear layer in our\n",
      "neural network is not enough to make our architecture convex: all lookup-tables (for each discrete\n",
      "feature) must also be fixed. The word-lookup table is simply fixed to the embeddings obtained from\n",
      "our language model LM2. All other discrete feature lookup-tables (caps, POS, Brown Clusters...)\n",
      "are fixed to a standard sparse representation. Using the notation introduced in Section 3.2.1, if LT\n",
      "Wk\n",
      "is the lookup-table of the kth discrete feature, we have W k ∈ R|Dk|×|Dk| and the representation of the\n",
      "discrete input w is obtained with:\n",
      "T\n",
      "LT (w) = hW ki1 = 0, · · · 0, 1 , 0, · · · 0 . (18)\n",
      "Wk w\n",
      "at index w\n",
      "(cid:18) (cid:19)\n",
      "Training our architecture in this convex setup with the sentence-level likelihood (13) corresponds\n",
      "to training a CRF. In that respect, these convex experiments show the performance of our word\n",
      "embeddings in a classical NLP framework.\n",
      "Following the Ratinov and Roth (2009) and Koo et al. (2008) setups, we generated 1, 000 Brown\n",
      "clusters using the implementation20 from Liang (2005). To make the comparison fair, the clusters\n",
      "were first induced on the concatenation of Wikipedia and Reuters data sets, as we did in Section 4\n",
      "20. Available at http://www.eecs.berkeley.edu/˜pliang/software.\n",
      "2524  \n",
      "\n",
      "NATURAL LANGUAGE PROCESSING (ALMOST) FROM SCRATCH\n",
      "Approach POS CHUNK NER SRL\n",
      "(PWA) (F1) (F1) (F1)\n",
      "Non-convex Approach\n",
      "LM2 (non-linear NN) 97.29 94.32 89.59 76.06\n",
      "LM2 (non-linear NN, fixed embeddings) 97.10 94.45 88.79 72.24\n",
      "Brown Clusters (non-linear NN, 130K words) 96.92 94.35 87.15 72.09\n",
      "Brown Clusters (non-linear NN, all words) 96.81 94.21 86.68 71.44\n",
      "Convex Approach\n",
      "LM2 (linear NN, fixed embeddings) 96.69 93.51 86.64 59.11\n",
      "Brown Clusters (linear NN, 130K words) 96.56 94.20 86.46 51.54\n",
      "Brown Clusters (linear NN, all words) 96.28 94.22 86.63 56.42\n",
      "Table 13: Generalization performance of our neural network architecture trained with our language\n",
      "model (LM2) word embeddings, and with the word representations derived from the\n",
      "Brown Clusters. As before, all networks are fed with a capitalization feature. Addition-\n",
      "ally, POS is using a word suffix of size 2 feature, CHUNK is fed with POS, NER uses\n",
      "the CoNLL 2003 gazetteer, and SRL is fed with levels 1–5 of the Charniak parse tree, as\n",
      "well as a verb position feature. We report performance with both convex and non-convex\n",
      "architectures (300 hidden units for all tasks, with an additional 500 hidden units layer for\n",
      "SRL). We also provide results for Brown Clusters induced with a 130K word dictionary,\n",
      "as well as Brown Clusters induced with all words of the given tasks.\n",
      "for training our largest language model LM2, using a 130K word dictionary. This dictionary covers\n",
      "about 99% of the words in the training set of each task. To cover the last 1%, we augmented the\n",
      "dictionary with the missing words (reaching about 140K words) and induced Brown Clusters using\n",
      "the concatenation of WSJ, Wikipedia, and Reuters.\n",
      "The Brown clustering approach is hierarchical and generates a binary tree of clusters. Each\n",
      "word in the vocabulary is assigned to a node in the tree. Features are extracted from this tree by\n",
      "considering the path from the root to the node containing the word of interest. Following Ratinov &\n",
      "Roth, we picked as features the path prefixes of size 4, 6, 10 and 20. In the non-convex experiments,\n",
      "we fed these four Brown Cluster features to our architecture using four different lookup tables,\n",
      "replacing our word lookup table. The size of the lookup tables was chosen to be 12 by validation. In\n",
      "the convex case, we used the classical sparse representation (18), as for any other discrete feature.\n",
      "We first report in Table 13 generalization performance of our best non-convex networks trained\n",
      "with our LM2 language model and with Brown Cluster features. Our embeddings perform at least\n",
      "as well as Brown Clusters. Results are more mitigated in a convex setup. For most tasks, going\n",
      "non-convex is better for both word representation types. In general, “fine-tuning” our embeddings\n",
      "for each task also gives an extra boost. Finally, using a better word coverage with Brown Clusters\n",
      "(“all words” instead of “130K words” in Table 13) did not help.\n",
      "More complex features could be possibly combined instead of using a non-linear model. For\n",
      "instance, Turian et al. (2010) performed a comparison of Brown Clusters and embeddings trained\n",
      "in the same spirit as ours21, with additional features combining labels and tokens. We believe this\n",
      "21. However they did not reach our embedding performance. There are several differences in how they trained their\n",
      "models that might explain this. Firstly, they may have experienced difficulties because they train 50-dimensional\n",
      "2525  \n",
      "\n",
      "COLLOBERT, WESTON, BOTTOU, KARLEN, KAVUKCUOGLU AND KUKSA\n",
      "Task Features\n",
      "POS Suffix of size 2\n",
      "CHUNK POS\n",
      "NER CoNLL 2003 gazetteer\n",
      "PT0 POS\n",
      "SRL PT0, verb position\n",
      "Table 14: Features used by SENNA implementation, for each task. In addition, all tasks use “low\n",
      "caps word” and “caps” features.\n",
      "Task Benchmark SENNA\n",
      "Part of Speech (POS) (Accuracy) 97.24 % 97.29 %\n",
      "Chunking (CHUNK) (F1) 94.29 % 94.32 %\n",
      "Named Entity Recognition (NER) (F1) 89.31 % 89.59 %\n",
      "Parse Tree level 0 (PT0) (F1) 91.94 % 92.25 %\n",
      "Semantic Role Labeling (SRL) (F1) 77.92 % 75.49 %\n",
      "Table 15: Performance of the engineered sweet spot (SENNA) on various tagging tasks. The PT0\n",
      "task replicates the sentence segmentation of the parse tree leaves. The corresponding\n",
      "benchmark score measures the quality of the Charniak parse tree leaves relative to the\n",
      "Penn Treebank gold parse trees.\n",
      "type of comparison should be taken with care, as combining a given feature with different word\n",
      "representations might not have the same effect on each word representation.\n",
      "6.7 Engineering a Sweet Spot\n",
      "We implemented a standalone version of our architecture, written in the C language. We gave\n",
      "the name “SENNA” (Semantic/syntactic Extraction using a Neural Network Architecture) to the\n",
      "resulting system. The parameters of each architecture are the ones described in Table 5. All the\n",
      "networks were trained separately on each task using the sentence-level likelihood (SLL). The word\n",
      "embeddings were initialized to LM2 embeddings, and then fine-tuned for each task. We summarize\n",
      "features used by our implementation in Table 14, and we report performance achieved on each task\n",
      "in Table 15. The runtime version22 contains about 2500 lines of C code, runs in less than 150MB\n",
      "of memory, and needs less than a millisecond per word to compute all the tags. Table 16 compares\n",
      "the tagging speeds for our system and for the few available state-of-the-art systems: the Toutanova\n",
      "et al. (2003) POS tagger23, the Shen et al. (2007) POS tagger24 and the Koomen et al. (2005) SRL\n",
      "embeddings for 269K distinct words using a comparatively small training set (RCV1, 37M words), unlikely to contain\n",
      "enough instances of the rare words. Secondly, they predict the correctness of the final word of each window instead\n",
      "of the center word (Turian et al., 2010), effectively restricting the model to unidirectional prediction. Finally, they do\n",
      "not fine tune their embeddings after unsupervised training.\n",
      "22. Available at http://ml.nec-labs.com/senna.\n",
      "23. Available at http://nlp.stanford.edu/software/tagger.shtml. We picked the 3.0 version (May 2010).\n",
      "24. Available at http://www.cis.upenn.edu/˜xtag/spinal.\n",
      "2526  \n",
      "\n",
      "NATURAL LANGUAGE PROCESSING (ALMOST) FROM SCRATCH\n",
      "POS System RAM (MB) Time (s)\n",
      "Toutanova et al. (2003) 800 64\n",
      "Shen et al. (2007) 2200 833\n",
      "SENNA 32 4\n",
      "SRL System RAM (MB) Time (s)\n",
      "Koomen et al. (2005) 3400 6253\n",
      "SENNA 124 51\n",
      "Table 16: Runtime speed and memory consumption comparison between state-of-the-art systems\n",
      "and our approach (SENNA). We give the runtime in seconds for running both the POS\n",
      "and SRL taggers on their respective testing sets. Memory usage is reported in megabytes.\n",
      "system.25 All programs were run on a single 3GHz Intel core. The POS taggers were run with\n",
      "Sun Java 1.6 with a large enough memory allocation to reach their top tagging speed. The beam\n",
      "size of the Shen tagger was set to 3 as recommended in the paper. Regardless of implementation\n",
      "differences, it is clear that our neural networks run considerably faster. They also require much less\n",
      "memory. Our POS and SRL tagger runs in 32MB and 120MB of RAM respectively. The Shen\n",
      "and Toutanova taggers slow down significantly when the Java machine is given less than 2.2GB and\n",
      "800MB of RAM respectively, while the Koomen tagger requires at least 3GB of RAM.\n",
      "We believe that a number of reasons explain the speed advantage of our system. First, our\n",
      "system only uses rather simple input features and therefore avoids the nonnegligible computation\n",
      "time associated with complex handcrafted features. Secondly, most network computations are dense\n",
      "matrix-vector operations. In contrast, systems that rely on a great number of sparse features experi-\n",
      "ence memory latencies when traversing the sparse data structures. Finally, our compact implemen-\n",
      "tation is self-contained. Since it does not rely on the outputs of disparate NLP system, it does not\n",
      "suffer from communication latency issues.\n",
      "7. Critical Discussion\n",
      "Although we believe that this contribution represents a step towards the “NLP from scratch” objec-\n",
      "tive, we are keenly aware that both our goal and our means can be criticized.\n",
      "The main criticism of our goal can be summarized as follows. Over the years, the NLP com-\n",
      "munity has developed a considerable expertise in engineering effective NLP features. Why should\n",
      "they forget this painfully acquired expertise and instead painfully acquire the skills required to train\n",
      "large neural networks? As mentioned in our introduction, we observe that no single NLP task really\n",
      "covers the goals of NLP. Therefore we believe that task-specific engineering (i.e., that does not gen-\n",
      "eralize to other tasks) is not desirable. But we also recognize how much our neural networks owe to\n",
      "previous NLP task-specific research.\n",
      "The main criticism of our means is easier to address. Why did we choose to rely on a twenty\n",
      "year old technology, namely multilayer neural networks? We were simply attracted by their ability\n",
      "to discover hidden representations using a stochastic learning algorithm that scales linearly with\n",
      "25. Available at http://l2r.cs.uiuc.edu/˜cogcomp/asoftware.php?skey=SRL.\n",
      "2527  \n",
      "\n",
      "COLLOBERT, WESTON, BOTTOU, KARLEN, KAVUKCUOGLU AND KUKSA\n",
      "the number of examples. Most of the neural network technology necessary for our work has been\n",
      "described ten years ago (e.g., Le Cun et al., 1998). However, if we had decided ten years ago to train\n",
      "the language model network LM2 using a vintage computer, training would only be nearing com-\n",
      "pletion today. Training algorithms that scale linearly are most able to benefit from such tremendous\n",
      "progress in computer hardware.\n",
      "8. Conclusion\n",
      "We have presented a multilayer neural network architecture that can handle a number of NLP tasks\n",
      "with both speed and accuracy. The design of this system was determined by our desire to avoid\n",
      "task-specific engineering as much as possible. Instead we rely on large unlabeled data sets and let\n",
      "the training algorithm discover internal representations that prove useful for all the tasks of interest.\n",
      "Using this strong basis, we have engineered a fast and efficient “all purpose” NLP tagger that we\n",
      "hope will prove useful to the community.\n",
      "Acknowledgments\n",
      "We acknowledge the persistent support of NEC for this research effort. We thank Yoshua Bengio,\n",
      "Samy Bengio, Eric Cosatto, Vincent Etter, Hans-Peter Graf, Ralph Grishman, and Vladimir Vapnik\n",
      "for their useful feedback and comments.\n",
      "Appendix A. Neural Network Gradients\n",
      "We consider a neural network fq (·), with parameters q . We maximize the likelihood (8), or minimize\n",
      "ranking criterion (17), with respect to the parameters q , using stochastic gradient. By negating the\n",
      "likelihood, we now assume it all corresponds to minimize a cost C( fq (·)), with respect to q .\n",
      "Following the classical “back-propagation” derivations (LeCun, 1985; Rumelhart et al., 1986)\n",
      "and the modular approach shown in Bottou (1991), any feed-forward neural network with L layers,\n",
      "like the ones shown in Figure 1 and Figure 2, can be seen as a composition of functions f l (·),\n",
      "q\n",
      "corresponding to each layer l:\n",
      "fq (·) = f q L( f q L−1(. . . f q 1(·) . . .))\n",
      "Partitioning the parameters of the network with respect to each layers 1 ≤ l ≤ L, we write:\n",
      "q = (q 1, . . . , q l, . . . , q L) .\n",
      "We are now interested in computing the gradients of the cost with respect to each q l. Applying the\n",
      "chain rule (generalized to vectors) we obtain the classical backpropagation recursion:\n",
      "¶ C ¶ f l ¶ C\n",
      "q\n",
      "= (19)\n",
      "¶q l ¶q l ¶ f l\n",
      "q\n",
      "¶ C ¶ f l ¶ C\n",
      "q\n",
      "= . (20)\n",
      "¶ f l−1 ¶ f l−1 ¶ f l\n",
      "q q q\n",
      "In other words, we first initialize the recursion by computing the gradient of the cost with respect to\n",
      "the last layer output ¶ C/¶ f L. Then each layer l computes the gradient respect to its own parameters\n",
      "q\n",
      "2528  \n",
      "\n",
      "NATURAL LANGUAGE PROCESSING (ALMOST) FROM SCRATCH\n",
      "with (19), given the gradient coming from its output ¶ C/¶ f l . To perform the backpropagation, it\n",
      "q\n",
      "also computes the gradient with respect to its own inputs, as shown in (20). We now derive the\n",
      "gradients for each layer we used in this paper.\n",
      "A.1 Lookup Table Layer\n",
      "Given a matrix of parameters q 1 = W 1 and word (or discrete feature) indices [w]T , the layer outputs\n",
      "1\n",
      "the matrix:\n",
      "f l ([w]T ) = hW i1 hW i1 . . . hW i1 .\n",
      "q l [w] [w] [w]\n",
      "1 2 T\n",
      "(cid:16) (cid:17)\n",
      "The gradients of the weights hW i1 are given by:\n",
      "i\n",
      "¶ C = (cid:229) h ¶ C i1\n",
      "¶ hW i1 ¶ f l i\n",
      "i {1≤t≤T /[w] =i} q\n",
      "t\n",
      "This sum equals zero if the index i in the lookup table does not corresponds to a word in the se-\n",
      "quence. In this case, the ith column of W does not need to be updated. As a Lookup Table Layer is\n",
      "always the first layer, we do not need to compute its gradients with respect to the inputs.\n",
      "A.2 Linear Layer\n",
      "Given parameters q l = (W l, bl), and an input vector f l−1 the output is given by:\n",
      "q\n",
      "f l = W l f l−1 + bl . (21)\n",
      "q q\n",
      "The gradients with respect to the parameters are then obtained with:\n",
      "¶ C ¶ C T ¶ C ¶ C\n",
      "= f l−1 and = , (22)\n",
      "¶ W l ¶ f l q ¶ bl ¶ f l\n",
      "(cid:20) q (cid:21) q\n",
      "h i\n",
      "and the gradients with respect to the inputs are computed with:\n",
      "¶ C T ¶ C\n",
      "= W l . (23)\n",
      "¶ f l−1 ¶ f l\n",
      "q q\n",
      "h i\n",
      "A.3 Convolution Layer\n",
      "Given a input matrix f l−1, a Convolution Layer f l (·) applies a Linear Layer operation (21) suc-\n",
      "q q\n",
      "cessively on each window h f l−1idwin (1 ≤ t ≤ T ) of size d . Using (22), the gradients of the\n",
      "q t win\n",
      "parameters are thus given by summing over all windows:\n",
      "¶ C = (cid:229) T h ¶ C i1 h f l−1idwin T and ¶ C = (cid:229) T h ¶ C i1 .\n",
      "¶ W l ¶ f l t q t ¶ bl ¶ f l t\n",
      "t=1 (cid:20) q (cid:21) t=1 q\n",
      "h i\n",
      "After initializing the input gradients ¶ C/¶ f l−1 to zero, we iterate (23) over all windows for 1 ≤ t ≤\n",
      "q\n",
      "T , leading the accumulation26:\n",
      "h ¶ C idwin+= W l T h ¶ C i1 .\n",
      "¶ f l−1 t ¶ f l t\n",
      "q q\n",
      "h i\n",
      "26. We denote “+=” any accumulation operation.\n",
      "2529  \n",
      "\n",
      "COLLOBERT, WESTON, BOTTOU, KARLEN, KAVUKCUOGLU AND KUKSA\n",
      "A.4 Max Layer\n",
      "Given a matrix f l−1, the Max Layer computes\n",
      "q\n",
      "f l = max h f l−1i1 and a = argmax h f l−1i1 ∀i ,\n",
      "q q t i q t\n",
      "i t i t i\n",
      "h i h i h i\n",
      "where a stores the index of the largest value. We only need to compute the gradient with respect to\n",
      "i\n",
      "the inputs, as this layer has no parameters. The gradient is given by\n",
      "¶ C h ¶ C i1 if t = a\n",
      "\"h\n",
      "¶ f q l−1\n",
      "i t1\n",
      "#\n",
      "=\n",
      "( h\n",
      "¶ f 0ql t ii otherwii\n",
      "se\n",
      ".\n",
      "i\n",
      "A.5 HardTanh Layer\n",
      "Given a vector f l−1, and the definition of the HardTanh (5) we get\n",
      "q\n",
      "0 if f l−1 < −1\n",
      "q\n",
      "¶ C i\n",
      "=  ¶ C if h− 1 <i= f l−1 <= 1 ,\n",
      "\" ¶ f q l−1 # i     h ¶ 0fql ii if f l−1 >h 1q ii\n",
      "q\n",
      "i\n",
      "  h i\n",
      "if we ignore non-differentiability poi nts.\n",
      "A.6 Word-Level Log-Likelihood\n",
      "The network outputs a score [ fq ] for each tag indexed by i. Following (11), if y is the true tag for a\n",
      "i\n",
      "given example, the stochastic score to minimize can be written as\n",
      "C( fq ) = logadd [ fq ] − [ fq ]\n",
      "j y\n",
      "j\n",
      "Considering the definition of the logadd (10), the gradient with respect to fq is given by\n",
      "¶ C e[ fq ] i\n",
      "= − 1 ∀i.\n",
      "¶ [ fq ]\n",
      "i\n",
      "(cid:229)\n",
      "k\n",
      "e[ fq ]\n",
      "k\n",
      "i=y\n",
      "A.7 Sentence-Level Log-Likelihood\n",
      "The network outputs a matrix where each element f gives a score for tag i at word t. Given a\n",
      "q i,t\n",
      "tag sequence [y]T and a input sequence [x]T , we maximize the likelihood (13), which corresponds\n",
      "1 1 (cid:2) (cid:3)\n",
      "to minimizing the score\n",
      "C( fq , A) = logadd s([x]T\n",
      "1\n",
      ", [ j]T\n",
      "1\n",
      ", q ˜ ) −s([x]T\n",
      "1\n",
      ", [y]T\n",
      "1\n",
      ", q ˜ ) ,\n",
      "∀[ j]T\n",
      "1\n",
      "Clogadd\n",
      "with | {z }\n",
      "T\n",
      "s([x]T 1 , [y]T 1 , q ˜ ) = (cid:229) [A] [y] t−1,[y]\n",
      "t\n",
      "+ [ f q ] [y] t,t .\n",
      "t=1\n",
      "(cid:16) (cid:17)\n",
      "2530  \n",
      "\n",
      "NATURAL LANGUAGE PROCESSING (ALMOST) FROM SCRATCH\n",
      "We first initialize all gradients to zero\n",
      "¶ C ¶ C\n",
      "= 0 ∀i,t and = 0 ∀i, j .\n",
      "¶ f ¶ [A]\n",
      "q i,t i, j\n",
      "(cid:2) (cid:3)\n",
      "We then accumulate gradients over the second part of the cost −s([x]T , [y]T , q ˜ ), which gives:\n",
      "1 1\n",
      "¶ C\n",
      "+= 1\n",
      "¶ f\n",
      "q [y] ,t\n",
      "t ∀t .\n",
      "(cid:2)¶ C(cid:3)\n",
      "+= 1\n",
      "¶ [A]\n",
      "[y] ,[y]\n",
      "t−1 t\n",
      "We now need to accumulate the gradients over the first part of the cost, that is C . We differen-\n",
      "logadd\n",
      "tiate C by applying the chain rule through the recursion (14). First we initialize our recursion\n",
      "logadd\n",
      "with\n",
      "¶ C logadd ed T (i)\n",
      "= ∀i .\n",
      "¶d T (i) (cid:229) k ed T (k)\n",
      "We then compute iteratively:\n",
      "¶ C logadd (cid:229) ¶ C logadd ed t−1(i)+[A] i, j\n",
      "= ,\n",
      "¶d t−1(i) j ¶d t ( j) (cid:229) k ed t−1(k)+[A] k, j\n",
      "where at each step t of the recursion we accumulate of the gradients with respect to the inputs fq ,\n",
      "and the transition scores [A] :\n",
      "i, j\n",
      "¶ C ¶ C ¶d (i) ¶ C\n",
      "logadd t logadd\n",
      "+= =\n",
      "¶ f ¶d (i) ¶ f ¶d (i)\n",
      "q i,t t q i,t t\n",
      "(cid:2) ¶ C(cid:3) ¶ C logadd ¶d (cid:2) t ( j(cid:3)) ¶ C logadd ed t−1(i)+[A] i, j\n",
      "+= = .\n",
      "¶ [A] i, j ¶d t ( j) ¶ [A] i, j ¶d t ( j) (cid:229) k ed t−1(k)+[A] k, j\n",
      "A.8 Ranking Criterion\n",
      "We use the ranking criterion (17) for training our language model. In this case, given a “positive”\n",
      "example x and a “negative” example x(w), we want to minimize:\n",
      "C( fq (x), fq (xw)) = max 0 , 1 − fq (x) + fq (x(w)) .\n",
      "n o\n",
      "Ignoring the non-differentiability of max(0, ·) in zero, the gradient is simply given by:\n",
      "¶ C −1 if 1 − fq (x) + fq (x(w)) > 0\n",
      "¶ fq (x) 1\n",
      "=  (cid:18) (cid:19) .\n",
      "¶ fq¶ (C xw) !    0 otherwise\n",
      "0\n",
      "(cid:18) (cid:19)\n",
      "\n",
      "\n",
      "\n",
      "2531  \n",
      "\n",
      "COLLOBERT, WESTON, BOTTOU, KARLEN, KAVUKCUOGLU AND KUKSA\n",
      "References\n",
      "R. K. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks and\n",
      "unlabeled data. Journal of Machine Learning Research (JMLR), 6:1817–1953, 2005.\n",
      "R. M. Bell, Y. Koren, and C. Volinsky. The BellKor solution to the Netflix Prize. Technical report,\n",
      "AT&T Labs, 2007. http://www.research.att.com/˜volinsky/netflix.\n",
      "Y. Bengio and R. Ducharme. A neural probabilistic language model. In Advances in Neural Infor-\n",
      "mation Processing Systems (NIPS 13), 2001.\n",
      "Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks.\n",
      "In Advances in Neural Information Processing Systems (NIPS 19), 2007.\n",
      "Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning. In International Con-\n",
      "ference on Machine Learning (ICML), 2009.\n",
      "L. Bottou. Stochastic gradient learning in neural networks. In Proceedings of Neuro-Nˆımes. EC2,\n",
      "1991.\n",
      "L. Bottou. Online algorithms and stochastic approximations. In David Saad, editor, Online Learning\n",
      "and Neural Networks. Cambridge University Press, Cambridge, UK, 1998.\n",
      "L. Bottou and P. Gallinari. A framework for the cooperation of learning algorithms. In Advances in\n",
      "Neural Information Processing Systems (NIPS 3). 1991.\n",
      "L. Bottou, Y. LeCun, and Yoshua Bengio. Global training of document processing systems using\n",
      "graph transformer networks. In Conference on Computer Vision and Pattern Recognition (CVPR),\n",
      "pages 489–493, 1997.\n",
      "J. S. Bridle. Probabilistic interpretation of feedforward classification network outputs, with rela-\n",
      "tionships to statistical pattern recognition. In F. Fogelman Soulie´ and J. He´rault, editors, Neu-\n",
      "rocomputing: Algorithms, Architectures and Applications, pages 227–236. NATO ASI Series,\n",
      "1990.\n",
      "P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. D. Pietra, and J C. Lai. Class-based n-gram models\n",
      "of natural language. Computational Linguistics, 18(4):467–479, 1992a.\n",
      "P. F. Brown, V. J. Della Pietra, R. L. Mercer, S. A. Della Pietra, and J. C. Lai. An estimate of an\n",
      "upper bound for the entropy of english. Computational Linguistics, 18(1):31–41, 1992b.\n",
      "C. J. C. Burges, R. Ragno, and Quoc Viet Le. Learning to rank with nonsmooth cost functions. In\n",
      "Advances in Neural Information Processing Systems (NIPS 19), pages 193–200. 2007.\n",
      "R. Caruana. Multitask Learning. Machine Learning, 28(1):41–75, 1997.\n",
      "O. Chapelle, B. Schlkopf, and A. Zien. Semi-Supervised Learning. Adaptive computation and\n",
      "machine learning. MIT Press, Cambridge, Mass., USA, September 2006.\n",
      "E. Charniak. A maximum-entropy-inspired parser. In Conference of the North American Chapter of\n",
      "the Association for Computational Linguistics & Human Language Technologies (NAACL-HLT),\n",
      "pages 132–139, 2000.\n",
      "2532  \n",
      "\n",
      "NATURAL LANGUAGE PROCESSING (ALMOST) FROM SCRATCH\n",
      "H. L. Chieu. Named entity recognition with a maximum entropy approach. In Conference on\n",
      "Natural Language Learning (CoNLL), pages 160–163, 2003.\n",
      "N. Chomsky. Three models for the description of language. IRE Transactions on Information\n",
      "Theory, 2(3):113–124, September 1956.\n",
      "S. Cle´menc¸on and N. Vayatis. Ranking the best instances. Journal of Machine Learning Research\n",
      "(JMLR), 8:2671–2699, 2007.\n",
      "W. W. Cohen, R. E. Schapire, and Y. Singer. Learning to order things. Journal of Artificial Intelli-\n",
      "gence Research (JAIR), 10:243–270, 1998.\n",
      "T. Cohn and P. Blunsom. Semantic role labelling with tree conditional random fields. In Conference\n",
      "on Computational Natural Language (CoNLL), 2005.\n",
      "M. Collins. Head-Driven Statistical Models for Natural Language Parsing. PhD thesis, University\n",
      "of Pennsylvania, 1999.\n",
      "R. Collobert. Large Scale Machine Learning. PhD thesis, Universite´ Paris VI, 2004.\n",
      "R. Collobert. Deep learning for efficient discriminative parsing. In International Conference on\n",
      "Artificial Intelligence and Statistics (AISTATS), 2011.\n",
      "T. Cover and R. King. A convergent gambling estimate of the entropy of english. IEEE Transactions\n",
      "on Information Theory, 24(4):413–421, July 1978.\n",
      "R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. Named entity recognition through classifier\n",
      "combination. In Conference of the North American Chapter of the Association for Computational\n",
      "Linguistics & Human Language Technologies (NAACL-HLT), pages 168–171, 2003.\n",
      "D. Gildea and D. Jurafsky. Automatic labeling of semantic roles. Computational Linguistics, 28(3):\n",
      "245–288, 2002.\n",
      "D. Gildea and M. Palmer. The necessity of parsing for predicate argument recognition. Meeting of\n",
      "the Association for Computational Linguistics (ACL), pages 239–246, 2002.\n",
      "J. Gime´nez and L. Ma`rquez. SVMTool: A general POS tagger generator based on support vector\n",
      "machines. In Conference on Language Resources and Evaluation (LREC), 2004.\n",
      "A. Haghighi, K. Toutanova, and C. D. Manning. A joint model for semantic role labeling. In\n",
      "Conference on Computational Natural Language Learning (CoNLL), June 2005.\n",
      "Z. S. Harris. Mathematical Structures of Language. John Wiley & Sons Inc., 1968.\n",
      "D. Heckerman, D. M. Chickering, C. Meek, R. Rounthwaite, and C. Kadie. Dependency networks\n",
      "for inference, collaborative filtering, and data visualization. Journal of Machine Learning Re-\n",
      "search (JMLR), 1:49–75, 2001.\n",
      "G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural\n",
      "Computation, 18(7):1527–1554, July 2006.\n",
      "2533  \n",
      "\n",
      "COLLOBERT, WESTON, BOTTOU, KARLEN, KAVUKCUOGLU AND KUKSA\n",
      "K. Hollingshead, S. Fisher, and B. Roark. Comparing and combining finite-state and context-free\n",
      "parsers. In Conference on Human Language Technology and Empirical Methods in Natural\n",
      "Language Processing (HLT-EMNLP), pages 787–794, 2005.\n",
      "F. Huang and A. Yates. Distributional representations for handling sparsity in supervised sequence-\n",
      "labeling. In Meeting of the Association for Computational Linguistics (ACL), pages 495–503,\n",
      "2009.\n",
      "F. Jelinek. Continuous speech recognition by statistical methods. Proceedings of the IEEE, 64(4):\n",
      "532–556, 1976.\n",
      "T. Joachims. Transductive inference for text classification using support vector machines. In Inter-\n",
      "national Conference on Machine learning (ICML), 1999.\n",
      "D. Klein and C. D. Manning. Natural language grammar induction using a constituent-context\n",
      "model. In Advances in Neural Information Processing Systems (NIPS 14), pages 35–42. 2002.\n",
      "T. Koo, X. Carreras, and M. Collins. Simple semi-supervised dependency parsing. In Meeting of\n",
      "the Association for Computational Linguistics (ACL), pages 595–603, 2008.\n",
      "P. Koomen, V. Punyakanok, D. Roth, and W. Yih. Generalized inference with multiple semantic\n",
      "role labeling systems (shared task paper). In Conference on Computational Natural Language\n",
      "Learning (CoNLL), pages 181–184, 2005.\n",
      "T. Kudo and Y. Matsumoto. Chunking with support vector machines. In Conference of the North\n",
      "American Chapter of the Association for Computational Linguistics & Human Language Tech-\n",
      "nologies (NAACL-HLT), pages 1–8, 2001.\n",
      "T. Kudoh and Y. Matsumoto. Use of support vector learning for chunk identification. In Conference\n",
      "on Natural Language Learning (CoNLL) and Second Learning Language in Logic Workshop\n",
      "(LLL), pages 142–144, 2000.\n",
      "J. Lafferty, A. McCallum, and F. Pereira. Conditional random fields: Probabilistic models for seg-\n",
      "menting and labeling sequence data. In International Conference on Machine Learning (ICML),\n",
      "2001.\n",
      "Y. Le Cun, L. Bottou, Y. Bengio, and P. Haffner. Gradient based learning applied to document\n",
      "recognition. Proceedings of IEEE, 86(11):2278–2324, 1998.\n",
      "Y. LeCun. A learning scheme for asymmetric threshold networks. In Proceedings of Cognitiva,\n",
      "pages 599–604, Paris, France, 1985.\n",
      "Y. LeCun, L. Bottou, G. B. Orr, and K.-R. Mu¨ller. Efficient backprop. In G.B. Orr and K.-R. Mu¨ller,\n",
      "editors, Neural Networks: Tricks of the Trade, pages 9–50. Springer, 1998.\n",
      "D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. Rcv1: A new benchmark collection for text categoriza-\n",
      "tion research. Journal of Machine Learning Research (JMLR), 5:361–397, 2004.\n",
      "P. Liang. Semi-supervised learning for natural language. Master’s thesis, Massachusetts Institute of\n",
      "Technology, 2005.\n",
      "2534  \n",
      "\n",
      "NATURAL LANGUAGE PROCESSING (ALMOST) FROM SCRATCH\n",
      "P. Liang, H. Daume´, III, and D. Klein. Structure compilation: trading structure for features. In\n",
      "International Conference on Machine learning (ICML), pages 592–599, 2008.\n",
      "D. Lin and X. Wu. Phrase clustering for discriminative learning. In Meeting of the Association for\n",
      "Computational Linguistics (ACL), pages 1030–1038, 2009.\n",
      "N. Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algo-\n",
      "rithm. In Machine Learning, pages 285–318, 1988.\n",
      "A. McCallum and Wei Li. Early results for named entity recognition with conditional random fields,\n",
      "feature induction and web-enhanced lexicons. In Conference of the North American Chapter of\n",
      "the Association for Computational Linguistics & Human Language Technologies (NAACL-HLT),\n",
      "pages 188–191, 2003.\n",
      "D. McClosky, E. Charniak, and M. Johnson. Effective self-training for parsing. Conference of the\n",
      "North American Chapter of the Association for Computational Linguistics & Human Language\n",
      "Technologies (NAACL-HLT), 2006.\n",
      "R. McDonald, K. Crammer, and F. Pereira. Flexible text segmentation with structured multilabel\n",
      "classification. In Conference on Human Language Technology and Empirical Methods in Natural\n",
      "Language Processing (HLT-EMNLP), pages 987–994, 2005.\n",
      "S. Miller, H. Fox, L. Ramshaw, and R. Weischedel. A novel use of statistical parsing to extract\n",
      "information from text. Applied Natural Language Processing Conference (ANLP), 2000.\n",
      "S. Miller, J. Guinness, and A. Zamanian. Name tagging with word clusters and discriminative\n",
      "training. In Conference of the North American Chapter of the Association for Computational\n",
      "Linguistics & Human Language Technologies (NAACL-HLT), pages 337–342, 2004.\n",
      "A Mnih and G. E. Hinton. Three new graphical models for statistical language modelling. In\n",
      "International Conference on Machine Learning (ICML), pages 641–648, 2007.\n",
      "G. Musillo and P. Merlo. Robust Parsing of the Proposition Bank. ROMAND 2006: Robust Methods\n",
      "in Analysis of Natural language Data, 2006.\n",
      "R. M. Neal. Bayesian Learning for Neural Networks. Number 118 in Lecture Notes in Statistics.\n",
      "Springer-Verlag, New York, 1996.\n",
      "D. Okanohara and J. Tsujii. A discriminative language model with pseudo-negative samples. Meet-\n",
      "ing of the Association for Computational Linguistics (ACL), pages 73–80, 2007.\n",
      "M. Palmer, D. Gildea, and P. Kingsbury. The proposition bank: An annotated corpus of semantic\n",
      "roles. Computational Linguistics, 31(1):71–106, 2005.\n",
      "J. Pearl. Probabilistic Reasoning in Intelligent Systems. Morgan Kaufman, San Mateo, 1988.\n",
      "D. C. Plaut and G. E. Hinton. Learning sets of filters using back-propagation. Computer Speech\n",
      "and Language, 2:35–61, 1987.\n",
      "M. F. Porter. An algorithm for suffix stripping. Program, 14(3):130–137, 1980.\n",
      "2535  \n",
      "\n",
      "COLLOBERT, WESTON, BOTTOU, KARLEN, KAVUKCUOGLU AND KUKSA\n",
      "S. Pradhan, W. Ward, K. Hacioglu, J. Martin, and D. Jurafsky. Shallow semantic parsing using\n",
      "support vector machines. Conference of the North American Chapter of the Association for\n",
      "Computational Linguistics & Human Language Technologies (NAACL-HLT), 2004.\n",
      "S. Pradhan, K. Hacioglu, W. Ward, J. H. Martin, and D. Jurafsky. Semantic role chunking combining\n",
      "complementary syntactic views. In Conference on Computational Natural Language Learning\n",
      "(CoNLL), pages 217–220, 2005.\n",
      "V. Punyakanok, D. Roth, and W. Yih. The necessity of syntactic parsing for semantic role labeling.\n",
      "In International Joint Conference on Artificial Intelligence (IJCAI), pages 1117–1123, 2005.\n",
      "L. R. Rabiner. A tutorial on hidden Markov models and selected applications in speech recognition.\n",
      "Proceedings of the IEEE, 77(2):257–286, 1989.\n",
      "L. Ratinov and D. Roth. Design challenges and misconceptions in named entity recognition. In Con-\n",
      "ference on Computational Natural Language Learning (CoNLL), pages 147–155. Association for\n",
      "Computational Linguistics, 2009.\n",
      "A. Ratnaparkhi. A maximum entropy model for part-of-speech tagging. In Conference on Empirical\n",
      "Methods in Natural Language Processing (EMNLP), pages 133–142, 1996.\n",
      "B. Rosenfeld and R. Feldman. Using Corpus Statistics on Entities to Improve Semi-supervised\n",
      "Relation Extraction from the Web. Meeting of the Association for Computational Linguistics\n",
      "(ACL), pages 600–607, 2007.\n",
      "D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning internal representations by back-\n",
      "propagating errors. In D.E. Rumelhart and J. L. McClelland, editors, Parallel Distributed Pro-\n",
      "cessing: Explorations in the Microstructure of Cognition, volume 1, pages 318–362. MIT Press,\n",
      "1986.\n",
      "H. Schu¨tze. Distributional part-of-speech tagging. In Meeting of the Association for Computational\n",
      "Linguistics (ACL), pages 141–148, 1995.\n",
      "H. Schwenk and J. L. Gauvain. Connectionist language modeling for large vocabulary continuous\n",
      "speech recognition. In International Conference on Acoustics, Speech, and Signal Processing\n",
      "(ICASSP), pages 765–768, 2002.\n",
      "F. Sha and F. Pereira. Shallow parsing with conditional random fields. In Conference of the North\n",
      "American Chapter of the Association for Computational Linguistics & Human Language Tech-\n",
      "nologies (NAACL-HLT), pages 134–141, 2003.\n",
      "C. E. Shannon. Prediction and entropy of printed english. Bell Systems Technical Journal, 30:\n",
      "50–64, 1951.\n",
      "H. Shen and A. Sarkar. Voting between multiple data representations for text chunking. Advances\n",
      "in Artificial Intelligence, pages 389–400, 2005.\n",
      "L. Shen, G. Satta, and A. K. Joshi. Guided learning for bidirectional sequence classification. In\n",
      "Meeting of the Association for Computational Linguistics (ACL), 2007.\n",
      "2536  \n",
      "\n",
      "NATURAL LANGUAGE PROCESSING (ALMOST) FROM SCRATCH\n",
      "N. A. Smith and J. Eisner. Contrastive estimation: Training log-linear models on unlabeled data. In\n",
      "Meeting of the Association for Computational Linguistics (ACL), pages 354–362, 2005.\n",
      "S. C. Suddarth and A. D. C. Holden. Symbolic-neural systems and the use of hints for developing\n",
      "complex systems. International Journal of Man-Machine Studies, 35(3):291–311, 1991.\n",
      "X. Sun, L.-P. Morency, D. Okanohara, and J. Tsujii. Modeling latent-dynamic in shallow parsing: a\n",
      "latent conditional model with improved inference. In International Conference on Computational\n",
      "Linguistics (COLING), pages 841–848, 2008.\n",
      "C. Sutton and A. McCallum. Joint parsing and semantic role labeling. In Conference on Computa-\n",
      "tional Natural Language (CoNLL), pages 225–228, 2005a.\n",
      "C. Sutton and A. McCallum. Composition of conditional random fields for transfer learning. Confer-\n",
      "ence on Human Language Technology and Empirical Methods in Natural Language Processing\n",
      "(HLT-EMNLP), pages 748–754, 2005b.\n",
      "C. Sutton, A. McCallum, and K. Rohanimanesh. Dynamic Conditional Random Fields: Factorized\n",
      "Probabilistic Models for Labeling and Segmenting Sequence Data. Journal of Machine Learning\n",
      "Research (JMLR), 8:693–723, 2007.\n",
      "J. Suzuki and H. Isozaki. Semi-supervised sequential labeling and segmentation using giga-word\n",
      "scale unlabeled data. In Conference of the North American Chapter of the Association for Com-\n",
      "putational Linguistics & Human Language Technologies (NAACL-HLT), pages 665–673, 2008.\n",
      "W. J. Teahan and J. G. Cleary. The entropy of english using ppm-based models. In Data Compres-\n",
      "sion Conference (DCC), pages 53–62. IEEE Computer Society Press, 1996.\n",
      "K. Toutanova, D. Klein, C. D. Manning, and Y. Singer. Feature-rich part-of-speech tagging with a\n",
      "cyclic dependency network. In Conference of the North American Chapter of the Association for\n",
      "Computational Linguistics & Human Language Technologies (NAACL-HLT), 2003.\n",
      "J. Turian, L. Ratinov, and Y. Bengio. Word representations: A simple and general method for semi-\n",
      "supervised learning. In Meeting of the Association for Computational Linguistics (ACL), pages\n",
      "384–392, 2010.\n",
      "N. Ueffing, G. Haffari, and A. Sarkar. Transductive learning for statistical machine translation. In\n",
      "Meeting of the Association for Computational Linguistics (ACL), pages 25–32, 2007.\n",
      "A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K.J. Lang. Phoneme recognition using time-\n",
      "delay neural networks. IEEE Transactions on Acoustics, Speech, and Signal Processing, 37(3):\n",
      "328–339, 1989.\n",
      "J. Weston, F. Ratle, and R. Collobert. Deep learning via semi-supervised embedding. In Interna-\n",
      "tional Conference on Machine learning (ICML), pages 1168–1175, 2008.\n",
      "2537\n"
     ]
    }
   ],
   "source": [
    "text = \"  \\n\\n\".join(texts)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7edd619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RONAN@COLLOBERT.COM', 'JWESTON@GOOGLE.COM', 'LEON@BOTTOU.ORG', 'MICHAEL.KARLEN@GMAIL.COM', 'KORAY@CS.NYU.EDU', 'PKUKSA@CS.RUTGERS.EDU']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "regex = '[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "emails = re.findall(regex,text)\n",
    "print(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a096921c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exhibit 99.1\n",
      "Ferrari N.V.\n",
      "Interim Report\n",
      "At and for the three and nine months ended September 30, 2020\n",
      "____________________________________________________________________________________________________\n",
      "CONTENTS\n",
      "Page\n",
      "BOARD OF DIRECTORS 1\n",
      "INDEPENDENT AUDITORS 1\n",
      "CERTAIN DEFINED TERMS 1\n",
      "INTRODUCTION 2\n",
      "FORWARD-LOOKING STATEMENTS 3\n",
      "MANAGEMENT’S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF\n",
      "OPERATIONS\n",
      "Highlights 5\n",
      "COVID-19 Pandemic Update 7\n",
      "Non-GAAP Financial Measures 10\n",
      "Results of Operations 14\n",
      "Liquidity and Capital Resources 22\n",
      "Risk Factors 31\n",
      "Outlook 33\n",
      "INTERIM CONDENSED CONSOLIDATED FINANCIAL STATEMENTS AT AND FOR THE THREE AND\n",
      "NINE MONTHS ENDED SEPTEMBER 30, 2020\n",
      "Interim Consolidated Income Statement F-1\n",
      "Interim Consolidated Statement of Comprehensive Income F-2\n",
      "Interim Consolidated Statement of Financial Position F-3\n",
      "Interim Consolidated Statement of Cash Flows F-4\n",
      "Interim Consolidated Statement of Changes in Equity F-5\n",
      "Notes to the Interim Condensed Consolidated Financial Statements F-6\n",
      "\n",
      "BOARD OF DIRECTORS\n",
      "Executive Chairman\n",
      "John Elkann\n",
      "Chief Executive Officer\n",
      "Louis C. Camilleri\n",
      "Vice Chairman\n",
      "Piero Ferrari\n",
      "Directors\n",
      "Delphine Arnault\n",
      "Francesca Bellettini\n",
      "Roberto Cingolani\n",
      "Eddy Cue\n",
      "Sergio Duca\n",
      "John Galantic\n",
      "Maria Patrizia Grieco\n",
      "Adam Keswick\n",
      "INDEPENDENT AUDITORS\n",
      "EY S.p.A.\n",
      "CERTAIN DEFINED TERMS\n",
      "In this report (the “Interim Report”), unless otherwise specified, the terms “we,” “our,” “us,” the “Group,” the\n",
      "“Company” and “Ferrari” refer to Ferrari N.V., individually or together with its subsidiaries, as the context may require.\n",
      "1\n",
      "\n",
      "INTRODUCTION\n",
      "The Interim Condensed Consolidated Financial Statements at and for the three and nine months ended September\n",
      "30, 2020 (the “Interim Condensed Consolidated Financial Statements”) included in this Interim Report have been prepared\n",
      "in accordance with International Financial Reporting Standards (“IFRS”) issued by the International Accounting Standards\n",
      "Board (“IASB”) and in accordance with IFRS as endorsed by the European Union, and in particular, in compliance with IAS\n",
      "34 - Interim Financial Reporting. There are no material effects on these Interim Condensed Consolidated Financial\n",
      "Statements resulting from differences between IFRS as issued by the IASB and IFRS as endorsed by the European Union. The\n",
      "accounting principles applied are consistent with those used for the preparation of the annual consolidated financial\n",
      "statements at and for the year ended December 31, 2019 (the “Annual Consolidated Financial Statements”), except as\n",
      "otherwise stated in “New standards and amendments effective from January 1, 2020” in the notes to the Interim Condensed\n",
      "Consolidated Financial Statements.\n",
      "The Group’s financial information in this Interim Report is presented in Euro except that, in some instances,\n",
      "information is presented in U.S. Dollars. All references in this report to “Euro” and “€” refer to the currency introduced at\n",
      "the start of the third stage of European Economic and Monetary Union pursuant to the Treaty on the Functioning of the\n",
      "European Union, as amended, and all references to “U.S. Dollars” and “$” refer to the currency of the United States of\n",
      "America (or “United States”).\n",
      "Certain totals in the tables included in this Interim Report may not add due to rounding.\n",
      "The financial data in “Results of Operations” is presented in millions of Euro, while the percentages presented are\n",
      "calculated using the underlying figures in thousands of Euro.\n",
      "This Interim Report is unaudited.\n",
      "2\n",
      "\n",
      "FORWARD-LOOKING STATEMENTS\n",
      "Statements contained in this report, particularly those regarding our possible or assumed future performance are\n",
      "“forward-looking statements” that contain risks and uncertainties. In some cases, words such as “may”, “will”, “expect”,\n",
      "“could”, “should”, “intend”, “estimate”, “anticipate”, “believe”, “remain”, “continue”, “on track”, “successful”,\n",
      "“grow”, “design”, “target”, “objective”, “goal”, “forecast”, “projection”, “outlook”, “prospects”, “plan”, “guidance”\n",
      "and similar expressions are used to identify forward-looking statements. These forward-looking statements reflect the\n",
      "respective current views of Ferrari with respect to future events and involve significant risks and uncertainties that could\n",
      "cause actual results to differ materially from those indicated in the forward-looking statements. Such risks and uncertainties\n",
      "include, without limitation:\n",
      "• our ability to preserve and enhance the value of the Ferrari brand;\n",
      "• the success of our Formula 1 racing team and the expenses we incur for our Formula 1 activities, the impact of the\n",
      "application of the new Formula 1 regulations (both financial and technical) progressively coming into effect from\n",
      "2021 and 2022, the uncertainty of the sponsorship and commercial revenues we generate from our participation in\n",
      "the Formula 1 World Championship, including as a result of the impact of the COVID-19 pandemic, as well as the\n",
      "popularity of Formula 1 more broadly;\n",
      "• our ability to keep up with advances in high performance car technology and to make appealing designs for our new\n",
      "models;\n",
      "• our ability to preserve our relationship with the automobile collector and enthusiast community;\n",
      "• changes in client preferences and automotive trends;\n",
      "• changes in the general economic environment, including changes in some of the markets in which we operate, and\n",
      "changes in demand for luxury goods, including high performance luxury cars, which is highly volatile;\n",
      "• competition in the luxury performance automobile industry;\n",
      "• our ability to successfully carry out our growth strategy and, particularly, our ability to grow our presence in\n",
      "growth and emerging market countries;\n",
      "• our low volume strategy;\n",
      "• reliance upon a number of key members of executive management and employees, and the ability of our current\n",
      "management team to operate and manage effectively;\n",
      "• the performance of our dealer network on which we depend for sales and services;\n",
      "• increases in costs, disruptions of supply or shortages of components and raw materials;\n",
      "• disruptions at our manufacturing facilities in Maranello and Modena;\n",
      "• the effects of the evolution of and response to the COVID-19 pandemic;\n",
      "• the effects of Brexit;\n",
      "• the performance of our licensees for Ferrari-branded products;\n",
      "• our ability to protect our intellectual property rights and to avoid infringing on the intellectual property rights of\n",
      "others;\n",
      "• the ability of Maserati, our engine customer, to sell its planned volume of cars;\n",
      "• our continued compliance with customs regulations of various jurisdictions;\n",
      "3\n",
      "\n",
      "• the impact of increasingly stringent fuel economy, emission and safety standards, including the cost of compliance,\n",
      "and any required changes to our products;\n",
      "• the challenges and costs of integrating hybrid and electric technology more broadly into our car portfolio over time;\n",
      "• product recalls, liability claims and product warranties;\n",
      "• the adequacy of our insurance coverage to protect us against potential losses;\n",
      "• our ability to ensure that our employees, agents and representatives comply with applicable law and regulations;\n",
      "• our ability to maintain the functional and efficient operation of our information technology systems, including our\n",
      "ability to defend from the risk of cyberattacks, including on our in-vehicle technology;\n",
      "• our ability to service and refinance our debt;\n",
      "• our ability to provide or arrange for adequate access to financing for our dealers and clients, and associated risks;\n",
      "• labor relations and collective bargaining agreements;\n",
      "• exchange rate fluctuations, interest rate changes, credit risk and other market risks;\n",
      "• changes in tax, tariff or fiscal policies and regulatory, political and labor conditions in the jurisdictions in which we\n",
      "operate, including possible future bans of combustion engine cars in cities and the potential advent of self-driving\n",
      "technology;\n",
      "• potential conflicts of interest due to director and officer overlaps with our largest shareholders; and\n",
      "• other factors discussed elsewhere in this document.\n",
      "We expressly disclaim and do not assume any liability in connection with any inaccuracies in any of the forward-\n",
      "looking statements in this document or in connection with any use by any third party of such forward-looking statements.\n",
      "Actual results could differ materially from those anticipated in such forward-looking statements. We do not undertake an\n",
      "obligation to update or revise publicly any forward-looking statements.\n",
      "4\n",
      "\n",
      "MANAGEMENT’S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION\n",
      "AND RESULTS OF OPERATIONS\n",
      "Highlights\n",
      "Consolidated Income Statement Data\n",
      "For the three months For the nine months\n",
      "ended September 30, ended September 30,\n",
      "2020 2019 2020 2019\n",
      "(€ million, except per share data)\n",
      "Net revenues 888 915 2,391 2,839\n",
      "EBIT 222 227 465 698\n",
      "Profit before taxes 208 211 427 666\n",
      "Net profit 171 169 346 533\n",
      "Net profit attributable to:\n",
      "Owners of the parent 171 168 346 529\n",
      "Non-controlling interests — 1 — 4\n",
      "Basic earnings per common share (in Euro) (1) 0.92 0.90 1.87 2.82\n",
      "Diluted earnings per common share (in Euro) (1) 0.92 0.90 1.86 2.81\n",
      "Dividend declared per common share (in Euro) (2) (3) 1.13 1.03 1.13 1.03\n",
      "Dividend declared per common share (in USD) (2) (3) (4) 1.23 1.16 1.23 1.16\n",
      "_____________________________\n",
      "(1) See Note 13 “Earnings per Share” to the Interim Condensed Consolidated Financial Statements for the calculation of basic and diluted earnings per\n",
      "common share.\n",
      "(2) Following approval of the annual accounts by the shareholders at the Annual General Meeting of the Shareholders on April 16, 2020, a dividend\n",
      "distribution of €1.13 per outstanding common share was approved, corresponding to a total distribution of €209 million. This distribution was made\n",
      "from the retained earnings reserve. In May 2020 the Company paid €195 million of the distribution to owners of the parent and the remaining balance,\n",
      "which relates to withholding taxes, was paid in the third quarter of 2020.\n",
      "(3) Following approval of the annual accounts by the shareholders at the Annual General Meeting of the Shareholders on April 12, 2019, a dividend\n",
      "distribution of €1.03 per outstanding common share was approved, corresponding to a total distribution of €193 million. This distribution was made\n",
      "from the retained earnings reserve. In May 2019 the Company paid €181 million of the distribution to owners of the parent and the remaining balance,\n",
      "which relates to withholding taxes, was paid in the third quarter of 2019.\n",
      "(4) Translated into U.S. Dollars at the exchange rates in effect on the dates on which the distribution was declared in U.S. Dollars for common shares that\n",
      "are traded on the New York Stock Exchange. These translations are examples only, and should not be construed as a representation that the Euro\n",
      "amount represents, or has been or could be converted into, U.S. Dollars at that or any other rate.\n",
      "Consolidated Statement of Financial Position Data\n",
      "At September 30, At December 31,\n",
      "2020 2019\n",
      "(€ million, except share data)\n",
      "Cash and cash equivalents 1,179 898\n",
      "Receivables from financing activities 968 966\n",
      "Total assets 6,004 5,446\n",
      "Debt 2,741 2,090\n",
      "Total equity 1,521 1,487\n",
      "Equity attributable to owners of the parent 1,517 1,481\n",
      "Non-controlling interests 4 6\n",
      "Share capital 3 3\n",
      "Common shares issued and outstanding (in thousands of shares) 184,748 185,283\n",
      "5\n",
      "\n",
      "Other Statistical Information\n",
      "Shipments (1)\n",
      "For the three months ended For the nine months ended\n",
      "(Number of cars and % of total cars)\n",
      "September 30, September 30,\n",
      "2020 % 2019 % 2020 % 2019 %\n",
      "EMEA\n",
      "Germany 269 11.6 % 262 10.6 % 796 12.4 % 709 9.1 %\n",
      "UK 247 10.7 % 202 8.2 % 689 10.7 % 782 10.1 %\n",
      "Italy 149 6.4 % 128 5.2 % 437 6.8 % 431 5.6 %\n",
      "Switzerland 123 5.3 % 109 4.4 % 327 5.1 % 320 4.1 %\n",
      "France 121 5.2 % 118 4.8 % 303 4.7 % 334 4.3 %\n",
      "Middle East (2) 64 2.8 % 80 3.2 % 190 3.0 % 193 2.5 %\n",
      "Other EMEA (3) 315 13.7 % 244 9.8 % 768 11.8 % 778 10.0 %\n",
      "Total EMEA 1,288 55.7 % 1,143 46.2 % 3,510 54.5 % 3,547 45.7 %\n",
      "Americas (4) 504 21.8 % 772 31.2 % 1,635 25.4 % 2,295 29.6 %\n",
      "Mainland China, Hong Kong and Taiwan 119 5.1 % 159 6.4 % 181 2.8 % 776 10.0 %\n",
      "Rest of APAC (5) 402 17.4 % 400 16.2 % 1,114 17.3 % 1,137 14.7 %\n",
      "Total 2,313 100.0 % 2,474 100.0 % 6,440 100.0 % 7,755 100.0 %\n",
      "_____________________________\n",
      "(1) Excluding the XX Programme, racing cars, Fuori Serie, one-off and pre-owned cars.\n",
      "(2) Middle East mainly includes the United Arab Emirates, Saudi Arabia, Bahrain, Lebanon, Qatar, Oman and Kuwait.\n",
      "(3) Other EMEA includes Africa and the other European markets not separately identified.\n",
      "(4) Americas includes the United States of America, Canada, Mexico, the Caribbean and Central and South America.\n",
      "(5) Rest of APAC mainly includes Japan, Australia, Singapore, Indonesia, South Korea, Thailand and Malaysia.\n",
      "Average number of employees for the period\n",
      "For the three months ended For the nine months ended\n",
      "September 30, September 30,\n",
      "2020 2019 2020 2019\n",
      "Average number of employees for the period 4,427 4,195 4,410 4,130\n",
      "6\n",
      "\n",
      "COVID-19 Pandemic Update\n",
      "The global spread of COVID-19 (“COVID-19”), a virus causing potentially deadly respiratory tract infections,\n",
      "which was declared a global pandemic by the World Health Organization in March 2020, has led governments around the\n",
      "world to mandate certain restrictive measures to contain the pandemic, including social distancing, quarantine, “shelter in\n",
      "place” or similar orders, travel restrictions and suspension of non-essential business activities. The main impacts on Ferrari\n",
      "during the first nine months of 2020 include the following:\n",
      "• Deliveries to the distribution network were temporarily suspended near the end of March 2020 due to restrictions on\n",
      "dealer activities or the inability of customers to collect their cars, and deliveries gradually recommenced during the\n",
      "month of May 2020. From May to October substantially all Ferrari dealerships remained fully operational.\n",
      "• With the safety and well-being of Ferrari employees in mind, production was suspended from March 14 and\n",
      "gradually restarted from May 4, with full production resuming on May 8. Ferrari continued to pay all employees\n",
      "throughout the suspension period and did not accede to any government aid programs. Ferrari experienced limited\n",
      "supply chain constraints in the first nine months of 2020, which were actively managed to mitigate any impacts on\n",
      "our production, and we have consciously increased our inventories of raw materials and components in an effort to\n",
      "mitigate possible supply disruptions.\n",
      "• The start of the 2020 Formula 1 World Championship was postponed to July 5, when the Austrian Grand Prix was\n",
      "held without spectators on site. The calendar for the current season has evolved over time and currently 17 Grand\n",
      "Prix races are scheduled for 2020, which is significantly less than the 2019 season. Furthermore, the actual number\n",
      "of races that will take place is dependent on developments in the COVID-19 pandemic and therefore remains\n",
      "uncertain. Most of the races to date have taken place without spectators present and a limited number of races this\n",
      "season are expected to take place with a significantly lower number of spectators compared to normal. This has\n",
      "adversely impacted our results as we accrued sponsorship and commercial revenues during the year based on certain\n",
      "assumptions and the number of races reasonably anticipated to take place for the 2020 season.\n",
      "• Brand activities were also adversely impacted as a result of the temporary closure of Ferrari stores and museums.\n",
      "Our stores and museums gradually started to reopen in May, with appropriate safety measures in place to protect our\n",
      "staff and customers. To date, in-store traffic and museum visitors remain significantly lower than pre-pandemic\n",
      "levels. This has been only partially offset by an increase in online sales of our merchandise.\n",
      "• Although production and certain other activities (i.e. Formula 1, stores, museums) were temporarily suspended, the\n",
      "Group has been able to continue many other key business activities and functions through remote working\n",
      "arrangements.\n",
      "• Ferrari continues to take measures to combat the spread of COVID-19 at its facilities, and in line with the laws and\n",
      "regulations enacted in Italy and other countries where the Company operates, Ferrari is continuing to guarantee the\n",
      "possibility of remote work for those employees whose job activity is compatible with such work arrangements.\n",
      "• There were no significant effects on the valuation of assets or liabilities and no increases in allowances for credit\n",
      "losses as of September 30, 2020. Moreover, no material impairment indicators were identified and there were no\n",
      "changes in accounting judgments or other significant accounting impacts relating to COVID-19.\n",
      "• No significant changes occurred in controls that materially affect internal control over financial reporting.\n",
      "For further information on the impact of the COVID-19 pandemic on our results of operations and liquidity, see “-\n",
      "Results of Operations” and “-Liquidity and Capital Resources”.\n",
      "The future impacts of COVID-19 on Ferrari’s results of operations and financial condition will depend on ongoing\n",
      "developments in relation to the pandemic, including the success of global containment measures and other actions taken by\n",
      "governments around the world, as well as the overall condition and outlook of the global economy. As further described\n",
      "under “-Risk Factors”, “We are subject to risks related to the evolution of and response to the coronavirus COVID-19\n",
      "pandemic that may materially and adversely affect our business” Ferrari’s performance will continue to be negatively\n",
      "affected in 2020 and possibly beyond. A so called “second wave” of the COVID-19 pandemic is being experienced in several\n",
      "European countries, including Italy, as well as in the United States and elsewhere, leading to a return of restrictive measures\n",
      "which may intensify over the coming periods. Significant uncertainty remains, especially in relation to Ferrari’s Formula 1\n",
      "and brand activities, as well as our supply chain, and the situation is evolving continuously. A reduced number of Formula 1\n",
      "races will be held for the 2020 season and the Group expects that brand activities will recover slowly towards pre-pandemic\n",
      "levels. For the entire month of April our production remained suspended. The closure and reopening of Ferrari dealerships\n",
      "worldwide as a result of lockdowns and other restrictions, and the gradual easing of those measures, were implemented to\n",
      "varying degrees from country to country. Substantially all Ferrari dealerships have reopened and order collections have\n",
      "resumed. However, new closures have recently been made necessary as a result of the resurgence of the pandemic in certain\n",
      "7\n",
      "\n",
      "territories. The waiting list for cars continues to extend beyond 12 months on average and the Group remains focused on\n",
      "maintaining a robust order book going forward. Ferrari has started to recover the effects of the COVID-19-related suspension\n",
      "of business activities during the third quarter of 2020.\n",
      "To preventively and prudently manage potential liquidity or refinancing risks in the foreseeable future, the Group\n",
      "has increased its available liquidity, which amounted to €1,879 million at September 30, 2020 (compared to €1,812 million at\n",
      "June 30, 2020 and €1,248 million at December 31, 2019), primarily as a result of:\n",
      "• increasing new undrawn committed credit lines to €700 million in April 2020 (€350 million at December 31, 2019);\n",
      "• the issuance of a €650 million bond in May 2020.\n",
      "Additionally, the Group elected to temporarily suspend its share repurchase program effective from March 30, 2020.\n",
      "Furthermore, we have taken actions to contain SG&A, R&D and capital expenditures in the remainder of 2020, while\n",
      "ensuring that all projects that are considered important for the continuing success of Ferrari and its future development are\n",
      "maintained. Ferrari has continued to manage financial risks generated by interest rates or foreign exchange fluctuations\n",
      "throughout the pandemic in line with Ferrari’s hedging policy. Management is continuously monitoring the evolution of\n",
      "COVID-19 as information becomes available as well as the related effects on the results of operations and financial position\n",
      "of the Group.\n",
      "To protect the health and well-being of its workforce and customers as Ferrari returned to business operations, the\n",
      "Company successfully implemented its “Back on Track” program, which facilitated our return to full production by May 8,\n",
      "2020. The program was developed in partnership with several virologists and other medical experts to ensure the highest\n",
      "level of safety for Ferrari employees, their families, Ferrari customers and suppliers and the greater community at large. The\n",
      "program includes the following measures:\n",
      "• full implementation of the Italian government’s ‘Protocol for the regulation of measures to combat and contain the\n",
      "spread of the COVID-19 virus in the workplace’, with additional measures to strengthen and customize the protocol\n",
      "with the support of specialists who have expert knowledge of Ferrari’s work environment;\n",
      "• voluntary serological testing of Ferrari employees, their family members, and suppliers; this testing takes place at\n",
      "the Fiorano Circuit, in a specially created facility of approximately 1,000m2, by doctors and health workers;\n",
      "• providing health and psychological assistance service to staff and special support to any employee who tests positive\n",
      "for COVID-19 (including free insurance coverage, accommodation for self-isolation, medical and nursing services\n",
      "and supply of required medical equipment such as medicines, oximeters and, in case of emergency, oxygen);\n",
      "• the launch of an “Installation Lap” phase, which includes several days of safety training (primarily for employees\n",
      "involved in the resumption of production from May 4) and the provision of personal protective equipment for\n",
      "employees, as well as the implementation of checks at workstation entrances and rules for sharing common areas.\n",
      "To date, the costs incurred to implement the Back on Track program have not had a significant impact on our results\n",
      "of operations or financial position.\n",
      "Ferrari continues to systematically implement actions aimed at containing the spread of the virus among Ferrari\n",
      "employees, their families and Ferrari suppliers. In addition to the COVID-19 screening activities offered by Ferrari on a\n",
      "voluntary basis to its employees, their cohabiting family members and on-site employees of suppliers, a flu vaccination\n",
      "campaign has been added, which, again on a voluntary basis, will be extended not only to its employees but also to their\n",
      "family members and employees of suppliers who frequent our manufacturing facilities.\n",
      "In response to the healthcare crisis caused by the COVID-19 pandemic and to support the communities in which\n",
      "Ferrari operates, Ferrari produced respirator valves and fittings for protective masks, and also agreed to fund various\n",
      "initiatives in the region to help those in need during the COVID-19 emergency, with the first activities concentrating on\n",
      "Ferrari’s local communities in the province of Modena. Aid to the different towns was coordinated directly with the local\n",
      "authorities and included the following, among others:\n",
      "• the purchase and distribution of ventilators, respiratory equipment, medically certified masks and other medical\n",
      "supplies, including from various overseas suppliers;\n",
      "• the purchase of COVID-19 test kits and equipment for the Policlinico di Modena and the hospitals of Baggiovara\n",
      "and Sassuolo;\n",
      "• the donation of emergency medical service vehicles for the local health service;\n",
      "8\n",
      "\n",
      "• the purchase of computer equipment for schools, including notebooks, tablets and portable modems. All of the IT\n",
      "equipment will remain with the schools;\n",
      "• the purchase and distribution of food in Maranello.\n",
      "These initiatives were partially funded thanks to the Chairman, the CEO and Board of Directors pledging their full\n",
      "cash compensation from April to the end of the year, with the remaining members of the Senior Management Team donating\n",
      "25 percent of their salaries for the same period, raising approximately €2 million.\n",
      "Ferrari also launched a collaborative fundraising initiative together with its Cavalcade clients to support the medical\n",
      "staff and health system of Ferrari’s surrounding communities, with Ferrari matching every donation made.\n",
      "Additionally, Ferrari has implemented a series of initiatives that seek to guarantee adequate support and care to its\n",
      "employees and their families, as well as local communities, including: the Company’s Formula Estate summer camp, which\n",
      "was offered to the children of Ferrari employees and was carried out in collaboration with local authorities; a program\n",
      "dedicated to the recovery of school education called “Back to School”, created by the Agnelli Foundation together with\n",
      "Ferrari and the non-profit organization Save The Children for the benefit of children in situations of fragility in the\n",
      "municipality of Maranello and surrounding area; the resumption of the Formula Benessere medical-sports prevention\n",
      "program carried out with check-ups and specialist visits available for all employees performed in full compliance with safety\n",
      "protocols; and the resumption of the company concierge service open to all employees for the handling of personal files and\n",
      "errands, useful to relieve the burden of having to manage various duties at public offices, which are now even more complex\n",
      "due to pandemic safety procedures.\n",
      "9\n",
      "\n",
      "Non-GAAP Financial Measures\n",
      "We monitor and evaluate our operating and financial performance using several non-GAAP financial measures\n",
      "including: EBITDA, Adjusted EBITDA, Adjusted EBIT, Adjusted Net Profit, Adjusted Basic and Diluted Earnings per\n",
      "Common Share, Net Debt, Net Industrial Debt, Free Cash Flow and Free Cash Flow from Industrial Activities, as well as a\n",
      "number of financial metrics measured on a constant currency basis. We believe that these non-GAAP financial measures\n",
      "provide useful and relevant information regarding our performance and our ability to assess our financial performance and\n",
      "financial position. They also provide us with comparable measures that facilitate management’s ability to identify operational\n",
      "trends, as well as make decisions regarding future spending, resource allocations and other operational decisions. While\n",
      "similar measures are widely used in the industry in which we operate, the financial measures we use may not be comparable\n",
      "to other similarly titled measures used by other companies nor are they intended to be substitutes for measures of financial\n",
      "performance or financial position as prepared in accordance with IFRS.\n",
      "EBITDA and Adjusted EBITDA\n",
      "EBITDA is defined as net profit before income tax expense, net financial expenses and amortization and\n",
      "depreciation. Adjusted EBITDA is defined as EBITDA as adjusted for certain income and costs which are significant in\n",
      "nature, expected to occur infrequently, and that management considers not reflective of ongoing operational activities.\n",
      "EBITDA is presented by management to aid investors in their analysis of the performance of the Group and to assist investors\n",
      "in the comparison of the Group’s performance with that of other companies. Adjusted EBITDA is presented to demonstrate\n",
      "how the underlying business has performed prior to the impact of the adjustments, which may obscure the underlying\n",
      "performance and impair comparability of results between periods.\n",
      "The following table sets forth the calculation of EBITDA and Adjusted EBITDA for the three and nine months\n",
      "ended September 30, 2020 and 2019, and provides a reconciliation of these non-GAAP measures to net profit. There were no\n",
      "adjustments impacting Adjusted EBITDA for the periods presented.\n",
      "For the three months ended For the nine months ended\n",
      "September 30, September 30,\n",
      "2020 2019 2020 2019\n",
      "(€ million)\n",
      "Net profit 171 169 346 533\n",
      "Income tax expense 37 42 81 133\n",
      "Net financial expenses 14 16 38 32\n",
      "Amortization and depreciation 108 84 306 238\n",
      "EBITDA and Adjusted EBITDA 330 311 771 936\n",
      "10\n",
      "\n",
      "Adjusted EBIT\n",
      "Adjusted EBIT represents EBIT as adjusted for certain income and costs which are significant in nature, expected to\n",
      "occur infrequently, and that management considers not reflective of ongoing operational activities. We provide such\n",
      "information in order to present how the underlying business has performed prior to the impact of such items, which may\n",
      "obscure the underlying performance and impair comparability of results between the periods.\n",
      "The following table sets forth the calculation of Adjusted EBIT for the three and nine months ended September 30,\n",
      "2020 and 2019. There were no adjustments impacting Adjusted EBIT for the periods presented.\n",
      "For the three months ended For the nine months ended\n",
      "September 30, September 30,\n",
      "2020 2019 2020 2019\n",
      "(€ million)\n",
      "EBIT and Adjusted EBIT 222 227 465 698\n",
      "Adjusted Net Profit\n",
      "Adjusted Net Profit represents net profit as adjusted for certain income and costs (net of tax effect) which are\n",
      "significant in nature, expected to occur infrequently, and that management considers not reflective of ongoing operational\n",
      "activities. We provide such information in order to present how the underlying business has performed prior to the impact of\n",
      "such items, which may obscure the underlying performance and impair comparability of results between the periods.\n",
      "The following table sets forth the calculation of Adjusted Net Profit for the three and nine months ended September\n",
      "30, 2020 and 2019. There were no adjustments impacting Adjusted Net Profit for the periods presented.\n",
      "For the three months ended For the nine months ended\n",
      "September 30, September 30,\n",
      "2020 2019 2020 2019\n",
      "(€ million)\n",
      "Net profit and Adjusted Net Profit 171 169 346 533\n",
      "11\n",
      "\n",
      "Adjusted Basic and Diluted Earnings per Common Share\n",
      "Adjusted Basic and Diluted Earnings per Common Share represent earnings per share, as adjusted for certain income\n",
      "and costs (net of tax effect) which are significant in nature, expected to occur infrequently, and that management considers\n",
      "not reflective of ongoing operational activities. We provide such information in order to present how the underlying business\n",
      "has performed prior to the impact of such items, which may obscure the underlying performance and impair comparability of\n",
      "results between the periods.\n",
      "The following table sets forth the calculation of Adjusted Basic and Diluted Earnings per Common Share for the\n",
      "three and nine months ended September 30, 2020 and 2019. There were no adjustments impacting Adjusted Basic and\n",
      "Diluted Earnings per Common Share for the periods presented.\n",
      "For the three months ended For the nine months ended\n",
      "September 30, September 30,\n",
      "2020 2019 2020 2019\n",
      "Net profit attributable to owners of the Company € million 171 168 346 529\n",
      "Adjusted net profit attributable to owners of the\n",
      "€ million 171 168 346 529\n",
      "Company\n",
      "Weighted average number of common shares for basic\n",
      "thousand 184,748 186,504 184,825 187,196\n",
      "earnings per common share\n",
      "Adjusted basic earnings per common share € 0.92 0.90 1.87 2.82\n",
      "Weighted average number of common shares(1) for\n",
      "thousand 185,344 187,302 185,422 187,994\n",
      "diluted earnings per common share\n",
      "Adjusted diluted earnings per common share € 0.92 0.90 1.86 2.81\n",
      "(1) For the three and nine months ended September 30, 2020 and 2019 the weighted average number of common shares for diluted earnings per share was\n",
      "increased to take into consideration the theoretical effect of the potential common shares that would be issued under the Group’s equity incentive plans.\n",
      "Net Debt and Net Industrial Debt\n",
      "Due to different sources of cash flows used for the repayment of debt between industrial activities and financial\n",
      "services activities, and the different business structure and leverage implications, Net Industrial Debt, together with Net Debt,\n",
      "are the primary measures used by us to analyze our capital structure and financial leverage. We believe the presentation of\n",
      "Net Industrial Debt aids management and investors in their analysis of the Group’s financial position and financial\n",
      "performance and to compare the Group’s financial position and financial performance with that of other companies. Net\n",
      "Industrial Debt is defined as total debt less total cash and cash equivalents (Net Debt), further adjusted to exclude the debt\n",
      "and cash and cash equivalents related to our financial services activities (Net Debt of Financial Services Activities).\n",
      "The following table sets forth a reconciliation of Net Debt and Net Industrial Debt at September 30, 2020 and\n",
      "December 31, 2019.\n",
      "At September 30, At December 31,\n",
      "2020 2019\n",
      "(€ million)\n",
      "Cash and cash equivalents 1,179 898\n",
      "Debt (2,741) (2,090)\n",
      "Net Debt (A) (1,562) (1,192)\n",
      "Net Debt of Financial Services Activities (B) (847) (855)\n",
      "Net Industrial Debt (A-B) (715) (337)\n",
      "Free Cash Flow and Free Cash Flow from Industrial Activities\n",
      "Free Cash Flow and Free Cash Flow from Industrial Activities are two of our primary key performance indicators to\n",
      "measure the Group’s performance. These measures are presented by management to aid investors in their analysis of the\n",
      "Group’s financial performance and to compare the Group’s financial performance with that of other companies. Free Cash\n",
      "12\n",
      "\n",
      "Flow is defined as cash flows from operating activities less investments in property, plant and equipment (excluding right-of-\n",
      "use assets recognized during the period in accordance with IFRS 16 – Leases) and intangible assets. Free Cash Flow from\n",
      "Industrial Activities is defined as Free Cash Flow adjusted to exclude the operating cash flow from our financial services\n",
      "activities (Free Cash Flow from Financial Services Activities). Prior to the first quarter of 2020, we defined Free Cash Flow\n",
      "and Free Cash Flow from Industrial Activities without excluding from investments in property, plant and equipment the right-\n",
      "of-use assets recognized during the period in accordance with IFRS 16 – Leases. Applying the current definition of Free Cash\n",
      "Flow and Free Cash Flow from Industrial Activities to the nine months ended September 30, 2019 would result in an\n",
      "immaterial difference compared to the figures presented below.\n",
      "The following table sets forth our Free Cash Flow and Free Cash Flow from Industrial Activities for the nine months\n",
      "ended September 30, 2020 and 2019.\n",
      "For the nine months ended September 30,\n",
      "2020 2019\n",
      "(€ million)\n",
      "Cash flows from operating activities 427 949\n",
      "Investments in property, plant and equipment and intangible assets (465) (453)\n",
      "Free Cash Flow (38) 496\n",
      "Free Cash Flow from Financial Services Activities (29) (63)\n",
      "Free Cash Flow from Industrial Activities (9) 559\n",
      "Constant Currency Information\n",
      "The “Results of Operations” discussion below includes information about our net revenues on a constant currency\n",
      "basis, which eliminates the effects of foreign currency translation from our subsidiaries with functional currencies other than\n",
      "Euro, as well as the effects of foreign currency transaction impact and foreign currency hedging. We use this information to\n",
      "assess how the underlying revenues changed independent of fluctuations in foreign currency exchange rates and hedging. We\n",
      "calculate constant currency by (i) applying the prior-period average foreign currency exchange rates to translate current\n",
      "period revenues of foreign subsidiaries expressed in local functional currency other than Euro, (ii) applying the prior-period\n",
      "average foreign currency exchange rates to current period revenues originated in a currency other than the functional\n",
      "currency of the applicable entity, and (iii) eliminating the variances of any foreign currency hedging (see Note 5 “Other\n",
      "Information” to the Interim Condensed Consolidated Financial Statements, included in this Interim Report, for information\n",
      "on the foreign currency exchange rates applied). Although we do not believe that these measures are a substitute for GAAP\n",
      "measures, we do believe that revenues excluding the impact of currency fluctuations and the impacts of hedging provide\n",
      "additional useful information to investors regarding the operating performance on a local currency basis.\n",
      "13\n",
      "\n",
      "Results of Operations\n",
      "Three months ended September 30, 2020 compared to three months ended September 30, 2019\n",
      "The following is a discussion of the results of operations for the three months ended September 30, 2020 compared to\n",
      "the three months ended September 30, 2019. The presentation includes line items as a percentage of net revenues for the\n",
      "respective periods presented to facilitate period-to-period comparisons.\n",
      "For the three months ended September 30,\n",
      "Percentage of net Percentage of net\n",
      "2020 2019\n",
      "revenues revenues\n",
      "(€ million, except percentages)\n",
      "Net revenues 888 100.0 % 915 100.0 %\n",
      "Cost of sales 426 47.9 % 425 46.5 %\n",
      "Selling, general and administrative costs 77 8.6 % 96 10.5 %\n",
      "Research and development costs 158 17.7 % 162 17.7 %\n",
      "Other expenses/(income), net 7 0.9 % 6 0.6 %\n",
      "Result from investments 2 0.1 % 1 0.1 %\n",
      "EBIT 222 25.0 % 227 24.8 %\n",
      "Net financial expenses 14 1.5 % 16 1.7 %\n",
      "Profit before taxes 208 23.5 % 211 23.1 %\n",
      "Income tax expense 37 4.2 % 42 4.6 %\n",
      "Net profit 171 19.3 % 169 18.5 %\n",
      "Net revenues\n",
      "For the three months ended September 30, Increase/(Decrease)\n",
      "Percentage Percentage\n",
      "2020 of net 2019 of net 2020 vs. 2019\n",
      "revenues revenues\n",
      "(€ million, except percentages)\n",
      "Cars and spare parts (1) 727 81.8 % 708 77.3 % 19 2.6 %\n",
      "Engines (2) 44 5.0 % 46 5.0 % (2) (4.4) %\n",
      "Sponsorship, commercial and brand (3) 93 10.5 % 135 14.8 % (42) (30.9) %\n",
      "Other (4) 24 2.7 % 26 2.9 % (2) (7.6) %\n",
      "Total net revenues 888 100.0 % 915 100.0 % (27) (3.0) %\n",
      "_____________________________\n",
      "(1) Includes the net revenues generated from shipments of our cars, including any personalization net revenue generated on cars, as well as sales of spare\n",
      "parts.\n",
      "(2) Includes net revenues generated from the sale of engines to Maserati for use in their cars, and the net revenues generated from the rental of engines to\n",
      "other Formula 1 racing teams.\n",
      "(3) Includes net revenues earned by our Formula 1 racing team through sponsorship agreements and our share of the Formula 1 World Championship\n",
      "commercial revenues as well as net revenues generated through the Ferrari brand, including merchandising, licensing and royalty income.\n",
      "(4) Primarily relates to financial services activities and management of the Mugello racetrack and other sports-related activities.\n",
      "Net revenues for the three months ended September 30, 2020 were €888 million, a decrease of €27 million, or 3.0\n",
      "percent (a decrease of 3.2 percent on a constant currency basis), from €915 million for the three months ended September 30,\n",
      "2019.\n",
      "The change in net revenues was attributable to the combination of (i) a €19 million increase in cars and spare parts,\n",
      "(ii) a €2 million decrease in engines, (iii) a €42 million decrease in sponsorship, commercial and brand, and (iv) a €2 million\n",
      "decrease in other.\n",
      "Cars and spare parts\n",
      "Net revenues generated from cars and spare parts were €727 million for the three months ended September 30, 2020,\n",
      "an increase of €19 million, or 2.6 percent, from €708 million for the three months ended September 30, 2019.\n",
      "14\n",
      "\n",
      "The increase in net revenues was primarily attributable to positive mix driven by the Ferrari Monza SP1 and SP2,\n",
      "partially offset by lower volumes, mainly due to the gradual phase out of the 488 Pista and 488 Pista Spider, which also\n",
      "implies a lower contribution from personalizations, as well as the Ferrari Portofino, which are approaching the end of their\n",
      "respective lifecycles.\n",
      "Overall, shipments decreased by 161 cars (6.5 percent) compared to the prior year, driven by the cadence of our full\n",
      "year production plan, which projects a recovery of 500 units out of 2,000 units lost following the seven-week production\n",
      "suspension as a result of the COVID-19 pandemic. The performance was driven by a 12.8 percent decrease in V8 models that\n",
      "was partially offset by a 15.4 percent increase in V12 models. The decrease in shipment reflects the previously mentioned\n",
      "gradual phase out of the 488 Pista and 488 Pista Spider, as well as fewer shipments of the Ferrari Portofino, which have\n",
      "essentially reached the end of their respective lifecycles, partially offset by the ramp up of the F8 Spider and 812 GTS,\n",
      "primarily in EMEA, as well as deliveries of the Ferrari Monza SP1 and SP2.\n",
      "The €19 million increase in net revenues was composed of: (i) a €110 million increase in EMEA, and (ii) a €4 million\n",
      "increase in the Rest of APAC, partially offset by (iii) an €82 million decrease in Americas, and (iv) a €13 million decrease in\n",
      "Mainland China, Hong Kong and Taiwan. Net revenues by geography were impacted by the deliberate geographic allocations\n",
      "driven by the phase-in pace of individual models.\n",
      "Engines\n",
      "Net revenues generated from engines of €44 million for the three months ended September 30, 2020 were\n",
      "substantially in line with €46 million for the three months ended September 30, 2019.\n",
      "Sponsorship, commercial and brand\n",
      "Net revenues generated from sponsorship, commercial agreements and brand management activities were €93 million\n",
      "for the three months ended September 30, 2020, a decrease of €42 million, or 30.9 percent, from €135 million for the three\n",
      "months ended September 30, 2019. The decrease was primarily attributable to the impacts of the COVID-19 pandemic\n",
      "resulting in a reduced number of Formula 1 races (17 races are currently confirmed, most of which without spectators on site)\n",
      "and corresponding lower revenue accrual - albeit improved in the quarter based on updated estimates - as well as reduced in-\n",
      "store traffic and museum visitors.\n",
      "Other\n",
      "Other net revenues, which primarily relate to financial services activities and management of the Mugello racetrack\n",
      "and other sports-related activities, were €24 million for the three months ended September 30, 2020, a decrease of €2 million,\n",
      "or 7.6 percent, from €26 million for the three months ended September 30, 2019.\n",
      "Cost of sales\n",
      "For the three months ended September 30, Increase/(Decrease)\n",
      "Percentage Percentage\n",
      "2020 of net 2019 of net 2020 vs. 2019\n",
      "revenues revenues\n",
      "(€ million, except percentages)\n",
      "Cost of sales 426 47.9 % 425 46.5 % 1 — %\n",
      "Cost of sales for the three months ended September 30, 2020 was €426 million, substantially in line with €425\n",
      "million for the three months ended September 30, 2019. As a percentage of net revenues, cost of sales was 47.9 percent for the\n",
      "three months ended September 30, 2020 compared to 46.5 percent for the three months ended September 30, 2019.\n",
      "Cost of sales was substantially in line with the prior year as the effect of reduced volumes was substantially offset by\n",
      "higher depreciation and mix impact.\n",
      "15\n",
      "\n",
      "Selling, general and administrative costs\n",
      "For the three months ended September 30, Increase/(Decrease)\n",
      "Percentage Percentage\n",
      "2020 of net 2019 of net 2020 vs. 2019\n",
      "revenues revenues\n",
      "(€ million, except percentages)\n",
      "Selling, general and administrative\n",
      "77 8.6 % 96 10.5 % (19) (20.2) %\n",
      "costs\n",
      "Selling, general and administrative costs for the three months ended September 30, 2020 were €77 million, a decrease\n",
      "of €19 million, or 20.2 percent, from €96 million for the three months ended September 30, 2019. As a percentage of net\n",
      "revenues, selling, general and administrative costs were 8.6 percent for the three months ended September 30, 2020 compared\n",
      "to 10.5 percent for the three months ended September 30, 2019.\n",
      "The decrease in selling, general and administrative costs was primarily attributable to lower costs for marketing\n",
      "initiatives as well as to the deployment of cost containment measures.\n",
      "Research and development costs\n",
      "For the three months ended September 30, Increase/(Decrease)\n",
      "Percentage Percentage\n",
      "2020 of net 2019 of net 2020 vs. 2019\n",
      "revenues revenues\n",
      "(€ million, except percentages)\n",
      "Research and development costs\n",
      "111 12.5 % 129 14.1 % (18) (14.3) %\n",
      "expensed during the period\n",
      "Amortization of capitalized\n",
      "47 5.2 % 33 3.6 % 14 41.0 %\n",
      "development costs\n",
      "Research and development costs 158 17.7 % 162 17.7 % (4) 3.1 %\n",
      "Research and development costs for the three months ended September 30, 2020 were €158 million, a decrease of €4\n",
      "million, or 3.1 percent, from €162 million for the three months ended September 30, 2019. As a percentage of net revenues,\n",
      "research and development costs were 17.7 percent for the three months ended September 30, 2020 and 2019.\n",
      "The decrease of €4 million in research and development costs during the period was primarily attributable to the\n",
      "spending cadence in Formula 1 racing activities, partially offset by an increase in amortization of capitalized development\n",
      "costs.\n",
      "We continue to invest in research and development projects that are considered important for the continuing success\n",
      "of Ferrari and its future development, despite certain actions taken to contain costs as a result of the COVID-19 pandemic.\n",
      "Other expenses/(income), net\n",
      "For the three months ended\n",
      "Increase/(Decrease)\n",
      "September 30,\n",
      "2020 2019 2020 vs. 2019\n",
      "(€ million, except percentages)\n",
      "Other expenses/(income), net 7 6 1 33.1%\n",
      "Other expenses/(income), net for the three months ended September 30, 2020 included other expenses of €8 million,\n",
      "mainly related to indirect taxes, provisions and other miscellaneous expenses, partially offset by other income of €1 million.\n",
      "Other expenses/(income), net for the three months ended September 30, 2019 included other expenses of €7 million,\n",
      "mainly related to indirect taxes and other miscellaneous expenses, partially offset by other income of €1 million.\n",
      "16\n",
      "\n",
      "EBIT\n",
      "For the three months ended September 30, Increase/(Decrease)\n",
      "Percentage Percentage\n",
      "2020 of net 2019 of net 2020 vs. 2019\n",
      "revenues revenues\n",
      "(€ million, except percentages)\n",
      "EBIT 222 25.0 % 227 24.8 % (5) (2.1) %\n",
      "EBIT for the three months ended September 30, 2020 was €222 million, a decrease of €5 million, or 2.1 percent, from\n",
      "€227 million for the three months ended September 30, 2019. EBIT margin for the three months ended September 30, 2020\n",
      "was 25.0 percent compared to 24.8 percent for the three months ended September 30, 2019.\n",
      "The decrease in EBIT was primarily attributable to the combined effects of (i) negative volume impact of €19 million,\n",
      "driven by a decrease in shipments, (ii) positive product mix and price impact of €46 million, (iii) an increase in industrial costs\n",
      "of €15 million, including higher depreciation, (iv) a decrease in research and development costs of €4 million, (v) a decrease in\n",
      "selling, general and administrative costs of €19 million, (vi) negative contribution of €45 million due to the impacts of\n",
      "COVID-19 on the Formula 1 racing calendar and lower traffic for brand related activities, and (vii) positive foreign currency\n",
      "exchange impact of €5 million (including foreign currency hedging instruments) primarily driven by the strengthening of the\n",
      "U.S. Dollar compared to the Euro.\n",
      "The positive product mix and price impact was primarily driven by deliveries of the Ferrari Monza SP1 and SP2,\n",
      "partially offset by lower contributions from our personalization programs, which are correlated to the decrease in volumes, as\n",
      "well as the gradual phase out of the 488 Pista and 488 Pista Spider, which are approaching the end of their lifecycle.\n",
      "Net financial expenses\n",
      "For the three months ended\n",
      "Increase/(Decrease)\n",
      "September 30,\n",
      "2020 2019 2020 vs. 2019\n",
      "(€ million, except percentages)\n",
      "Net financial expenses 14 16 (2) (14.3)%\n",
      "Net financial expenses for the three months ended September 30, 2020 decreased to €14 million compared to €16\n",
      "million for the three months ended September 30, 2019, primarily due to lower interest expenses in the third quarter of 2020\n",
      "compared to the third quarter of 2019, which was impacted by costs for bond repurchases (repurchase price, premium and\n",
      "previously unamortized issuance costs), partially offset by higher interest expenses incurred as a result of the decision to early\n",
      "refinance part of the upcoming debt maturities and to secure longer tenors through a public issuance (2025 Bond issued in May\n",
      "2020) and higher net foreign exchange losses, including the net costs of hedging.\n",
      "Income tax expense\n",
      "For the three months ended\n",
      "Increase/(Decrease)\n",
      "September 30,\n",
      "2020 2019 2020 vs. 2019\n",
      "(€ million, except percentages)\n",
      "Income tax expense 37 42 (5) (11.3)%\n",
      "Income tax expense for the three months ended September 30, 2020 was €37 million compared to €42 million for the\n",
      "three months ended September 30, 2019. The decrease in income tax expense was primarily attributable to deductions for\n",
      "eligible research and development costs and for hyper- and super-depreciation of fixed assets. The effective tax rate was 18.0\n",
      "percent and 20.0 percent for the three months ended September 30, 2020 and 2019, respectively. Income taxes for both the\n",
      "three months ended September 30, 2020 and 2019 benefited from the application of the Patent Box tax regime.\n",
      "17\n",
      "\n",
      "Nine months ended September 30, 2020 compared to nine months ended September 30, 2019\n",
      "The following is a discussion of the results of operations for the nine months ended September 30, 2020 compared to\n",
      "the nine months ended September 30, 2019. The presentation includes line items as a percentage of net revenues for the\n",
      "respective periods presented to facilitate period-to-period comparisons.\n",
      "As a result of the temporary suspension of production and shipments, as well as the changes to the calendar and\n",
      "format of the 2020 Formula 1 World Championship, caused by the COVID-19 pandemic, revenues were significantly reduced\n",
      "and, as a consequence, costs as a percentage of net revenues increased during the nine months ended September 30, 2020\n",
      "compared to the nine months ended September 30, 2019. Furthermore, a portion of our costs are fixed in nature and we\n",
      "decided to pay all employees throughout the whole suspension period and not accede to any government aid programs,\n",
      "therefore management actions to reduce costs only partially compensated the decrease in net revenues. Consequently, our\n",
      "EBIT and EBIT margin decreased compared to the same period of the prior year.\n",
      "For the nine months ended September 30,\n",
      "Percentage of net Percentage of net\n",
      "2020 2019\n",
      "revenues revenues\n",
      "(€ million, except percentages)\n",
      "Net revenues 2,391 100.0 % 2,839 100.0 %\n",
      "Cost of sales 1,176 49.2 % 1,367 48.2 %\n",
      "Selling, general and administrative costs 234 9.8 % 255 9.0 %\n",
      "Research and development costs 505 21.1 % 517 18.2 %\n",
      "Other expenses/(income), net 15 0.5 % 4 0.1 %\n",
      "Result from investments 4 0.1 % 2 0.1 %\n",
      "EBIT 465 19.5 % 698 24.6 %\n",
      "Net financial expenses 38 1.6 % 32 1.1 %\n",
      "Profit before taxes 427 17.9 % 666 23.5 %\n",
      "Income tax expense 81 3.4 % 133 4.7 %\n",
      "Net profit 346 14.5 % 533 18.8 %\n",
      "Net revenues\n",
      "For the nine months ended September 30, Increase/(Decrease)\n",
      "Percentage Percentage\n",
      "2020 of net 2019 of net 2020 vs. 2019\n",
      "revenues revenues\n",
      "(€ million, except percentages)\n",
      "Cars and spare parts (1) 1,965 82.2 % 2,209 77.8 % (244) (11.1) %\n",
      "Engines (2) 97 4.1 % 157 5.5 % (60) (37.9) %\n",
      "Sponsorship, commercial and brand (3) 265 11.1 % 394 13.9 % (129) (32.6) %\n",
      "Other (4) 64 2.6 % 79 2.8 % (15) (19.5) %\n",
      "Total net revenues 2,391 100.0 % 2,839 100.0 % (448) (15.8) %\n",
      "_____________________________\n",
      "(1) Includes net revenues generated from shipments of our cars, including any personalization net revenue generated on cars as well as sales of spare\n",
      "parts.\n",
      "(2) Includes net revenues generated from the sale of engines to Maserati for use in their cars, and the revenues generated from the rental of engines to\n",
      "other Formula 1 racing teams.\n",
      "(3) Includes net revenues earned by our Formula 1 racing team through sponsorship agreements and our share of the Formula 1 World Championship\n",
      "commercial revenues as well as net revenues generated through the Ferrari brand, including merchandising, licensing and royalty income.\n",
      "(4) Primarily relates to financial services activities and management of the Mugello racetrack and other sports-related activities.\n",
      "Net revenues for the nine months ended September 30, 2020 were €2,391 million, a decrease of €448 million, or 15.8\n",
      "percent (a decrease of 16.7 percent on a constant currency basis), from €2,839 million for the nine months ended September\n",
      "30, 2019.\n",
      "18\n",
      "\n",
      "The change in net revenues was attributable to the combination of (i) a €244 million decrease in cars and spare parts,\n",
      "(ii) a €60 million decrease in engines (iii) a €129 million decrease in sponsorship, commercial and brand, and (iv) a €15\n",
      "million decrease in other.\n",
      "Cars and spare parts\n",
      "Net revenues generated from cars and spare parts were €1,965 million for the nine months ended September 30, 2020\n",
      "a decrease of €244 million, or 11.1 percent, from €2,209 million for the nine months ended September 30, 2019.\n",
      "The decrease in net revenues was primarily attributable to lower volumes and related reduced contribution from our\n",
      "personalization programs, driven by the temporary suspension of production and shipments caused by the COVID-19\n",
      "pandemic, as well as fewer shipments of the FXX-K EVO, partially offset by deliveries of the Ferrari Monza SP1 and SP2.\n",
      "Overall, shipments decreased by 1,315 cars (17.0 percent) compared to the prior year, driven by the cadence of our\n",
      "full year production plan, which projects a recovery of 500 units out of 2,000 units lost following the seven-week production\n",
      "suspension as a result of the COVID-19 pandemic. The performance was driven by a 19.5 percent decrease in V8 models and\n",
      "an 8.5 percent decrease in V12 models. The decrease in shipments also reflects the gradual phase out of the 488 Pista and 488\n",
      "Pista Spider, as well as fewer shipments of the Ferrari Portofino, which have essentially reached the end of their respective\n",
      "lifecycles, partially offset by the ramp up of the F8 Spider and 812 GTS, primarily in EMEA, as well as deliveries of the\n",
      "Ferrari Monza SP1 and SP2.\n",
      "The €244 million decrease in net revenues was composed of (i) a €153 million decrease in Americas (including\n",
      "positive foreign currency translation impact driven by the strengthening of the U.S. Dollar compared to the Euro), and (ii) a\n",
      "€207 million decrease in Mainland China, Hong Kong and Taiwan, partially offset by (iii) a €101 million increase in EMEA,\n",
      "and (iv) a €15 million increase in the Rest of APAC. Net revenues by geography were impacted by the deliberate geographic\n",
      "allocations driven by the phase-in pace of individual models. The decrease in Mainland China, Hong Kong and Taiwan was\n",
      "primarily impacted by the decision to deliberately accelerate client deliveries in the first half of 2019, in addition to the effects\n",
      "of COVID-19 in 2020.\n",
      "Engines\n",
      "Net revenues generated from engines were €97 million for the nine months ended September 30, 2020, a decrease of\n",
      "€60 million, or 37.9 percent, from €157 million for the nine months ended September 30, 2019. The decrease was attributable\n",
      "to lower shipments of engines to Maserati and lower revenues from the rental of engines to other Formula 1 racing teams.\n",
      "Sponsorship, commercial and brand\n",
      "Net revenues generated from sponsorship, commercial agreements and brand management activities were €265\n",
      "million for the nine months ended September 30, 2020, a decrease of €129 million, or 32.6 percent, from €394 million for the\n",
      "nine months ended September 30, 2019. The decrease was primarily attributable to impacts of the COVID-19 pandemic\n",
      "resulting in a reduced number of Formula 1 races and corresponding lower revenue accrual as well as reduced in-store traffic\n",
      "and museum visitors.\n",
      "Other\n",
      "Other net revenues, which primarily relate to financial services activities and management of the Mugello racetrack\n",
      "and other sports-related activities, were €64 million for the nine months ended September 30, 2020, a decrease of €15 million,\n",
      "or 19.5 percent, from €79 million for the nine months ended September 30, 2019. The decrease was primarily attributable to\n",
      "the cancellation of the Moto GP event at the Mugello racetrack and reduced sports-related activities, only partially offset by\n",
      "the first ever Formula 1 grand prix held at the Mugello racetrack.\n",
      "Cost of sales\n",
      "For the nine months ended September 30, Increase/(Decrease)\n",
      "Percentage Percentage\n",
      "2020 of net 2019 of net 2020 vs. 2019\n",
      "revenues revenues\n",
      "(€ million, except percentages)\n",
      "Cost of sales 1,176 49.2 % 1,367 48.2 % (191) (14.0) %\n",
      "19\n",
      "\n",
      "Cost of sales for nine months ended September 30, 2020 was €1,176 million, a decrease of €191 million, or 14.0\n",
      "percent, from €1,367 million for the nine months ended September 30, 2019. As a percentage of net revenues, cost of sales\n",
      "were 49.2 percent, for the nine months ended September 30, 2020 compared to 48.2 percent for the nine months ended\n",
      "September 30, 2019.\n",
      "The decrease in cost of sales was primarily attributable to a decrease in car volumes due to COVID-19 and lower\n",
      "engine volumes produced for Maserati, partially offset by higher depreciation.\n",
      "Selling, general and administrative costs\n",
      "For the nine months ended September 30, Increase/(Decrease)\n",
      "Percentage Percentage\n",
      "2020 of net 2019 of net 2020 vs. 2019\n",
      "revenues revenues\n",
      "(€ million, except percentages)\n",
      "Selling, general and administrative\n",
      "234 9.8 % 255 9.0 % (21) (8.4) %\n",
      "costs\n",
      "Selling, general and administrative costs for the nine months ended September 30, 2020 were €234 million, a\n",
      "decrease of €21 million, or 8.4 percent, from €255 million for the nine months ended September 30, 2019. As a percentage of\n",
      "net revenues, selling, general and administrative costs were 9.8 percent for the nine months ended September 30, 2020\n",
      "compared to 9.0 percent for the nine months ended September 30, 2019.\n",
      "The decrease in selling, general and administrative costs was primarily attributable to lower costs for marketing\n",
      "initiatives as well as to the deployment of cost containment measures.\n",
      "Research and development costs\n",
      "For the nine months ended September 30, Increase/(Decrease)\n",
      "Percentage Percentage\n",
      "2020 of net 2019 of net 2020 vs. 2019\n",
      "revenues revenues\n",
      "(€ million, except percentages)\n",
      "Research and development costs\n",
      "373 15.6 % 423 14.9 % (50) (11.9) %\n",
      "expensed during the period\n",
      "Amortization of capitalized\n",
      "132 5.5 % 94 3.3 % 38 40.4 %\n",
      "development costs\n",
      "Research and development costs 505 21.1 % 517 18.2 % (12) (2.4) %\n",
      "Research and development costs for the nine months ended September 30, 2020 were €505 million, a decrease of €12\n",
      "million, or (2.4) percent, from €517 million for the nine months ended September 30, 2019. As a percentage of net revenues,\n",
      "research and development costs were 21.1 percent for the nine months ended September 30, 2020 compared to 18.2 percent for\n",
      "the nine months ended September 30, 2019.\n",
      "The decrease of €12 million in research and development costs during the period was primarily attributable to lower\n",
      "research and development costs expensed during the period of €50 million, including the effects of technology incentives\n",
      "recognized in 2020, partially offset by an increase in amortization of capitalized development costs of €38 million.\n",
      "We continue to invest in research and development projects that are considered important for the continuing success\n",
      "of Ferrari and its future development, despite certain actions taken to contain costs as a result of the COVID-19 pandemic.\n",
      "Other expenses/(income), net\n",
      "For the nine months ended\n",
      "Increase/(Decrease)\n",
      "September 30,\n",
      "2020 2019 2020 vs. 2019\n",
      "(€ million, except percentages)\n",
      "Other expenses/(income), net 15 4 11 n.m.\n",
      "20\n",
      "\n",
      "Other expenses/(income), net for the nine months ended September 30, 2020 included other expenses of €18 million,\n",
      "mainly related to indirect taxes, provisions and other miscellaneous expenses, partially offset by other income of €3 million.\n",
      "Other expenses/(income), net for the nine months ended September 30, 2019 included other income of €17 million,\n",
      "mainly related to indirect taxes, provisions and other miscellaneous expenses, partially offset by income of €13 million, mainly\n",
      "related to a change in estimate of the risk and related provision associated with a legal dispute, based on developments that\n",
      "occurred in the first quarter of 2019, as well as other miscellaneous income.\n",
      "EBIT\n",
      "For the nine months ended September 30, Increase/(Decrease)\n",
      "Percentage Percentage\n",
      "2020 of net 2019 of net 2020 vs. 2019\n",
      "revenues revenues\n",
      "(€ million, except percentages)\n",
      "EBIT 465 19.5% 698 24.6% (233) (33.3)%\n",
      "EBIT for the nine months ended September 30, 2020 was €465 million, a decrease of €233 million, or 33.3 percent,\n",
      "from €698 million for the nine months ended September 30, 2019. EBIT margin for the nine months ended September 30,\n",
      "2020 was 19.5 percent compared to 24.6 percent for the nine months ended September 30, 2019.\n",
      "The decrease in EBIT was attributable to the combined effects of (i) negative volume impact of €159 million, (ii)\n",
      "positive product mix and price impact of €76 million, (iii) an increase in industrial costs of €53 million, including higher\n",
      "depreciation, (iv) a decrease in research and development costs of €12 million, (v) a decrease in selling, general and\n",
      "administrative costs of €21 million, (vi) negative contribution of €159 million due to the impacts of COVID-19 on the Formula\n",
      "1 racing calendar, lower traffic for brand related activities and lower engine sales to Maserati, and (vii) positive foreign\n",
      "currency exchange impact of €29 million (including foreign currency hedging instruments) primarily driven by the\n",
      "strengthening of the U.S. Dollar and Japanese Yen compared to the Euro. Industrial costs include the full cost of employees’\n",
      "paid days of absence during the COVID-19 production suspension.\n",
      "The negative volume impact was primarily attributable to the temporary suspension of shipments for seven weeks\n",
      "during the first half of 2020 as a result of the COVID-19 pandemic. The positive product mix and price impact was primarily\n",
      "attributable to deliveries of the Ferrari Monza SP1 and SP2, partially offset by lower contributions from our personalization\n",
      "programs, which are correlated to the decrease in volumes and, in particular, to certain special series models, as well as fewer\n",
      "shipments of the FXX-K EVO.\n",
      "Net financial expenses\n",
      "For the nine months ended\n",
      "Increase/(Decrease)\n",
      "September 30,\n",
      "2020 2019 2020 vs. 2019\n",
      "(€ million, except percentages)\n",
      "Net financial expenses 38 32 6 17.7%\n",
      "Net financial expenses for the nine months ended September 30, 2020 increased to €38 million compared to €32\n",
      "million for the nine months ended September 30, 2019.\n",
      "The increase in net financial expenses was primarily attributable to (i) an increase in net foreign exchange losses,\n",
      "including the net costs of hedging, and (ii) a decrease in the fair value of investments held by the Group, partially offset by (iii)\n",
      "lower interest expenses compared to the prior year, which was impacted by the costs of bond repurchases (repurchase price,\n",
      "premium, previously unamortized issuance costs).\n",
      "21\n",
      "\n",
      "Income tax expense\n",
      "For the nine months ended\n",
      "Increase/(Decrease)\n",
      "September 30,\n",
      "2020 2019 2020 vs. 2019\n",
      "(€ million, except percentages)\n",
      "Income tax expense 81 133 (52) (39.0)%\n",
      "Income tax expense for the nine months ended September 30, 2020 was €81 million compared to €133 million for the\n",
      "nine months ended September 30, 2019. The decrease in income tax expense was primarily attributable to a decrease in profit\n",
      "before taxes, as well as the effects of deductions for eligible research and development costs and the hyper- and super-\n",
      "depreciation of fixed assets. The effective tax rate was 19.0 percent and 20.0 percent for the nine months ended September 30,\n",
      "2020 and 2019. Income taxes for the nine months ended September 30, 2020 and 2019 benefited from the application of the\n",
      "Patent Box tax regime.\n",
      "Liquidity and Capital Resources\n",
      "Liquidity Overview\n",
      "We require liquidity in order to fund our operations and meet our obligations. Short-term liquidity is required to\n",
      "purchase raw materials, parts and components for car production, as well as to fund selling, general, administrative, research\n",
      "and development, and other expenses. In addition to our general working capital and operational needs, we require cash for\n",
      "capital investments to support continuous product range renewal and expansion and, more recently, for research and\n",
      "development to transition our product portfolio to hybrid and electric technology. We also make investments for initiatives to\n",
      "enhance manufacturing efficiency, improve capacity, ensure environmental compliance and carry out maintenance activities.\n",
      "We fund our capital expenditures primarily with cash generated from our operating activities.\n",
      "We centrally manage our operating cash management, liquidity and cash flow requirements with the objective of\n",
      "ensuring efficient and effective management of our funds. We believe that our cash generation together with our available\n",
      "liquidity, including committed credit lines granted from primary financial institutions, will be sufficient to meet our\n",
      "obligations and fund our business and capital expenditures.\n",
      "See the “Net Debt and Net Industrial Debt” section below for additional details relating to our liquidity.\n",
      "Cyclical Nature of Our Cash Flows\n",
      "Our working capital is subject to month to month fluctuations due to, among other things, production and sales\n",
      "volumes, our financial services activities, the timing of capital expenditures and tax payments. In particular, our inventory\n",
      "levels increase in the periods leading up to launches of new models, during the phase out of existing models when we build up\n",
      "spare parts, and at the end of the second quarter when our inventory levels are generally higher to support the summer plant\n",
      "shutdown. The impacts of the COVID-19 pandemic on our working capital were greater in the second quarter due to the\n",
      "continued suspension of our production and shipments until early May 2020 whilst they were only limited in the first quarter\n",
      "of 2020.\n",
      "We generally receive payment for cars between 30 and 40 days after the car is shipped (or earlier when financing\n",
      "schemes are utilized by us or our dealers) while we generally pay most suppliers between 60 and 90 days after we receive the\n",
      "raw materials or components. Additionally, we also receive advance payments from our customers, mainly for our hypercars\n",
      "and limited edition cars (and starting in the first quarter of 2019, our Icona cars). We maintain sufficient inventory of raw\n",
      "materials and components to ensure continuity of our production lines, however delivery of most raw materials and\n",
      "components takes place monthly or more frequently in order to minimize inventories. The manufacture of one of our cars\n",
      "typically takes between 30 and 45 days, depending on the level of automation of the relevant production line, and the car is\n",
      "generally shipped to our dealers three to six days following the completion of production, although we may warehouse cars in\n",
      "local markets for longer periods of time to ensure prompt deliveries in certain regions. As a result of the above, including the\n",
      "advances received from customers in certain models, we tend to receive payment for cars shipped before we are required to\n",
      "make payment for the raw materials and components used in manufacturing the cars. Given the exceptional circumstances of\n",
      "the COVID-19 pandemic, we granted certain temporary, short-term support and payment extensions to the dealer network and\n",
      "other partners during the lockdown period, as well as early payments for commercial incentives due; however, our standard\n",
      "payment terms remain unchanged.\n",
      "22\n",
      "\n",
      "Our investments for capital expenditure and research and development are, among other factors, influenced by the\n",
      "timing and number of new models launches. Our development costs, as well as our other investments in capital expenditure,\n",
      "generally peak in periods when we develop a significant number of new models to renew or expand our product range. Our\n",
      "research and development costs are also influenced by the timing of research costs for our Formula 1 activities, for which\n",
      "expenditure in a normal season is generally higher in the first and last quarters of the year, and otherwise depends on the\n",
      "evolution of the applicable Formula 1 technical regulations, as well as the number and cadence of races during the course of\n",
      "the racing season. We significantly increased our capital expenditure in 2019 to support the growth of our product range and to\n",
      "expand our production facilities in Maranello, and we continued to make significant capital investments in the first nine\n",
      "months of 2020, including our acquisition of tracts of land adjacent to our facilities in Maranello as part of our expansion\n",
      "plans, despite certain actions taken to contain expenditures as a result of the COVID-19 pandemic.\n",
      "The payment of income taxes also affects our cash flows. We have historically paid our income taxes in two advances\n",
      "in the second and fourth quarters of the year, however, as a result of signing an agreement in September 2018 with the Italian\n",
      "Revenue Agency in relation to our application of the Patent Box tax regime for the years 2015 to 2019, our tax expense and\n",
      "therefore tax payments were significantly reduced in 2019 and we expect this to continue in 2020 as the Group is currently\n",
      "applying the Patent Box tax regime for the period from 2020 to 2024, in line with currently applicable tax regulations in Italy.\n",
      "In 2020 we paid the first installment in the second quarter of the year and the remaining portion will be paid in the fourth\n",
      "quarter of 2020. See Note 12 “Income Tax Expense” to the Interim Condensed Consolidated Financial Statements for\n",
      "additional details related to the Patent Box tax regime in Italy.\n",
      "23\n",
      "\n",
      "Cash Flows\n",
      "The following table summarizes the cash flows from/(used in) operating, investing and financing activities for the\n",
      "nine months ended September 30, 2020 and 2019. For additional details of our cash flows, see our Interim Condensed\n",
      "Consolidated Financial Statements included elsewhere in this Interim Report.\n",
      "For the nine months ended September 30,\n",
      "2020 2019\n",
      "(€ million)\n",
      "Cash and cash equivalents at beginning of the period 898 794\n",
      "Cash flows from operating activities 427 949\n",
      "Cash flows used in investing activities (464) (451)\n",
      "Cash flows from/(used) in financing activities 321 (423)\n",
      "Translation exchange differences (3) 2\n",
      "Total change in cash and cash equivalents 281 77\n",
      "Cash and cash equivalents at end of the period 1,179 871\n",
      "For the nine months ended September 30, 2020 the total change in cash and cash equivalents was €281 million\n",
      "compared to €77 million for the nine months ended September 30, 2019. The increase in cash generation of €204 million\n",
      "compared to the same period in the prior year was primarily attributable to cash proceeds of €640 million from the issuance of\n",
      "a bond in May 2020 and lower share repurchases of €173 million (€130 million in 2020 compared to €303 million for the same\n",
      "period in 2019) driven by our decision to temporarily suspend the share repurchase program in March 2020, partially offset by\n",
      "adverse impacts on our cash flows from operating activities as a result of the temporary suspension of production and\n",
      "deliveries for seven weeks during the first half of 2020 due to the COVID-19 pandemic, as well as lower net proceeds from\n",
      "our securitization programs. For the nine months ended September 30, 2019, our cash flows from operating activities were\n",
      "also impacted by the advances collected in the first nine months of 2019 in relation to the Ferrari Monza SP1 and SP2, ahead\n",
      "of shipments, including for cars actually delivered in the first nine months of 2020.\n",
      "Operating Activities - Nine Months Ended September 30, 2020\n",
      "Our cash flows from operating activities for the nine months ended September 30, 2020 were €427 million, primarily\n",
      "the result of:\n",
      "(i) profit before taxes of €427 million adjusted for €306 million for depreciation and amortization expense, €38 million\n",
      "of net finance costs and net other non-cash expenses of €41 million (including net gains on disposals of property,\n",
      "plant and equipment, provisions accrued and result from investments).\n",
      "These cash inflows were partially offset by:\n",
      "(i) €192 million related to cash absorbed from the net change in inventories, trade receivables and trade payables,\n",
      "consisting of cash absorbed by trade payables of €71 million, trade receivables of €65 million and inventories of €56\n",
      "million, driven by the build up of raw materials;\n",
      "(ii) €76 million of cash absorbed related to the net change in other operating assets and liabilities, primarily attributable to\n",
      "reversals of advances received for the Ferrari Monza SP1 and SP2, as well as early payments for commercial\n",
      "incentives due to our dealer network;\n",
      "(iii) €51 million related to cash absorbed from receivables from financing activities, primarily attributable to an increase\n",
      "in the financial receivables portfolio;\n",
      "(iv) €39 million of net finance costs paid; and\n",
      "(v) €27 million of income taxes paid.\n",
      "Operating Activities - Nine Months Ended September 30, 2019\n",
      "24\n",
      "\n",
      "Our cash flows from operating activities for the nine months ended September 30, 2019 were €949 million, primarily\n",
      "the result of:\n",
      "(i) profit before taxes of €666 million adjusted to add back €238 million for depreciation and amortization expense, €32\n",
      "million of net finance costs and €27 million of other non-cash expenses and income (including net gains on disposals\n",
      "of property, plant and equipment and intangible assets as well as non-cash result from investments) and €3 million in\n",
      "provisions accrued; and\n",
      "(ii) €188 million of cash related to the change in other operating assets and liabilities, primarily attributable to advances\n",
      "received for the Ferrari Monza SP1 and SP2.\n",
      "These cash inflows were partially offset by:\n",
      "(i) €96 million related to cash absorbed from the net change in inventories, trade receivables and trade payables, driven\n",
      "by cash absorbed by trade payables of €53 million, trade receivables of €36 million and inventories of €7 million,\n",
      "respectively;\n",
      "(ii) €58 million relating to cash absorbed from receivables from financing activities, primarily attributable to an increase\n",
      "in the financial receivables portfolio;\n",
      "(iii) €27 million of net finance costs paid; and\n",
      "(iv) €24 million of income taxes paid.\n",
      "Investing Activities - Nine Months Ended September 30, 2020\n",
      "For the nine months ended September 30, 2020 our net cash used in investing activities was €464 million, primarily\n",
      "the result of\n",
      "(i) €235 million for additions to intangible assets, mainly related to externally acquired and internally generated\n",
      "development costs, and (ii) €230 million of capital expenditures additions to property, plant and equipment, mainly\n",
      "related to plant and machinery for new models. These cash flows were partially offset by proceeds of €1 million from\n",
      "the disposal of property, plant and equipment. For a detailed analysis of additions to property, plant and equipment\n",
      "and intangible assets see “Capital Expenditures.”\n",
      "Investing Activities - Nine Months Ended September 30, 2019\n",
      "For the nine months ended September 30, 2019 our net cash used in investing activities was €451 million, primarily\n",
      "the result of\n",
      "(i) €238 million for additions to intangible assets, mainly related to externally acquired and internally generated\n",
      "development costs, and (ii) €215 million of capital expenditures additions to property, plant and equipment, mainly\n",
      "related to plant and machinery for new models. These cash flows were partially offset by proceeds of €2 million from\n",
      "the disposal of property, plant and equipment. For a detailed analysis of additions to property, plant and equipment\n",
      "and intangible assets see “Capital Expenditures.”\n",
      "Financing Activities - Nine Months Ended September 30, 2020\n",
      "For the nine months ended September 30, 2020, net cash from financing activities was €321 million, primarily the\n",
      "result of:\n",
      "(i) €640 million of proceeds from the issuance of a bond in May 2020;\n",
      "(ii) €34 million of proceeds net of repayments related to our revolving securitization programs in the U.S.; and\n",
      "25\n",
      "\n",
      "(iii) €3 million related to the net change in other debt.\n",
      "These cash inflows were partially offset by:\n",
      "(i) €211 million of dividends paid, of which €3 million was to non-controlling interests;\n",
      "(ii) €130 million paid to repurchase common shares under the Company’s share repurchase program;\n",
      "(iii) €14 million in repayments of lease liabilities; and\n",
      "(iv) €1 million related to the net change in bank borrowings.\n",
      "Financing Activities - Nine Months Ended September 30, 2019\n",
      "For the nine months ended September 30, 2019 net cash used in financing activities was €423 million, primarily the\n",
      "result of:\n",
      "(i) €315 million related to the cash tender offer to repurchase an aggregate nominal amount of €200 million of the 2021\n",
      "Bond and an aggregate nominal amount of €115 million of the 2023 Bond;\n",
      "(ii) €303 million paid to repurchase common shares under the Company’s share repurchase program;\n",
      "(iii) €195 million of dividends paid, of which €2 million was to non-controlling interests; and\n",
      "(iv) €4 million related to the net change in bank borrowings and lease liabilities.\n",
      "These cash outflows were partially offset by:\n",
      "(i) €298 million of net proceeds from the Company’s issuance of 1.12 percent senior notes due August 2029 and 1.27\n",
      "percent senior notes due August 2031, each having a principal of €150 million; and\n",
      "(ii) €89 million of proceeds net of repayments related to our revolving securitization programs in the U.S.;\n",
      "(iii) €7 million related to the net change in other debt.\n",
      "26\n",
      "\n",
      "Capital Expenditures\n",
      "Capital expenditures are defined as cash outflows that result in additions to property, plant and equipment (including\n",
      "right-of-use assets recognized in accordance with IFRS 16 - Leases) and intangible assets. Capital expenditures for the nine\n",
      "months ended September 30, 2020 and 2019 were €490 million and €453 million, respectively.\n",
      "The following table sets forth a breakdown of capital expenditures by category for each of the nine months ended\n",
      "September 30, 2020 and 2019:\n",
      "For the nine months ended September 30,\n",
      "2020 2019\n",
      "(€ million)\n",
      "Intangible assets\n",
      "Externally acquired and internally generated development costs 222 228\n",
      "Patents, concessions and licenses 8 8\n",
      "Other intangible assets 5 2\n",
      "Total intangible assets 235 238\n",
      "Property, plant and equipment\n",
      "Industrial buildings 24 11\n",
      "Plant, machinery and equipment 76 58\n",
      "Other assets 12 13\n",
      "Advances and assets under construction 143 133\n",
      "Total property, plant and equipment 255 215\n",
      "Total capital expenditures 490 453\n",
      "Intangible assets\n",
      "Our total capital expenditures in intangible assets were €235 million and €238 million for the nine months ended\n",
      "September 30, 2020 and 2019, respectively.\n",
      "The most significant investments relate to externally acquired and internally generated development costs. In\n",
      "particular, we make such investments to support the development of our current and future product offering. The capitalized\n",
      "development costs primarily include materials and personnel costs relating to engineering, design and development activities\n",
      "focused on content enhancement of existing cars and new models, including to broaden our product range and our ongoing\n",
      "investments in hybrid and electric technology, which are necessary to provide continuing performance upgrades to our sports\n",
      "car customers and to help us capture the preferences of the urban, affluent purchasers of GT cars whom we are increasingly\n",
      "targeting to transition our product portfolio to hybrid technology. We constantly invest in product development to ensure we\n",
      "can quickly and efficiently respond to market demand and/or technological breakthroughs and in order to maintain our\n",
      "position at the top of the luxury performance sports cars market.\n",
      "For the nine months ended September 30, 2020 we invested €222 million in externally acquired and internally\n",
      "generated development costs, of which €163 million primarily related to the development of models to be launched in future\n",
      "years and, to a much lesser extent, to investments required for new technical regulations applicable for the 2022 to 2025\n",
      "Formula 1 seasons, and €59 million related to the development of our current product portfolio and car components.\n",
      "For the nine months ended September 30, 2019 we invested €228 million in externally acquired and internally\n",
      "generated development costs, of which €122 million related to the development of models to be launched in future years and\n",
      "€106 million related to the development of models in our current product portfolio and car components.\n",
      "Property, plant and equipment\n",
      "Our total capital expenditures in property, plant and equipment were €255 million and €215 million for the nine\n",
      "months ended September 30, 2020 and 2019, respectively.\n",
      "27\n",
      "\n",
      "Our most significant investments generally relate to plant, machinery and equipment, which amounted to €76 million\n",
      "and €58 million for the nine months ended September 30, 2020 and 2019, respectively, as well as advances and assets under\n",
      "construction, which amounted to €143 million and €133 million for the nine months ended September 30, 2020 and 2019,\n",
      "respectively. Our main investments primarily related to industrial tools needed for the production of cars and investments in\n",
      "car production lines (including those for models to be launched in future years), as well as investments related to our\n",
      "personalization programs and engine assembly lines. The increase in advances and assets under construction reflects our focus\n",
      "on the hybridization and broadening of our product range and supporting future model launches, as well as our acquisition of\n",
      "tracts of land adjacent to our facilities in Maranello as part of our expansion plans.\n",
      "At September 30, 2020, the Group had contractual commitments for the purchase of property, plant and equipment\n",
      "amounting to €111 million (€105 million at December 31, 2019).\n",
      "Net Debt and Net Industrial Debt\n",
      "Due to different sources of cash flows used for the repayment of debt between industrial activities and financial\n",
      "services activities, and the different business structure and leverage implications, Net Industrial Debt, together with Net Debt,\n",
      "are the primary measures used by us to analyze our capital structure and financial leverage. We believe the presentation of Net\n",
      "Industrial Debt aids management and investors in their analysis of the Group’s financial position and financial performance\n",
      "and to compare the Group’s financial position and financial performance with that of other companies. Net Industrial Debt is\n",
      "defined as total debt less cash and cash equivalents (Net Debt), further adjusted to exclude the debt and cash and cash\n",
      "equivalents related to our financial services activities (Net Debt of Financial Services Activities).\n",
      "The following table sets forth a reconciliation of Net Debt and Net Industrial Debt at September 30, 2020 and\n",
      "December 31, 2019.\n",
      "At September 30, 2020 At December 31, 2019\n",
      "(€ million)\n",
      "Cash and cash equivalents 1,179 898\n",
      "Total liquidity 1,179 898\n",
      "Bonds and notes (1,829) (1,186)\n",
      "Asset-backed financing (Securitizations) (789) (788)\n",
      "Lease liabilities (68) (60)\n",
      "Borrowings from banks (30) (33)\n",
      "Other Debt (25) (23)\n",
      "Total Debt (2,741) (2,090)\n",
      "Net Debt (A) (1,562) (1,192)\n",
      "Net Debt of Financial Services Activities (B) (847) (855)\n",
      "Net Industrial Debt (A-B) (715) (337)\n",
      "In May 2020 the Company issued 1.5 percent coupon notes due May 2025 (“2025 Bond”), having a principal of\n",
      "€650 million. The notes were issued at a discount for an issue price of 98.898 percent, resulting in net proceeds of €640\n",
      "million after related expenses and a yield to maturity of 1.732 percent. The bond was admitted to trading on the regulated\n",
      "market of Euronext Dublin.\n",
      "For further details on total debt, see Note 23 “Debt” to the Interim Condensed Consolidated Financial Statements\n",
      "included elsewhere in this document.\n",
      "Cash and cash equivalents\n",
      "Cash and cash equivalents amounted to €1,179 million at September 30, 2020 compared to €898 million at December\n",
      "31, 2019.\n",
      "Approximately 88 percent of our cash and cash equivalents were denominated in Euro at September 30, 2020\n",
      "(approximately 77 percent at December 31, 2019). Our cash and cash equivalents denominated in currencies other than the\n",
      "Euro are available mostly to Ferrari S.p.A. and certain subsidiaries which operate in areas other than Europe. Cash held in such\n",
      "countries may be subject to transfer restrictions depending on the jurisdictions in which these subsidiaries operate. In\n",
      "28\n",
      "\n",
      "particular, cash held in China (including in foreign currencies), which amounted to €53 million at September 30, 2020 (€115\n",
      "million at December 31, 2019), is subject to certain repatriation restrictions and may only be repatriated as a repayment of\n",
      "payables, debt, dividends or capital distributions. We do not currently believe that such transfer restrictions have an adverse\n",
      "impact on our ability to meet our liquidity requirements.\n",
      "The following table sets forth an analysis of the currencies in which our cash and cash equivalents were denominated\n",
      "at the dates presented.\n",
      "At September 30, 2020 At December 31, 2019\n",
      "(€ million)\n",
      "Euro 1,034 690\n",
      "U.S. Dollar 68 63\n",
      "Chinese Yuan 48 110\n",
      "Japanese Yen 15 12\n",
      "Other currencies 14 23\n",
      "Total 1,179 898\n",
      "Cash collected from the settlement of receivables or credit lines pledged as collateral under securitization programs is\n",
      "subject to certain restrictions regarding its use and is primarily applied to repay principal and interest of the related funding.\n",
      "Such cash amounted to €39 million at September 30, 2020 (€28 million at December 31, 2019).\n",
      "Total available liquidity\n",
      "Total available liquidity (defined as cash and cash equivalents plus undrawn committed credit lines) at September 30,\n",
      "2020 increased to €1,879 million compared to €1,248 million at December 31, 2019.\n",
      "The following table summarizes our total available liquidity:\n",
      "At September 30, 2020 At December 31, 2019\n",
      "(€ million)\n",
      "Cash and cash equivalents 1,179 898\n",
      "Undrawn committed credit lines 700 350\n",
      "Total available liquidity 1,879 1,248\n",
      "The undrawn committed credit lines at December 31, 2019 relates to a revolving credit facility. In April 2020,\n",
      "additional committed credit lines of €350 million were secured, with tenors ranging from 18 to 24 months, therefore doubling\n",
      "our total committed credit lines available and undrawn. For further details, see Note 23 “Debt” in the Interim Condensed\n",
      "Consolidated Financial Statements included elsewhere in this document.\n",
      "29\n",
      "\n",
      "Free Cash Flow and Free Cash Flow from Industrial Activities\n",
      "Free Cash Flow and Free Cash Flow from Industrial Activities are two of our primary key performance indicators to\n",
      "measure the Group’s performance. These measures are presented by management to aid investors in their analysis of the\n",
      "Group’s financial performance and to compare the Group’s financial performance with that of other companies. Free Cash\n",
      "Flow is defined as cash flows from operating activities less investments in property, plant and equipment (excluding right-of-\n",
      "use assets recognized during the period in accordance with IFRS 16 - Leases) and intangible assets. Free Cash Flow from\n",
      "Industrial Activities is defined as Free Cash Flow adjusted to exclude the operating cash flow from our financial services\n",
      "activities (Free Cash Flow from Financial Services Activities). Prior to the first quarter of 2020, we defined Free Cash Flow\n",
      "and Free Cash Flow from Industrial Activities without excluding from investments in property, plant and equipment the right-\n",
      "of-use assets recognized during the period in accordance with IFRS 16 - Leases. Applying the current definition of Free Cash\n",
      "Flow and Free Cash Flow from Industrial Activities to the nine months ended September 30, 2019 would result in an\n",
      "immaterial difference compared to the figures presented below.\n",
      "The following table sets forth our Free Cash Flow and Free Cash Flow from Industrial Activities for the nine months\n",
      "ended September 30, 2020 and 2019.\n",
      "For the nine months ended September 30,\n",
      "2020 2019\n",
      "(€ million)\n",
      "Cash flows from operating activities 427 949\n",
      "Investments in property, plant and equipment and intangible assets (465) (453)\n",
      "Free Cash Flow (38) 496\n",
      "Free Cash Flow from Financial Services Activities (29) (63)\n",
      "Free Cash Flow from Industrial Activities (9) 559\n",
      "Free Cash Flow for the nine months ended September 30, 2020 was negative €38 million, a change of €534 million\n",
      "compared to positive €496 million for the nine months ended September 30, 2019.\n",
      "For an explanation of the drivers in Free Cash Flow see “Cash Flows” above.\n",
      "Free Cash Flow from Industrial Activities for the nine months ended September 30, 2020 was negative €9 million, a\n",
      "change of €568 million compared to positive €559 million for the nine months ended September 30, 2019. The change in Free\n",
      "Cash Flow from Industrial Activities was primarily driven by the impacts of the COVID-19 pandemic and higher inventories,\n",
      "driven by the build up of raw materials, as well as the effects of management actions to grant certain temporary, short-term\n",
      "support and payment extensions to the dealer network and other partners, and early payments for commercial incentives due.\n",
      "The prior year Free Cash Flow from Industrial Activities benefited from advances collected in the first nine months of 2019 in\n",
      "relation to the Ferrari Monza SP1 and SP2, ahead of shipments, including for cars actually delivered in the first nine months of\n",
      "2020.\n",
      "30\n",
      "\n",
      "Risk Factors\n",
      "We face a variety of risks and uncertainties in our business. For a description of such risks and uncertainties please\n",
      "see “Risk Factors” in the Group’s Annual Report and Form 20-F for the year ended December 31, 2019 filed with the AFM\n",
      "and the SEC on February 18, 2020, as well as the risk factor described below. All such risks factors should be read in\n",
      "conjunction with this Interim Report. Additional risks and uncertainties that we are unaware of, or that we currently believe\n",
      "to be immaterial, may also become important factors that affect us.\n",
      "We are subject to risks related to the evolution of and response to the coronavirus COVID-19 pandemic or similar public\n",
      "health crises that may materially and adversely affect our business\n",
      "Public health crises such as pandemics or similar outbreaks could adversely impact our business. The global spread\n",
      "of COVID-19, a virus causing potentially deadly respiratory tract infections, which was declared a global pandemic by the\n",
      "World Health Organization in March 2020, has led to governments around the world mandating increasingly restrictive\n",
      "measures to contain the pandemic, including social distancing, quarantine, “shelter in place” or similar orders, travel\n",
      "restrictions and suspension of non-essential business activities. The impact of COVID-19, including changes in consumer\n",
      "behavior, pandemic fears and market downturns, as well as restrictions on business and individual activities, has led to a\n",
      "global economic slowdown and a severe recession in several of the markets in which we operate, which may persist after the\n",
      "restrictions are lifted.\n",
      "Those measures, though temporary in nature, may continue for an extended period of time and intensify depending\n",
      "on developments in the COVID-19 pandemic, including potential subsequent waves of the outbreak. Beginning in mid-March\n",
      "2020, we suspended production at our plants in Maranello and Modena, while implementing remote working arrangements\n",
      "for all non-manufacturing related activities. Ferrari generally realizes minimal revenue while its facilities are shut down, but it\n",
      "continues to incur expenses. The negative cash impact is exacerbated by the fact that, despite not selling cars, Ferrari has to\n",
      "continue to pay suppliers for components previously ordered. In line with the Italian government’s plan to ease restrictions on\n",
      "business activities, implemented as a result of the COVID-19 pandemic, Ferrari gradually returned to full production at its\n",
      "Maranello and Modena plants on May 8, 2020. Ferrari continues to take measures to combat the spread of COVID-19 at its\n",
      "facilities, however in line with the laws and regulations enacted in Italy and other countries where the Company operates,\n",
      "Ferrari is continuing to guarantee the possibility of remote work for those employees whose job activity is compatible with\n",
      "such work arrangements.\n",
      "In connection with the COVID-19 outbreak and related government measures, we have experienced delays in\n",
      "shipments of cars due to restrictions on dealers’ activities or the inability of customers to take delivery of cars. Deliveries\n",
      "gradually restarted during May 2020, and substantially all Ferrari dealerships worldwide are open as of the date of this\n",
      "document. For further information on the impact of the COVID-19 pandemic on our results of operations and liquidity, see “-\n",
      "Results of Operations” and “-Liquidity and Capital Resources”.\n",
      "While production activities at our plants restarted in early May 2020, and although we have implemented several\n",
      "measures (including our “Back on Track” program) in an attempt to manage and mitigate the effects of the virus, we are\n",
      "unable to predict the ultimate impact from COVID-19. For example, we may yet experience a new shutdown or slowdown of\n",
      "all or part of our manufacturing facilities, including in the event our employees are diagnosed with COVID-19 or our supply\n",
      "chains are disrupted, or in the event new waves of disease lead to new government actions. The recent resurgence of the\n",
      "pandemic in several European countries, including Italy, as well as in the United States and elsewhere have led governments\n",
      "to reintroduce social distancing measures and curfews, and increasingly stringent measures may be imposed in the coming\n",
      "periods. Management time and resources may need to be spent on COVID-19-related matters, distracting from the\n",
      "implementation of the Group’s strategy. In addition, the prophylactic measures we are required to adopt at our facilities are\n",
      "costly and may affect production levels. Our suppliers, customers, dealers, franchisees and other contractual counterparties\n",
      "may be restricted or prevented from conducting business activities for indefinite or intermittent periods of time, including as a\n",
      "result of safety concerns, shutdowns, slowdowns, illness of such parties’ workforce and other actions and restrictions\n",
      "requested or mandated by governmental authorities. Furthermore, the COVID-19 pandemic may lead to financial distress for\n",
      "our suppliers or dealers, as a result of which they may have to permanently discontinue or substantially reduce their\n",
      "operations. In addition, the COVID-19 outbreak may lead to higher working capital needs, reduced liquidity and certain\n",
      "limitations in the supply of credit, which may ultimately lead to higher costs of capital for Ferrari. Any of the foregoing could\n",
      "limit customer demand or our capacity to meet customer demand and have a material adverse effect on our business, results\n",
      "from operations and financial condition.\n",
      "31\n",
      "\n",
      "Our brand activities across different jurisdictions have also been, and may continue to be, adversely impacted, due to\n",
      "the temporary closure of the Ferrari stores, museums and theme parks to comply with government orders, with an adverse\n",
      "impact on the Group’s revenues originating from such activities. Our stores and museums gradually started to reopen in May,\n",
      "with appropriate safety measures in place to protect our staff and customers; however, in-store traffic and museum visitors\n",
      "remain significantly lower than pre-pandemic levels. The Formula 1 2020 World Championship which was suspended in the\n",
      "first half of 2020 due to the COVID-19 outbreak resumed in July 2020 with a revised calendar. As of the date of this\n",
      "document, only 17 races have been scheduled for the 2020 Formula 1 season, and most of these races have been and will be\n",
      "held without spectators present, depending on local regulations and decisions made by FIA (the governing body of the\n",
      "Formula 1 World Championship). It is uncertain what the final number and format of races for the remainder of 2020 and\n",
      "beyond will be. The initial suspension of the World Championship, the reduced racing calendar and the fact that most races\n",
      "will be held without fans have had, and will continue to have an adverse effect on our sponsorship and commercial revenues\n",
      "from Formula 1 activities, as well as revenues from the rental of engines to other Formula 1 teams.\n",
      "The future impact of the COVID-19 pandemic on our results of operations and financial condition will depend on\n",
      "ongoing developments in the pandemic, including the success of containment measures and other actions taken by\n",
      "governments around the world, as well as the overall condition and outlook of the global economy. While we are continuing\n",
      "to monitor and assess the evolution of the pandemic and its effects on both the macroeconomic scenario and the Group’s\n",
      "financial position and results of operations, significant uncertainty remains around the length and extent of the restrictions in\n",
      "the markets in which we operate. However, the effects on our business, results of operations, financial performance and cash\n",
      "flows may be material and adverse.\n",
      "The COVID-19 pandemic may also exacerbate other risks disclosed in the “Risk Factors” section in the Group’s\n",
      "Annual Report and Form 20-F for the year ended December 31, 2019 filed with the AFM and the SEC on February 18, 2020,\n",
      "including, but not limited to, our competitiveness, demand for our products, shifting consumer preferences, exchange rate\n",
      "fluctuations, customers’ and dealers’ access to affordable financing, and credit market conditions affecting the availability of\n",
      "capital and financial resources.\n",
      "32\n",
      "\n",
      "Outlook\n",
      "2020 Guidance revised to the top end of the August 3rd guidance subject to trading conditions unaffected by further\n",
      "COVID-19 pandemic restrictions:\n",
      "AUGUST 3rd 2020\n",
      "(€ billion, unless otherwise stated) 2019 ACTUAL 2020 GUIDANCE\n",
      "GUIDANCE\n",
      "NET REVENUES 3.8 >3.4 >3.4\n",
      "1.27 1.075 - 1.125 ~1.125\n",
      "ADJUSTED EBITDA (margin %)\n",
      "33.7% 31% - 32.5% ~32.5%\n",
      "0.92 0.65 - 0.70 ~0.70\n",
      "ADJUSTED EBIT (margin %)\n",
      "24.4% 18.5% - 20% ~20%\n",
      "ADJUSTED DILUTED EPS (€) 3.71 2.6 - 2.8(1) ~2.8(1)\n",
      "INDUSTRIAL FREE CASH FLOW 0.7 0.10 - 0.15 ~0.15\n",
      "(1) Calculated using the weighted average diluted number of common shares as of June 30, 2020 (185,460 thousand)\n",
      "33\n",
      "\n",
      "FERRARI N.V.\n",
      "INTERIM CONDENSED CONSOLIDATED FINANCIAL STATEMENTS\n",
      "AT AND FOR THE THREE AND NINE MONTHS ENDED SEPTEMBER 30, 2020\n",
      "CONTENTS\n",
      "Page\n",
      "Interim Consolidated Income Statement F-1\n",
      "Interim Consolidated Statement of Comprehensive Income F-2\n",
      "Interim Consolidated Statement of Financial Position F-3\n",
      "Interim Consolidated Statement of Cash Flows F-4\n",
      "Interim Consolidated Statement of Changes in Equity F-5\n",
      "Notes to the Interim Condensed Consolidated Financial Statements F-6\n",
      "\n",
      "FERRARI N.V.\n",
      "INTERIM CONSOLIDATED INCOME STATEMENT\n",
      "for the three and nine months ended September 30, 2020 and 2019\n",
      "(Unaudited)\n",
      "For the three months ended For the nine months ended\n",
      "September 30, September 30,\n",
      "Note 2020 2019 2020 2019\n",
      "(€ thousand)\n",
      "Net revenues 6 887,970 915,314 2,390,980 2,838,974\n",
      "Cost of sales 7 425,577 425,457 1,175,629 1,367,449\n",
      "Selling, general and administrative costs 8 76,745 96,153 233,853 255,427\n",
      "Research and development costs 9 157,372 162,374 504,780 517,282\n",
      "Other expenses/(income), net 10 7,341 5,517 14,848 3,346\n",
      "Result from investments 1,096 1,011 3,554 2,401\n",
      "EBIT 222,031 226,824 465,424 697,871\n",
      "Net financial expenses 11 13,392 15,629 37,785 32,093\n",
      "Profit before taxes 208,639 211,195 427,639 665,778\n",
      "Income tax expense 12 37,451 42,240 81,251 133,156\n",
      "Net profit 171,188 168,955 346,388 532,622\n",
      "Net profit attributable to:\n",
      "Owners of the parent 170,750 167,851 345,697 528,246\n",
      "Non-controlling interests 438 1,104 691 4,376\n",
      "Basic earnings per common share (in €) 13 0.92 0.90 1.87 2.82\n",
      "Diluted earnings per common share (in €) 13 0.92 0.90 1.86 2.81\n",
      "The accompanying notes are an integral part of the Interim Condensed Consolidated Financial Statements.\n",
      "F-1\n",
      "\n",
      "FERRARI N.V.\n",
      "INTERIM CONSOLIDATED STATEMENT OF COMPREHENSIVE INCOME\n",
      "for the three and nine months ended September 30, 2020 and 2019\n",
      "(Unaudited)\n",
      "For the three months ended For the nine months ended\n",
      "September 30, September 30,\n",
      "Note 2020 2019 2020 2019\n",
      "(€ thousand)\n",
      "Net profit 171,188 168,955 346,388 532,622\n",
      "Items that may be reclassified to the consolidated income\n",
      "statement in subsequent periods:\n",
      "Gains/(Losses) on cash flow hedging instruments 20 12,109 (21,916) 34,142 (20,442)\n",
      "Exchange differences on translating foreign operations 20 (6,702) 7,210 (7,896) 8,061\n",
      "Related tax impact 20 (3,409) 6,108 (9,663) 5,680\n",
      "Total items that may be reclassified to the consolidated\n",
      "1,998 (8,598) 16,583 (6,701)\n",
      "income statement in subsequent periods\n",
      "Total other comprehensive income/(loss), net of tax 20 1,998 (8,598) 16,583 (6,701)\n",
      "Total comprehensive income 173,186 160,357 362,971 525,921\n",
      "Total comprehensive income attributable to:\n",
      "Owners of the parent 172,742 159,211 362,363 521,424\n",
      "Non-controlling interests 444 1,146 608 4,497\n",
      "The accompanying notes are an integral part of the Interim Condensed Consolidated Financial Statements.\n",
      "F-2\n",
      "\n",
      "FERRARI N.V.\n",
      "INTERIM CONSOLIDATED STATEMENT OF FINANCIAL POSITION\n",
      "at September 30, 2020 and at December 31, 2019\n",
      "(Unaudited)\n",
      "At September 30, At December 31,\n",
      "Note\n",
      "2020 2019\n",
      "(€ thousand)\n",
      "Assets\n",
      "Goodwill 785,182 785,182\n",
      "Intangible assets 14 923,345 837,938\n",
      "Property, plant and equipment 15 1,161,643 1,069,652\n",
      "Investments and other financial assets 16 40,721 38,716\n",
      "Deferred tax assets 54,846 73,683\n",
      "Total non-current assets 2,965,737 2,805,171\n",
      "Inventories 17 458,241 420,051\n",
      "Trade receivables 18 290,821 231,439\n",
      "Receivables from financing activities 18 967,837 966,448\n",
      "Current tax receivables 18 10,778 21,078\n",
      "Other current assets 18 94,880 92,830\n",
      "Current financial assets 19 37,123 11,409\n",
      "Cash and cash equivalents 1,178,616 897,946\n",
      "Total current assets 3,038,296 2,641,201\n",
      "Total assets 6,004,033 5,446,372\n",
      "Equity and liabilities\n",
      "Equity attributable to owners of the parent 1,517,346 1,481,290\n",
      "Non-controlling interests 3,677 5,998\n",
      "Total equity 20 1,521,023 1,487,288\n",
      "Employee benefits 54,533 88,116\n",
      "Provisions 22 157,240 165,572\n",
      "Deferred tax liabilities 97,378 82,208\n",
      "Debt 23 2,740,492 2,089,737\n",
      "Other liabilities 24 771,295 800,015\n",
      "Other financial liabilities 19 5,690 14,791\n",
      "Trade payables 25 639,978 711,539\n",
      "Current tax payables 16,404 7,106\n",
      "Total equity and liabilities 6,004,033 5,446,372\n",
      "The accompanying notes are an integral part of the Interim Condensed Consolidated Financial Statements.\n",
      "F-3\n",
      "\n",
      "FERRARI N.V.\n",
      "INTERIM CONSOLIDATED STATEMENT OF CASH FLOWS\n",
      "for the nine months ended September 30, 2020 and 2019\n",
      "(Unaudited)\n",
      "For the nine months ended\n",
      "September 30,\n",
      "2020 2019\n",
      "(€ thousand)\n",
      "Cash and cash equivalents at beginning of the period 897,946 793,664\n",
      "Cash flows from operating activities:\n",
      "Profit before taxes 427,639 665,778\n",
      "Amortization and depreciation 306,352 237,727\n",
      "Provision accruals 16,564 2,935\n",
      "Result from investments (3,554) (2,401)\n",
      "Net finance costs 37,785 32,093\n",
      "Other non-cash expenses, net 27,131 30,044\n",
      "Net gains on disposal of property, plant and equipment (30) (74)\n",
      "Change in inventories (56,326) (7,430)\n",
      "Change in trade receivables (64,873) (35,962)\n",
      "Change in trade payables (71,380) (53,273)\n",
      "Change in receivables from financing activities (50,867) (58,201)\n",
      "Change in other operating assets and liabilities (75,682) 188,460\n",
      "Finance income received 1,651 2,266\n",
      "Finance costs paid (40,426) (28,886)\n",
      "Income tax paid (26,829) (24,274)\n",
      "Total 427,155 948,802\n",
      "Cash flows used in investing activities:\n",
      "Investments in property, plant and equipment (229,992) (214,446)\n",
      "Investments in intangible assets (234,869) (238,456)\n",
      "Proceeds from the sale of property, plant and equipment 732 2,333\n",
      "Total (464,129) (450,569)\n",
      "Cash flows from/(used) in financing activities:\n",
      "Proceeds from bonds and notes 640,073 298,316\n",
      "Repayment of bonds — (315,395)\n",
      "Proceeds from securitizations net of repayments 34,225 89,385\n",
      "Net change in bank borrowings (1,740) (1,754)\n",
      "Net change in lease liabilities (14,158) (2,412)\n",
      "Net change in other debt 3,239 6,844\n",
      "Dividends paid to owners of the parent (208,131) (192,664)\n",
      "Dividends paid to non-controlling interest (2,929) (2,120)\n",
      "Share repurchases (129,793) (303,285)\n",
      "Total 320,786 (423,085)\n",
      "Translation exchange differences (3,142) 2,585\n",
      "Total change in cash and cash equivalents 280,670 77,733\n",
      "Cash and cash equivalents at end of the period 1,178,616 871,397\n",
      "The accompanying notes are an integral part of the Interim Condensed Consolidated Financial Statements.\n",
      "F-4\n",
      "\n",
      "FERRARI N.V.\n",
      "INTERIM CONSOLIDATED STATEMENT OF CHANGES IN EQUITY\n",
      "for the nine months ended September 30, 2020 and 2019\n",
      "(Unaudited)\n",
      "Retained Equity\n",
      "Cash flow Currency Remeasurement Non-\n",
      "Share earnings attributable\n",
      "hedge translation of defined controlling Total\n",
      "capital and other to owners of\n",
      "reserve differences benefit plans interests\n",
      "reserves the parent\n",
      "(€ thousand)\n",
      "At December 31, 2018 2,504 1,319,478 (2,992) 37,850 (8,118) 1,348,722 5,117 1,353,839\n",
      "Net profit — 528,246 — — — 528,246 4,376 532,622\n",
      "Other comprehensive (loss)/\n",
      "— — (14,762) 7,940 — (6,822) 121 (6,701)\n",
      "income\n",
      "Dividends — (193,238) — — — (193,238) (2,120) (195,358)\n",
      "Share-based compensation — 13,223 — — — 13,223 — 13,223\n",
      "Share repurchases — (303,285) — — — (303,285) — (303,285)\n",
      "Special voting shares\n",
      "issuance (1) 69 (69) — — — — — —\n",
      "At September 30, 2019 2,573 1,364,355 (17,754) 45,790 (8,118) 1,386,846 7,494 1,394,340\n",
      "Currency\n",
      "Retained Equity Non-\n",
      "Cash flow translatio Remeasuremen\n",
      "Share earnings attributable controllin\n",
      "hedge n t of defined Total\n",
      "capital and other to owners of g\n",
      "reserve difference benefit plans\n",
      "reserves the parent interests\n",
      "s\n",
      "(€ thousand)\n",
      "At December 31, 2019 2,573 1,452,720 (4,654) 40,391 (9,740) 1,481,290 5,998 1,487,288\n",
      "Net profit — 345,697 — — — 345,697 691 346,388\n",
      "Other comprehensive (loss)/\n",
      "— — 24,479 (7,813) — 16,666 (83) 16,583\n",
      "income\n",
      "Dividends — (208,765) — — — (208,765) (2,929) (211,694)\n",
      "Share-based compensation — 12,251 — — — 12,251 — 12,251\n",
      "Share repurchases — (129,793) — — — (129,793) — (129,793)\n",
      "At September 30, 2020 2,573 1,472,110 19,825 32,578 (9,740) 1,517,346 3,677 1,521,023\n",
      "___________________________________\n",
      "(1) Relates to the issuance and de-registration of certain special voting shares under the Company’s special voting shares terms and conditions.\n",
      "The accompanying notes are an integral part of the Interim Condensed Consolidated Financial Statements.\n",
      "F-5\n",
      "\n",
      "NOTES TO THE INTERIM CONDENSED CONSOLIDATED FINANCIAL STATEMENTS\n",
      "1. BACKGROUND AND BASIS OF PRESENTATION\n",
      "Background\n",
      "Ferrari is among the world’s leading luxury brands. The activities of Ferrari N.V. (herein referred to as “Ferrari” or\n",
      "the “Company” and together with its subsidiaries the “Group”) and its subsidiaries are focused on the design, engineering,\n",
      "production and sale of luxury performance sports cars. The cars are designed, engineered and produced in Maranello and\n",
      "Modena, Italy and sold in more than 60 markets worldwide through a network of 167 authorized dealers operating 187 points\n",
      "of sale. The Ferrari brand is licensed to a selected number of producers and retailers of luxury and lifestyle goods, with\n",
      "Ferrari branded merchandise also sold through a network of 18 Ferrari-owned stores and 17 franchised stores (including 13\n",
      "Ferrari Store Junior), as well as on the Group’s website. To facilitate the sale of new and pre-owned cars, the Group provides\n",
      "various forms of financing to clients and dealers, including through cooperations and other agreements. Ferrari also\n",
      "participates in the Formula 1 World Championship through Scuderia Ferrari. The activities of Scuderia Ferrari are a core\n",
      "element of Ferrari’s marketing and promotional activities and an important source of innovation supporting the technological\n",
      "advancement of Ferrari sport and street cars.\n",
      "2. AUTHORIZATION OF INTERIM CONDENSED CONSOLIDATED FINANCIAL STATEMENTS AND\n",
      "COMPLIANCE WITH INTERNATIONAL FINANCIAL REPORTING STANDARDS\n",
      "These Interim Condensed Consolidated Financial Statements of Ferrari N.V. were authorized for issuance on\n",
      "November 3, 2020, and have been prepared in accordance with IAS 34 - Interim Financial Reporting. The Interim Condensed\n",
      "Consolidated Financial Statements should be read in conjunction with the Group’s consolidated financial statements at and\n",
      "for the year ended December 31, 2019 (the “Consolidated Financial Statements”), which have been prepared in accordance\n",
      "with International Financial Reporting Standards (“IFRS”) as issued by the International Accounting Standards Board\n",
      "(“IASB”) and IFRS as endorsed by the European Union. There are no material effects on these Interim Condensed\n",
      "Consolidated Financial Statements resulting from differences between IFRS as issued by the IASB and IFRS as endorsed by\n",
      "the European Union. The designation IFRS also includes International Accounting Standards (“IAS”) as well as all the\n",
      "interpretations of the International Financial Reporting Interpretations Committee (“IFRIC” and “SIC”). The accounting\n",
      "policies adopted are consistent with those used at December 31, 2019, except as described in the section “New standards and\n",
      "amendments effective from January 1, 2020”.\n",
      "3. BASIS OF PREPARATION FOR INTERIM CONDENSED CONSOLIDATED FINANCIAL STATEMENTS\n",
      "The preparation of the Interim Condensed Consolidated Financial Statements requires management to make\n",
      "estimates and assumptions that affect the reported amounts of revenues, expenses, assets and liabilities as well as the\n",
      "disclosure of contingent liabilities. If in the future such estimates and assumptions, which are based on management’s best\n",
      "judgment at the date of these Interim Condensed Consolidated Financial Statements, deviate from the actual circumstances,\n",
      "the original estimates and assumptions will be modified as appropriate in the period in which the circumstances change.\n",
      "Reference should be made to the section “Use of estimates” in the Consolidated Financial Statements for a detailed\n",
      "description of the more significant valuation procedures used by the Group.\n",
      "Moreover, in accordance with IAS 34, certain valuation procedures, in particular those of a more complex nature\n",
      "regarding matters such as any impairment of non-current assets are only carried out in full during the preparation of the\n",
      "annual consolidated financial statements, when all the related information necessary is available, other than in the event that\n",
      "there are indications of impairment, in which case an immediate assessment is necessary. Similarly, the actuarial valuations\n",
      "that are required for the determination of employee benefit provisions are also usually carried out during the preparation of\n",
      "the annual consolidated financial statements, except in the event of significant market fluctuations or significant plan\n",
      "amendments, curtailments or settlements.\n",
      "F-6\n",
      "\n",
      "New standards and amendments effective from January 1, 2020\n",
      "The following new standards and amendments effective on or subsequent to January 1, 2020 have been adopted by\n",
      "the Group.\n",
      "Amendments to IFRS 3 - Business Combinations\n",
      "The Group adopted narrow scope amendments to IFRS 3 - Business Combinations. The amendments aim to help\n",
      "companies determine whether an acquisition made is of a business or a group of assets, emphasizing that the output of a\n",
      "business is to provide goods and services to customers, whereas the previous definition focused on returns in the form of\n",
      "dividends, lower costs or other economic benefits to investors and others. There was no effect from the adoption of these\n",
      "amendments.\n",
      "Amendments to IAS 1 - Presentation of Financial Statements and IAS 8 - Accounting Policies, Changes in Accounting\n",
      "Estimates and Errors\n",
      "The Group adopted amendments to IAS 1 - Presentation of Financial Statements and IAS 8 - Accounting Policies,\n",
      "Changes in Accounting Estimates and Errors. The amendments clarify the definition of ‘material’, as well as how materiality\n",
      "should be applied by including in the definition guidance that is included elsewhere in IFRS standards. There was no effect\n",
      "from the adoption of these amendments.\n",
      "Amendments to IFRS 9 - Financial Instruments, IAS 39 - Financial Instruments: Recognition and Measurement and IFRS 7 -\n",
      "Financial Instruments: Disclosures\n",
      "The Group adopted amendments to IFRS 9 - Financial Instruments, IAS 39 - Financial Instruments: Recognition\n",
      "and Measurement and IFRS 7 - Financial Instruments: Disclosures, collectively the “Interest Rate Benchmark Reform”.\n",
      "These amendments modify certain hedge accounting requirements in order to provide relief from potential effects of the\n",
      "uncertainty caused by the interbank offered rates (IBOR) reform and require companies to provide additional information to\n",
      "investors about their hedging relationships that are directly affected by these uncertainties. There was no effect from the\n",
      "adoption of these amendments.\n",
      "Review of the Conceptual Framework for Financial Reporting\n",
      "The Group adopted the changes envisaged by the review of the Conceptual Framework for Financial Reporting,\n",
      "which applies to companies that use the Conceptual Framework to develop accounting policies when no IFRS standard\n",
      "applies to a particular transaction. Key changes include (i) increasing the prominence of stewardship in the objective of\n",
      "financial reporting; (ii) reinstating prudence as a component of neutrality, defined as the exercise of caution when making\n",
      "judgements under conditions of uncertainty; (iii) defining a reporting entity; (iv) revising the definitions of an asset and a\n",
      "liability; (v) removing the probability threshold for recognition, and adding guidance on derecognition; (vi) adding guidance\n",
      "on the information provided by different measurement bases, and explaining factors to consider when selecting a\n",
      "measurement basis; and (vii) stating that profit or loss is the primary performance indicator and income and expenses in other\n",
      "comprehensive income should be recycled where the relevance or faithful representation of the financial statements would be\n",
      "enhanced. There was no immediate effect from adoption, however the Group will apply the changes to develop accounting\n",
      "policies when no IFRS standard applies to a particular transaction in the future.\n",
      "Amendment to IFRS 16 - Leases\n",
      "In May 2020 the IASB issued an amendment to IFRS 16 - Leases for COVID-19-related Rent Concessions. The\n",
      "amendment permits lessees, as a practical expedient, not to assess whether particular rent concessions occurring as a direct\n",
      "consequence of the COVID-19 pandemic are lease modifications and instead to account for those rent concessions as if they\n",
      "are not lease modifications. The Group adopted this amendment from its effective date of June 1, 2020 and there was no\n",
      "significant effect from the adoption of this amendment.\n",
      "F-7\n",
      "\n",
      "New standards, amendments and interpretations not yet effective\n",
      "The standards, amendments and interpretations issued by the International Accounting Standards Board (“IASB”)\n",
      "that will have mandatory application in 2021 or subsequent years are listed below:\n",
      "In May 2017 the IASB issued IFRS 17 - Insurance Contracts, which establishes principles for the recognition,\n",
      "measurement, presentation and disclosure of insurance contracts issued as well as guidance relating to reinsurance contracts\n",
      "held and investment contracts with discretionary participation features issued. In June 2020 the IASB issued amendments to\n",
      "IFRS 17 aimed at helping companies implement IFRS 17 and make it easier for companies to explain their financial\n",
      "performance. The new standard and amendments are effective on or after January 1, 2023.\n",
      "In January 2020 the IASB issued amendments to IAS 1 - Presentation of Financial Statements: Classification of\n",
      "Liabilities as Current or Non-Current to clarify how to classify debt and other liabilities as current or non-current, and in\n",
      "particular how to classify liabilities with an uncertain settlement rate and liabilities that may be settled by converting to\n",
      "equity. These amendments are effective on or after January 1, 2023. The Group does not expect any material impact from the\n",
      "adoption of these amendments.\n",
      "In May 2020 the IASB issued amendments to IFRS 3 - Business combinations to update a reference in IFRS 3 to the\n",
      "Conceptual Framework for Financial Reporting without changing the accounting requirements for business combinations.\n",
      "These amendments are effective on or after January 1, 2022. The Group does not expect any material impact from the\n",
      "adoption of these amendments.\n",
      "In May 2020 the IASB issued amendments to IAS 16 - Property, Plant and Equipment. The amendments prohibit a\n",
      "company from deducting from the cost of property, plant and equipment amounts received from selling items produced while\n",
      "the company is preparing the asset for its intended use. Instead, a company should recognize such sales proceeds and the\n",
      "related cost in the income statement. These amendments are effective on or after January 1, 2022. The Group does not expect\n",
      "any material impact from the adoption of these amendments.\n",
      "In May 2020 the IASB issued amendments to IAS 37 - Provisions, Contingent Liabilities and Contingent Assets,\n",
      "which specify which costs a company includes when assessing whether a contract will be loss-making. These amendments\n",
      "are effective on or after January 1, 2022. The Group does not expect any material impact from the adoption of these\n",
      "amendments.\n",
      "In May 2020 the IASB issued Annual Improvements to IFRSs 2018 - 2020 Cycle. The improvements have amended\n",
      "four standards with effective date January 1, 2022: i) IFRS 1 - First-time Adoption of International Financial Reporting\n",
      "Standards in relation to allowing a subsidiary to measure cumulative translation differences using amounts reported by its\n",
      "parent, ii) IFRS 9 - Financial Instruments in relation to which fees an entity includes when applying the ‘10 percent’ test for\n",
      "derecognition of financial liabilities, iii) IAS 41 - Agriculture in relation to the exclusion of taxation cash flows when\n",
      "measuring the fair value of a biological asset, and iv) IFRS 16 - Leases in relation to an illustrative example of reimbursement\n",
      "for leasehold improvements. The Group does not expect any material impact from the adoption of these amendments.\n",
      "In June 2020 the IASB issued amendments to IFRS 4 - Insurance Contracts which defer the expiry date of the\n",
      "temporary exemption from applying IFRS 9 to annual periods beginning on or after January 1, 2021. The Group does not\n",
      "expect any impact from the adoption of these amendments.\n",
      "In August 2020 the IASB issued a package of amendments to IFRS 9 – Financial Instruments, IAS 39 – Financial\n",
      "Instruments: Recognition and Measurement, IFRS 7 – Financial Instruments: Disclosures, IFRS 4 – Insurance Contracts and\n",
      "IFRS 16 – Leases in response to the ongoing reform of inter-bank offered rates (IBOR) and other interest rate benchmarks.\n",
      "The amendments are aimed at helping companies to provide investors with useful information about the effects of the reform\n",
      "on those companies’ financial statements. These amendments complement amendments issued in 2019 and focus on the\n",
      "effects on financial statements when a company replaces the old interest rate benchmark with an alternative benchmark rate\n",
      "as a result of the reform. The new amendments relate to:\n",
      "• changes to contractual cash flows – a company will not be required to derecognize or adjust the carrying amount of\n",
      "financial instruments for changes required by the interest rate benchmark reform, but will instead update the\n",
      "effective interest rate to reflect the change to the alternative benchmark rate;\n",
      "F-8\n",
      "\n",
      "• hedge accounting – a company will not have to discontinue its hedge accounting solely because it makes changes\n",
      "required by the interest rate benchmark reform if the hedge meets other hedge accounting criteria; and\n",
      "• disclosures – a company will be required to disclose information about new risks that arise from the interest rate\n",
      "benchmark reform and how the company manages the transition to alternative benchmark rates.\n",
      "These amendments are effective on or after 1 January 2021, with early adoption permitted.\n",
      "Scope of consolidation\n",
      "There were no changes in the scope of consolidation for the periods presented in these Interim Condensed\n",
      "Consolidated Financial Statements.\n",
      "4. FINANCIAL RISK FACTORS\n",
      "The Group is exposed to various operational financial risks, including financial market risk (relating mainly to\n",
      "foreign currency exchange rates and interest rates) credit risk and liquidity risk. The Interim Condensed Consolidated\n",
      "Financial Statements do not include all the information and notes on financial risk management required in the annual\n",
      "consolidated financial statements. For a detailed description of the financial risk factors and financial risk management of the\n",
      "Group, reference should be made to Note 30 of the Consolidated Financial Statements at and for the year ended December\n",
      "31, 2019.\n",
      "Although there was no significant impacts from the coronavirus COVID-19 (“COVID 19”) pandemic on the\n",
      "Group’s financial risks or risk management procedures in the periods presented by the these Interim Condensed Consolidated\n",
      "Financial Statements, management is continuously monitoring the evolution of COVID-19 as information becomes available\n",
      "and the related effects on the financial position and results of operations of the Group.\n",
      "To preventively and prudently manage potential liquidity or refinancing risks in the foreseeable future, the Group\n",
      "has increased its available liquidity, mainly through securing undrawn committed credit lines (an additional amount of €350\n",
      "million was secured in April 2020, doubling the total committed credit lines available and undrawn to €700 million) and the\n",
      "issuance of a bond for a principal amount of €650 million in May 2020. See Note 23 for additional details.\n",
      "5. OTHER INFORMATION\n",
      "The principal foreign currency exchange rates used to translate other currencies into Euro were as follows:\n",
      "2020 2019\n",
      "Average for the Average for the\n",
      "nine months ended At September 30, nine months ended At September 30, At December 31,\n",
      "September 30, September 30,\n",
      "U.S. Dollar 1.1250 1.1708 1.1236 1.0889 1.1234\n",
      "Pound Sterling 0.8851 0.9124 0.8835 0.8857 0.8508\n",
      "Swiss Franc 1.0680 1.0804 1.1179 1.0847 1.0854\n",
      "Japanese Yen 120.9108 123.7600 122.5696 117.5900 121.9400\n",
      "Chinese Yuan 7.8659 7.9720 7.7135 7.7784 7.8205\n",
      "Australian Dollar 1.6627 1.6438 1.6077 1.6126 1.5995\n",
      "Canadian Dollar 1.5218 1.5676 1.4935 1.4426 1.4598\n",
      "Singapore Dollar 1.5635 1.6035 1.5332 1.5060 1.5111\n",
      "Hong Kong Dollar 8.7273 9.0742 8.8074 8.5368 8.7473\n",
      "F-9\n",
      "\n",
      "6. NET REVENUES\n",
      "Net revenues are as follows:\n",
      "For the three months ended For the nine months ended\n",
      "September 30, September 30,\n",
      "2020 2019 2020 2019\n",
      "(€ thousand) (€ thousand)\n",
      "Revenues from:\n",
      "Cars and spare parts 726,229 707,845 1,964,704 2,209,262\n",
      "Engines 44,173 46,186 97,407 156,924\n",
      "Sponsorship, commercial and brand 93,402 135,121 265,395 393,934\n",
      "Other 24,166 26,162 63,474 78,854\n",
      "Total net revenues 887,970 915,314 2,390,980 2,838,974\n",
      "Other net revenues primarily relate to financial services activities and management of the Mugello racetrack and\n",
      "other sports-related activities.\n",
      "7. COST OF SALES\n",
      "Cost of sales for the three months ended September 30, 2020 and 2019 amounted to €425,577 thousand and\n",
      "€425,457 thousand, respectively, and for the nine months ended September 30, 2020 and 2019 amounted to €1,175,629\n",
      "thousand and €1,367,449 thousand, respectively, consisting mainly of the cost of materials, components and labor related to\n",
      "the manufacturing and distribution of cars and spare parts, engines sold to Maserati and engines rented to other Formula 1\n",
      "racing teams. The remaining costs principally include depreciation, insurance and transportation costs, as well as warranty\n",
      "and product-related costs, which are estimated and recorded at the time of shipment.\n",
      "Interest and other financial expenses from financial services activities included within cost of sales for the three\n",
      "months ended September 30, 2020 and 2019 amounted to €7,906 thousand and €11,056 thousand, respectively, and for the\n",
      "nine months ended September 30, 2020 and 2019 amounted to €29,514 thousand and €35,514 thousand, respectively.\n",
      "8. SELLING, GENERAL AND ADMINISTRATIVE COSTS\n",
      "Selling costs for the three months ended September 30, 2020 and 2019 amounted to €39,430 thousand and €53,046\n",
      "thousand, respectively, and for the nine months ended September 30, 2020 and 2019 amounted to €115,874 thousand and\n",
      "€134,220 thousand, respectively, consisting mainly of costs for sales personnel, marketing and events, and retail stores.\n",
      "Marketing and events expenses consist primarily of costs in connection with trade and auto shows, media and client events\n",
      "for the launch of new models, as well as sponsorship and indirect marketing costs incurred through the Formula 1 racing\n",
      "team, Scuderia Ferrari.\n",
      "General and administrative costs for the three months ended September 30, 2020 and 2019 amounted to €37,315\n",
      "thousand and €43,107 thousand, respectively, and for the nine months ended September 30, 2020 and 2019 amounted to\n",
      "€117,979 thousand and €121,207 thousand, respectively, consisting mainly of administrative and other general expenses,\n",
      "including for personnel, that are not directly attributable to manufacturing, sales or research and development activities.\n",
      "F-10\n",
      "\n",
      "9. RESEARCH AND DEVELOPMENT COSTS\n",
      "Research and development costs are as follows:\n",
      "For the three months ended For the nine months ended\n",
      "September 30, September 30,\n",
      "2020 2019 2020 2019\n",
      "(€ thousand)\n",
      "Research and development costs expensed during\n",
      "110,904 129,421 372,875 423,363\n",
      "the period\n",
      "Amortization of capitalized development costs 46,468 32,953 131,905 93,919\n",
      "Total research and development costs 157,372 162,374 504,780 517,282\n",
      "Research and development costs expensed during the period primarily relate to Formula 1 activities and research and\n",
      "development activities to support the innovation of our product range and components and, in particular, in relation to hybrid\n",
      "and electric technology.\n",
      "Research and development costs for the nine months ended September 30, 2020 are recognized net of technology\n",
      "incentives received in the first half of 2020.\n",
      "10. OTHER EXPENSES/(INCOME), NET\n",
      "Other expenses/(income), net for the three months ended September 30, 2020 is composed of other expenses of\n",
      "€8,400 thousand (€6,794 thousand for the three months ended September 30, 2019), mainly related to provisions, indirect\n",
      "taxes and other miscellaneous expenses, net of other income of €1,059 thousand (€1,277 thousand for the three months ended\n",
      "September 30, 2019).\n",
      "Other expenses/(income), net for the nine months ended September 30, 2020 is composed of other expenses of\n",
      "€18,008 thousand (€16,219 thousand for the nine months ended September 30, 2019) , mainly related to provisions, indirect\n",
      "taxes and other miscellaneous expenses, net of other income of €3,160 thousand (€12,873 thousand for the nine months\n",
      "ended September 30, 2019, mainly related to a change in estimate of the risk and related provision associated with a legal\n",
      "dispute based on developments that occurred in the first quarter of 2019, as well as other miscellaneous income.).\n",
      "F-11\n",
      "\n",
      "11. NET FINANCIAL EXPENSES\n",
      "For the three months ended For the nine months ended\n",
      "September 30, September 30,\n",
      "2020 2019 2020 2019\n",
      "(€ thousand)\n",
      "Financial income\n",
      "Related to:\n",
      "Industrial activities (A) 9,622 11,788 24,879 28,370\n",
      "Financial services activities (reported within net\n",
      "17,042 17,168 50,486 50,472\n",
      "revenues)\n",
      "Financial expenses and expenses from derivative\n",
      "financial instruments and foreign currency\n",
      "exchange rate differences\n",
      "Related to:\n",
      "Industrial activities (B) (23,014) (27,417) (62,664) (60,463)\n",
      "Financial services activities (reported within cost\n",
      "(7,906) (11,056) (29,514) (35,514)\n",
      "of sales)\n",
      "Net financial expenses relating to industrial\n",
      "(13,392) (15,629) (37,785) (32,093)\n",
      "activities (A - B)\n",
      "Net financial expenses primarily relate to net foreign exchange losses, including the net costs of hedging, as well as\n",
      "fair value gains and losses on financial assets and interest expenses on debt.\n",
      "12. INCOME TAX EXPENSE\n",
      "Income tax expense is as follows:\n",
      "For the three months ended For the nine months ended\n",
      "September 30, September 30,\n",
      "2020 2019 2020 2019\n",
      "(€ thousand)\n",
      "Current tax expense 47,644 29,592 58,198 97,687\n",
      "Deferred tax (benefit)/expense (10,142) 12,219 22,873 28,876\n",
      "Taxes relating to prior periods (51) 429 180 6,593\n",
      "Total income tax expense 37,451 42,240 81,251 133,156\n",
      "Income tax expense amounted to €37,451 thousand and €42,240 thousand for the three months ended September 30,\n",
      "2020 and 2019, respectively, and €81,251 thousand and €133,156 thousand for the nine months ended September 30, 2020\n",
      "and 2019, respectively. Income taxes for all periods benefited from the application of the Patent Box tax regime.\n",
      "In accordance with current tax legislation in Italy, Ferrari self-determines the income eligible for the Patent Box\n",
      "regime and will recognize the Patent Box tax benefit in three equal annual installments in 2020, 2021 and 2022. This resulted\n",
      "in an increase of current tax expense for the three months ended September 30, 2020 compared to the same prior year period,\n",
      "substantially offset by deferred tax assets recognized in relation to the Patent Box tax benefit for 2021 and 2022.\n",
      "Taxes relating to prior periods in 2019 are primarily attributable to the agreements reached with the Italian Revenue\n",
      "Agency for the settlement of previous years’ claims.\n",
      "The effective tax rate was 19.0 percent for the nine months ended September 30, 2020 compared to 20.0 percent for\n",
      "the nine months ended September 30, 2019. The decrease in income tax expense and effective tax rate for the nine months\n",
      "F-12\n",
      "\n",
      "ended September 30, 2020 was primarily attributable to a decrease in profit before taxes, to the effects of deductions for\n",
      "eligible research and development costs and the hyper and super-depreciation of fixed assets.\n",
      "IRAP (current and deferred) for the nine months ended September 30, 2020 and 2019 amounted to €10,559 thousand\n",
      "and €17,632 thousand, respectively. IRAP is only applicable to Italian entities and is calculated on a measure of income\n",
      "defined by the Italian Civil Code as the difference between operating revenues and costs, before financial income and\n",
      "expense, and in particular before the cost of fixed-term employees, credit losses and any interest included in lease payments.\n",
      "IRAP is calculated using financial information prepared under Italian accounting standards. IRAP is applied on the tax base\n",
      "at 3.9 percent for each of the nine months ended September 30, 2020 and 2019, respectively.\n",
      "Deferred tax assets and liabilities of the individual consolidated companies are offset within the interim consolidated\n",
      "statement of financial position when a legally enforceable right to offset exists.\n",
      "13. EARNINGS PER SHARE\n",
      "Basic earnings per share\n",
      "Basic earnings per share is calculated by dividing the profit attributable to equity holders of Ferrari by the weighted\n",
      "average number of common shares in issue and outstanding. The following table provides the amounts used in the calculation\n",
      "of basic earnings per share for the periods presented:\n",
      "For the three months ended For the nine months ended\n",
      "September 30, September 30,\n",
      "2020 2019 2020 2019\n",
      "Profit attributable to owners of the parent € thousand 170,750 167,851 345,697 528,246\n",
      "Weighted average number of common shares for\n",
      "thousand 184,748 186,504 184,825 187,196\n",
      "basic earnings per share\n",
      "Basic earnings per share € 0.92 0.90 1.87 2.82\n",
      "Diluted earnings per share\n",
      "For the three and nine months ended September 30, 2020 and 2019, the weighted average number of shares for\n",
      "diluted earnings per share was increased to take into consideration the theoretical effect of the potential common shares that\n",
      "would be issued for the Group’s equity incentive plans. See Note 21 for additional details on the equity incentive plans.\n",
      "The following table provides the amounts used in the calculation of diluted earnings per share for the three and nine\n",
      "months ended September 30, 2020 and 2019:\n",
      "For the three months ended For the nine months ended\n",
      "September 30, September 30,\n",
      "2020 2019 2020 2019\n",
      "Profit attributable to owners of the parent € thousand 170,750 167,851 345,697 528,246\n",
      "Weighted average number of common shares for\n",
      "thousand 185,344 187,302 185,422 187,994\n",
      "diluted earnings per share\n",
      "Diluted earnings per share € 0.92 0.90 1.86 2.81\n",
      "14. INTANGIBLE ASSETS\n",
      "At December 31, Translation At September 30,\n",
      "Additions Amortization\n",
      "2019 differences and other 2020\n",
      "(€ thousand)\n",
      "Intangible assets 837,938 234,869 (147,521) (1,941) 923,345\n",
      "F-13\n",
      "\n",
      "Additions of €234,869 thousand for the nine months ended September 30, 2020 primarily related to externally\n",
      "acquired and internally generated development costs for new and existing models.\n",
      "15. PROPERTY, PLANT AND EQUIPMENT\n",
      "Translation\n",
      "At December 31, At September 30,\n",
      "Additions Disposals Depreciation differences\n",
      "2019 2020\n",
      "and other\n",
      "(€ thousand)\n",
      "Property, plant and equipment 1,069,652 254,857 (3,058) (158,831) (977) 1,161,643\n",
      "of which right-of-use assets 57,765 24,865 (1,975) (15,167) (610) 64,878\n",
      "Additions of €254,857 thousand for the nine months ended September 30, 2020 were mainly comprised of additions\n",
      "to advances and assets under construction, as well as plant, machinery and equipment, primarily related to car production and\n",
      "engine assembly lines (including those for models to be launched in future years), industrial tools used for the production of\n",
      "cars, and our personalization programs, as well as our acquisition of tracts of land adjacent to our facilities in Maranello as\n",
      "part of our expansion plans.\n",
      "For the nine months ended September 30, 2020 depreciation of right-of-use assets amounted to €15,167 thousand\n",
      "and interest expense on lease liabilities amounted to €710 thousand (€12,647 thousand and €892 thousand respectively for the\n",
      "nine months ended September 30, 2019).\n",
      "At September 30, 2020 the Group had contractual commitments for the purchase of property, plant and equipment\n",
      "amounting to €110,840 thousand (€105,335 thousand at December 31, 2019).\n",
      "16. INVESTMENTS AND OTHER FINANCIAL ASSETS\n",
      "The composition of investments and other financial assets is as follows:\n",
      "At September 30, 2020 At December 31, 2019\n",
      "(€ thousand)\n",
      "Investments accounted for using the equity method 33,566 30,012\n",
      "Other securities and financial assets 7,155 8,704\n",
      "Total investments and other financial assets 40,721 38,716\n",
      "Investments accounted for using the equity method\n",
      "Investments accounted for using the equity method relate to the Group’s investment in FFS GmbH and changes\n",
      "were as follows:\n",
      "(€ thousand)\n",
      "Balance at January 1, 2020 30,012\n",
      "Proportionate share of net profit for the period from January 1, 2020 to September 30, 2020 3,554\n",
      "Balance at September 30, 2020 33,566\n",
      "Other securities and financial assets\n",
      "Other securities and financial assets primarily include Series C Liberty Formula One shares (the “Liberty Media\n",
      "Shares”) of Liberty Media Corporation, the group responsible for the promotion of the Formula 1 World Championship, The\n",
      "Liberty Media Shares are measured at fair value and amounted to €6,131 thousand at September 30, 2020 (€7,674 thousand at\n",
      "December 31, 2019).\n",
      "F-14\n",
      "\n",
      "17. INVENTORIES\n",
      "At September 30, 2020 At December 31, 2019\n",
      "(€ thousand)\n",
      "Raw materials 108,219 85,155\n",
      "Semi-finished goods 110,533 91,119\n",
      "Finished goods 239,489 243,777\n",
      "Total inventories 458,241 420,051\n",
      "The amount of inventory write-downs recognized as an expense within cost of sales was €16,013 thousand and\n",
      "€11,718 thousand for the nine months ended September 30, 2020 and 2019, respectively.\n",
      "18. CURRENT RECEIVABLES AND OTHER CURRENT ASSETS\n",
      "At September 30, 2020 At December 31, 2019\n",
      "(€ thousand)\n",
      "Receivables from financing activities 967,837 966,448\n",
      "Trade receivables 290,821 231,439\n",
      "Current tax receivables 10,778 21,078\n",
      "Other current assets 94,880 92,830\n",
      "Total 1,364,316 1,311,795\n",
      "Trade receivables at September 30, 2020 reflect certain temporary payment extensions granted as a reaction to the\n",
      "COVID-19 pandemic as well as certain sponsorship receivables due to the change in the calendar and format of the 2020\n",
      "Formula 1 World Championship.\n",
      "Receivables from financing activities\n",
      "Receivables from financing activities are as follows:\n",
      "At September 30, 2020 At December 31, 2019\n",
      "(€ thousand)\n",
      "Client financing 954,653 950,842\n",
      "Dealer financing 13,184 15,606\n",
      "Total 967,837 966,448\n",
      "Receivables from financing activities relate to the financial services portfolio in the United States and are generally\n",
      "secured on the title of cars or other guarantees.\n",
      "19. CURRENT FINANCIAL ASSETS AND OTHER FINANCIAL LIABILITIES\n",
      "At September 30, 2020 At December 31, 2019\n",
      "(€ thousand)\n",
      "Financial derivatives 34,042 9,423\n",
      "Other financial assets 3,081 1,986\n",
      "Current financial assets 37,123 11,409\n",
      "F-15\n",
      "\n",
      "The following table provides the analysis of derivative assets and liabilities at September 30, 2020 and December\n",
      "31, 2019.\n",
      "At September 30, 2020 At December 31, 2019\n",
      "Positive fair value Negative fair value Positive fair value Negative fair value\n",
      "(€ thousand)\n",
      "Cash flow hedge:\n",
      "Foreign currency derivatives 33,234 (5,093) 8,039 (14,547)\n",
      "Commodities 398 — — —\n",
      "Interest rate caps and other 50 — 87 —\n",
      "Total cash flow hedges 33,682 (5,093) 8,126 (14,547)\n",
      "Other foreign currency derivatives 360 (597) 1,297 (244)\n",
      "Derivatives assets/(liabilities) 34,042 (5,690) 9,423 (14,791)\n",
      "At September 30, 2020 and December 31, 2019, substantially all foreign currency derivatives had a maturity within\n",
      "twelve months.\n",
      "Foreign currency derivatives that do not meet the requirements to be recognized as cash flow hedges are presented\n",
      "as other foreign currency derivatives. Interest rate caps and other primarily relate to derivative instruments required as part of\n",
      "certain securitization agreements.\n",
      "20. EQUITY\n",
      "Share capital\n",
      "At September 30, 2020 the fully paid up share capital of the Company was €2,573 thousand, consisting of\n",
      "193,923,499 common shares and 63,349,112 special voting shares, all with a nominal value of €0.01 (€2,573 thousand at\n",
      "December 31, 2019 consisting of 193,923,499 common shares and 63,349,111 special voting shares, all with a nominal value\n",
      "of €0.01). At September 30, 2020, the Company held in treasury 9,175,609 common shares and 2,190 special voting shares,\n",
      "while at December 31, 2019 the Company held in treasury 8,640,176 common shares and 2,190 special voting shares. The\n",
      "increase in common shares held in treasury primarily reflects the repurchase of shares by the Company through its share\n",
      "repurchase program, partially offset by shares assigned under the Group’s equity incentive plans. On March 30, 2020 the\n",
      "Company elected to temporarily suspend its share repurchase program.\n",
      "The following table summarizes the changes in the number of outstanding common shares and outstanding special\n",
      "voting shares of the Company for the nine months ended September 30, 2020:\n",
      "Special Voting\n",
      "Common Shares Total\n",
      "Shares\n",
      "Balance at December 31, 2019 185,283,323 63,346,921 248,630,244\n",
      "Common shares repurchased under share repurchase program(1) (819,483) — (819,483)\n",
      "Common shares assigned under equity incentive plans(2) 284,050 — 284,050\n",
      "Other changes — 1 1\n",
      "Balance at September 30, 2020 184,747,890 63,346,922 248,094,812\n",
      "_______________________________________\n",
      "(1) Includes shares repurchased under the share repurchase program between January 1, 2020 and September 30, 2020 based on the transaction trade\n",
      "date, for a total consideration of €119,771 thousand, including transaction costs.\n",
      "(2) On March 16, 2020, 366,199 common shares, which were previously held in treasury, were assigned to participants of the equity incentive plans as a\n",
      "result of the vesting of certain performance share unit and retention restricted share unit awards. On March 17, 2020, the Company purchased 82,149\n",
      "common shares, for a total consideration of €10,022 thousand, from a group of those employees who were assigned shares in order to cover the\n",
      "individual’s taxable income as is standard practice (“Sell to Cover”) in an over-the-counter transaction. See Note 21 “Share-Based Compensation”\n",
      "for additional details relating to the Group’s equity incentive plans.\n",
      "F-16\n",
      "\n",
      "Other comprehensive income/(loss)\n",
      "The following table presents other comprehensive income/(loss):\n",
      "For the three months ended For the nine months\n",
      "September 30, ended September 30,\n",
      "2020 2019 2020 2019\n",
      "(€ thousand)\n",
      "Gains/(Losses) on cash flow hedging instruments\n",
      "16,528 (24,077) 33,098 (32,695)\n",
      "arising during the period\n",
      "(Gains)/Losses on cash flow hedging instruments\n",
      "(4,419) 2,161 1,044 12,253\n",
      "reclassified to the consolidated income statement\n",
      "Gains/(Losses) on cash flow hedging instruments 12,109 (21,916) 34,142 (20,442)\n",
      "Exchange differences on translating foreign\n",
      "(6,702) 7,210 (7,896) 8,061\n",
      "operations arising during the period\n",
      "Total items that may be reclassified to the\n",
      "consolidated income statement in subsequent 5,407 (14,706) 26,246 (12,381)\n",
      "periods\n",
      "Total other comprehensive income/(loss) 5,407 (14,706) 26,246 (12,381)\n",
      "Related tax impact (3,409) 6,108 (9,663) 5,680\n",
      "Total other comprehensive income/(loss), net of\n",
      "1,998 (8,598) 16,583 (6,701)\n",
      "tax\n",
      "Gains and losses on cash flow hedging instruments relate to changes in the fair value of derivative financial\n",
      "instruments used for cash flow hedging purposes.\n",
      "The tax effects relating to other comprehensive loss are as follows:\n",
      "For the nine months ended September 30,\n",
      "2020 2019\n",
      "Pre-tax Net Pre-tax Net\n",
      "Tax impact Tax impact\n",
      "balance balance balance balance\n",
      "(€ thousand)\n",
      "Gains/(Losses) on cash flow hedging instruments 34,142 (9,663) 24,479 (20,442) 5,680 (14,762)\n",
      "Exchange gains on translating foreign operations (7,896) — (7,896) 8,061 — 8,061\n",
      "Total other comprehensive income/(loss) 26,246 (9,663) 16,583 (12,381) 5,680 (6,701)\n",
      "21. SHARE-BASED COMPENSATION\n",
      "Equity Incentive Plan 2016-2020\n",
      "During the first quarter of 2020, 213,020 performance share units (“PSUs”) vested based on the achievement of the\n",
      "defined performance conditions for the period from 2016 to 2019 and 31,510 retention restricted share units (“RSUs”) vested\n",
      "based on the achievement of the related service conditions. As a result, 329,735 common shares, which were previously held\n",
      "in treasury, were assigned to participants of the plan. The number of shares assigned was greater than the number of awards\n",
      "that vested as a result of the Group’s level of achievement against the defined performance conditions. See Note 21 “Share-\n",
      "Based Compensation” to the Consolidated Financial Statements for further details relating to the Equity Incentive Plan\n",
      "2016-2020.\n",
      "Equity Incentive Plan 2019-2021\n",
      "During the first quarter of 2020, 17,572 PSUs vested based on the achievement of the defined performance\n",
      "conditions for 2019 and 18,892 RSUs vested based on the achievement of the related service conditions. As a result, 36,464\n",
      "F-17\n",
      "\n",
      "common shares, which were previously held in treasury, were assigned to participants of the plan. See Note 21 “Share-Based\n",
      "Compensation” to the Consolidated Financial Statements for further details relating to the Equity Incentive Plan 2019-2021.\n",
      "Equity Incentive Plan 2020-2022\n",
      "Under a new equity incentive plan approved in 2020, 60,089 PSUs and 47,513 RSUs, which each represent the right\n",
      "to receive one Ferrari common share, were awarded to the Executive Chairman, members of the Senior Management Team\n",
      "(“SMT”) and other key members of the Group (“Equity Incentive Plan 2020-2022”). The PSUs and RSUs cover a three-year\n",
      "performance period from 2020 to 2022.\n",
      "Equity Incentive Plan 2020-2022 - Performance Share Units (PSUs)\n",
      "The vesting of the PSUs is based on the achievement of defined key performance indicators relating to: i) a total\n",
      "shareholder return (“TSR”) ranking, ii) an EBITDA target, and iii) innovation targets, which will each be settled\n",
      "independently of the other targets. The total number of shares that will be assigned upon vesting of the PSUs will depend on\n",
      "the level of achievement of the targets. The PSUs vest in 2023.\n",
      "Of the total number of PSU awards, 50 percent vest based on the achievement of the TSR ranking of Ferrari\n",
      "compared to an industry specific peer group of eight, including the Company, (the “Peer Group”):\n",
      "Ferrari TSR Ranking Payout Ratio\n",
      "1 150%\n",
      "2 120%\n",
      "3 100%\n",
      "4 75%\n",
      "5 50%\n",
      ">5 0%\n",
      "The defined Peer Group is as follows:\n",
      "Ferrari Aston Martin Burberry Hermes\n",
      "Kering LVMH Moncler Richemont\n",
      "Of the total number of PSU awards, 30 percent vest based on the achievement of an EBITDA target determined by\n",
      "comparing Adjusted EBITDA to the Adjusted EBITDA targets derived from the Group’s business plan:\n",
      "Actual Adjusted EBITDA Compared to Business Plan Payout Ratio\n",
      "+10% 140%\n",
      "+5% 120%\n",
      "Business Plan Target 100%\n",
      "-5% 80%\n",
      "<-5% 0%\n",
      "Of the total number of PSU awards, 20 percent vest based on the achievement of defined objectives for\n",
      "technological innovation and the development of the new model pipeline over the performance period.\n",
      "The performance period for the PSUs commenced on January 1, 2020. The fair value of the awards used for\n",
      "accounting purposes was measured at the grant date using a Monte Carlo Simulation model. The fair value of the PSUs that\n",
      "F-18\n",
      "\n",
      "were awarded is €136.06 per share. The key assumptions utilized to calculate the grant-date fair values for these awards are\n",
      "summarized below:\n",
      "Key Assumptions\n",
      "Grant date share price €142.95\n",
      "Expected volatility 26.6%\n",
      "Dividend yield 0.8\n",
      "Risk-free rate 0%\n",
      "The expected volatility was based on the observed volatility of the Peer Group. The risk-free rate was based on the\n",
      "iBoxx sovereign Eurozone yield.\n",
      "At September 30, 2020 none of the PSU awards had vested or were forfeited.\n",
      "Equity Incentive Plan 2020-2022 - Retention Restricted Share Units (RSUs)\n",
      "The vesting of the RSUs is conditional on the recipients continued employment with the Company at the time of\n",
      "vesting. The RSUs vest in 2023. The fair value of the RSUs awarded is €139.39 per share.\n",
      "At September 30, 2020 none of the RSU awards had vested or were forfeited.\n",
      "Outstanding share awards\n",
      "For the nine months ended September 30, 2020, changes in the outstanding number of PSU and RSU share awards\n",
      "under the Group’s equity incentive plans are as follows:\n",
      "Outstanding PSU Awards Outstanding RSU Awards\n",
      "Balance at December 31, 2019 598,719 171,145\n",
      "Granted(1) 60,089 47,513\n",
      "Vested(2) (230,592) (50,402)\n",
      "Balance at September 30, 2020 428,216 168,256\n",
      "_____________________________________\n",
      "(1) Granted under the Equity Incentive Plan 2020-2022\n",
      "(2) Vested under the Equity Incentive Plan 2016-2020 and the Equity Incentive Plan 2019-2021\n",
      "Share-based compensation expense\n",
      "For the nine months ended September 30, 2020 and 2019, the Company recognized €12,251 thousand and €13,223\n",
      "thousand, respectively, as share-based compensation expense and an increase to other reserves in equity in relation to the\n",
      "PSU awards and RSU awards of the Group’s equity incentive plans. At September 30, 2020, unrecognized compensation\n",
      "expense amounted to €22,018 thousand and will be recognized over the remaining vesting periods through 2022, subject to\n",
      "achievement of the related performance conditions.\n",
      "F-19\n",
      "\n",
      "22. PROVISIONS\n",
      "Provisions are as follows:\n",
      "At September 30, 2020 At December 31, 2019\n",
      "(€ thousand)\n",
      "Warranty and recall campaigns provision 103,610 107,811\n",
      "Legal proceedings and disputes 28,677 27,097\n",
      "Other risks 24,953 30,664\n",
      "Total provisions 157,240 165,572\n",
      "The provision for other risks primarily relates to disputes and matters which are not subject to legal proceedings,\n",
      "including contract related disputes with suppliers, employees and other parties, as well as environmental risks.\n",
      "Movements in provisions are as follows:\n",
      "Balance at Additional Translation Balance at\n",
      "December 31, Utilization Releases differences and September 30,\n",
      "provisions\n",
      "2019 other 2020\n",
      "(€ thousand)\n",
      "Warranty and recall\n",
      "107,811 21,809 (22,175) (3,659) (176) 103,610\n",
      "campaigns provision\n",
      "Legal proceedings and\n",
      "27,097 4,917 (483) (2,680) (174) 28,677\n",
      "disputes\n",
      "Other risks 30,664 5,803 (1,593) (9,626) (295) 24,953\n",
      "Total provisions 165,572 32,529 (24,251) (15,965) (645) 157,240\n",
      "23. DEBT\n",
      "Interest\n",
      "Balance at Proceeds Repayments Balance at\n",
      "accrued/ Translation\n",
      "December from of September\n",
      "(paid) and differences\n",
      "31, 2019 borrowings borrowings other (*) 30, 2020\n",
      "(€ thousand)\n",
      "Bonds and notes 1,185,470 640,073 — 3,280 — 1,828,823\n",
      "Asset-backed financing (Securitizations) 788,269 205,133 (170,908) (516) (33,420) 788,558\n",
      "Leases liabilities 60,496 — (14,158) 22,509 (501) 68,346\n",
      "Borrowings from banks 32,946 — (1,740) (17) (1,264) 29,925\n",
      "Other debt 22,556 22,815 (19,576) — (955) 24,840\n",
      "Total debt 2,089,737 868,021 (206,382) 25,256 (36,140) 2,740,492\n",
      "_____________________________________\n",
      "(*) Other changes in lease liabilities primarily relates to non-cash movements for the recognition of additional lease liabilities in accordance with IFRS 16.\n",
      "Bonds and notes\n",
      "2023 Bond\n",
      "On March 16, 2016, the Company issued 1.5 percent coupon notes due March 2023, having a principal of €500\n",
      "million. The bond was issued at a discount for an issue price of 98.977 percent, resulting in net proceeds of €490,729\n",
      "thousand after the debt discount and issuance costs, and a yield to maturity of 1.656 percent. The net proceeds were used,\n",
      "together with additional cash held by the Company, to fully repay a €500 million bank loan. The bond is unrated and was\n",
      "admitted to trading on the regulated market of the Irish Stock Exchange. Following a cash tender offer, on July 16, 2019 the\n",
      "Company executed the repurchase of these notes for an aggregate nominal amount of €115,395 thousand. The amount\n",
      "F-20\n",
      "\n",
      "outstanding at September 30, 2020 of €385,136 thousand includes accrued interest of €3,130 thousand (€385,776 thousand\n",
      "including €4,567 thousand of accrued interest at December 31, 2019).\n",
      "2021 Bond\n",
      "On November 16, 2017, the Company issued 0.25 percent coupon notes due January 2021, having a principal of\n",
      "€700 million. The bond was issued at a discount for an issue price of 99.557 percent, resulting in net proceeds of €694,172\n",
      "thousand after the debt discount and issuance costs, and a yield to maturity of 0.391 percent. The net proceeds were primarily\n",
      "used to repay a €700 million bank loan. The bond is unrated and was admitted to trading on the regulated market of the Irish\n",
      "Stock Exchange. Following a cash tender offer, on July 16, 2019 the Company executed the repurchase of these notes for an\n",
      "aggregate nominal amount of €200,000 thousand. The amount outstanding at September 30, 2020 of €500,503 thousand\n",
      "includes accrued interest of €887 thousand (€499,824 thousand including €1,199 thousand of accrued interest at December\n",
      "31, 2019).\n",
      "2029 and 2031 Notes\n",
      "On July 31, 2019, the Company issued 1.12 percent senior notes due August 2029 (“2029 Notes”) and 1.27 percent\n",
      "senior notes due August 2031 (“2031 Notes”) through a private placement to certain US institutional investors, each having a\n",
      "principal of €150 million. The net proceeds from the issuances amounted to €298,316 thousand, and the yields to maturity, on\n",
      "an annual basis, equal the nominal coupon rates of the Notes. The Notes are primarily used for general corporate purposes,\n",
      "including the funding of capital expenditures.\n",
      "The amount outstanding of the 2029 Notes at September 30, 2020 was €149,531 thousand, including accrued\n",
      "interest of €280 thousand (€149,891 thousand including accrued interest of €700 thousand at December 31, 2019). The\n",
      "amount outstanding of the 2031 Notes at September 30, 2020 was €149,552 thousand, including accrued interest of €318\n",
      "thousand (€149,979 thousand including accrued interest of €794 thousand at December 31, 2019).\n",
      "2025 Bond\n",
      "On May 27, 2020 the Company issued 1.5 percent coupon notes due May 2025 (“2025 Bond”), having a principal of\n",
      "€650 million. The notes were issued at a discount for an issue price of 98.898 percent, resulting in net proceeds of €640,073\n",
      "thousand after related expenses, and a yield to maturity of 1.732 percent. The bond was admitted to trading on the regulated\n",
      "market of Euronext Dublin. The amount outstanding of the 2025 Bond at September 30, 2020 was €644,101 thousand,\n",
      "including accrued interest of €3,392 thousand.\n",
      "Asset-backed financing (Securitizations)\n",
      "As a means of diversifying its sources of funds, the Group sells certain of its receivables originated by its financial\n",
      "services activities in the US through asset-backed financing or securitization programs (the terms asset-backed financing and\n",
      "securitization programs are used synonymously throughout this document), without transferring the risks typically associated\n",
      "with such receivables. As a result, the receivables sold through securitization programs are still consolidated until collection\n",
      "from the customer. As of September 30, 2020, the following revolving securitization programs were in place:\n",
      "• revolving securitization program for funding of up to $625 million by pledging retail financial receivables in the\n",
      "United States as collateral. The notes bear interest at a rate per annum equal to the aggregate of LIBOR plus a\n",
      "margin of 65 basis points. As of September 30, 2020 total proceeds net of repayments from the sales of financial\n",
      "receivables under the program were $625 million ($547 million at December 31, 2019). The securitization\n",
      "agreement requires the maintenance of an interest rate cap. The program is subject to renewal in December 2020.\n",
      "• revolving securitization program for funding of up to $250 million by pledging leasing financial receivables in the\n",
      "United States as collateral. The notes bore interest at a rate per annum equal to the aggregate of LIBOR plus a\n",
      "margin of 65 basis points. As of September 30, 2020, total proceeds net of repayments from the sales of financial\n",
      "receivables under the program were $228 million ($238 million at December 31, 2019). In October 2020 the\n",
      "program was renewed for a tenor of 24 months and the funding limit increased to $275 million, with the notes\n",
      "bearing interest at a rate per annum equal to the aggregate of LIBOR plus a margin of 80 basis points. The\n",
      "securitization agreement requires the maintenance of an interest rate cap.\n",
      "F-21\n",
      "\n",
      "• revolving securitization program for funding of up to $110 million by pledging credit lines to Ferrari customers\n",
      "secured by personal vehicle collections and personal guarantees in the United States as collateral. The notes bear\n",
      "interest at a rate per annum equal to the aggregate of LIBOR plus a margin of 115 basis points. As of September 30,\n",
      "2020 total proceeds net of repayments from the sales of financial receivables under the program were $70 million\n",
      "($101 million at December 31, 2019). The program is subject to renewal in March 2021.\n",
      "The funding limits of the revolving securitization programs have been progressively increased since inception as the\n",
      "related receivables portfolios have increased.\n",
      "Cash collected from the settlement of receivables or credit lines pledged as collateral under securitization programs\n",
      "is subject to certain restrictions regarding its use and is primarily applied to repay principal and interest of the related funding.\n",
      "Such cash amounted to €38,505 thousand at September 30, 2020 (€27,524 thousand at December 31, 2019).\n",
      "Lease liabilities\n",
      "The Group recognizes lease liabilities in relation to right-of-use assets in accordance with IFRS 16 - Leases. At\n",
      "September 30, 2020 lease liabilities amounted to €68,346 thousand (€60,496 thousand at December 31, 2019).\n",
      "Borrowings from banks\n",
      "Borrowings from banks at September 30, 2020 relates to financial liabilities of FFS Inc to support financial services\n",
      "activities, and in particular €29,925 thousand (€31,211 thousand at December 31, 2019) relating to a U.S. Dollar denominated\n",
      "credit facility for up to $50 million (drawn down for $35 million at September 30, 2020) and bearing interest at LIBOR plus a\n",
      "range of between 60 and 65 basis points.\n",
      "Revolving credit facility and other committed credit lines\n",
      "In December 2019, the Company negotiated a €350 million unsecured committed revolving credit facility (the\n",
      "“RCF”), which is intended for general corporate and working capital purposes. The RCF has a 5 year-tenor with two further\n",
      "one-year extension options, exercisable on the first and second anniversary of the signing date on the Company’s request and\n",
      "the approval of each participating bank. In April 2020, additional committed credit lines of €350 million were secured with\n",
      "tenors ranging from 18 to 24 months, doubling total committed credit lines available to €700 million. At September 30, 2020\n",
      "all of the above mentioned committed credit facilities were undrawn and at December 31, 2019 the RCF was undrawn.\n",
      "Other debt\n",
      "Other debt mainly relates to funding for operating activities of the Group.\n",
      "F-22\n",
      "\n",
      "24. OTHER LIABILITIES\n",
      "An analysis of other liabilities is as follows:\n",
      "At September 30, 2020 At December 31, 2019\n",
      "(€ thousand)\n",
      "Deferred income 378,543 275,439\n",
      "Advances and security deposits 258,683 348,899\n",
      "Accrued expenses 47,359 85,965\n",
      "Payables to personnel 39,985 28,272\n",
      "Social security payables 19,535 20,334\n",
      "Other 27,190 41,106\n",
      "Total other liabilities 771,295 800,015\n",
      "Deferred income primarily includes amounts received under maintenance and power warranty programs of €213,565\n",
      "thousand at September 30, 2020 and €219,209 thousand at December 31, 2019, which are deferred and recognized as net\n",
      "revenues over the length of the maintenance program. Deferred income also includes amounts collected under various other\n",
      "agreements, which are dependent upon the future performance of a service or other act of the Group. The balance at\n",
      "September 30, 2020 reflects additional amounts relating to sponsorship agreements due to the change in the calendar and\n",
      "format of the 2020 Formula 1 World Championship.\n",
      "Advances and security deposits at September 30, 2020 and at December 31, 2019 primarily include advances\n",
      "received from clients for the purchase of our hypercars limited edition cars and Icona cars. Upon shipment of such cars, the\n",
      "advances are recognized as revenue. The decrease primarily relates to shipments of the Ferrari Monza SP1 and SP2.\n",
      "25. TRADE PAYABLES\n",
      "Trade payables of €639,978 thousand at September 30, 2020 (€711,539 thousand at December 31, 2019) are entirely\n",
      "due within one year. The carrying amount of trade payables is considered to be equivalent to their fair value.\n",
      "F-23\n",
      "\n",
      "26. FAIR VALUE MEASUREMENT\n",
      "IFRS 13 establishes a three level hierarchy for the inputs to the valuation techniques used to measure fair value by\n",
      "giving the highest priority to quoted prices (unadjusted) in active markets for identical assets and liabilities (level 1 inputs)\n",
      "and the lowest priority to unobservable inputs (level 3 inputs). In some cases, the inputs used to measure the fair value of an\n",
      "asset or a liability might be categorized within different levels of the fair value hierarchy. In those cases, the fair value\n",
      "measurement is categorized in its entirety in the same level of the fair value hierarchy at the lowest level input that is\n",
      "significant to the entire measurement.\n",
      "Levels used in the hierarchy are as follows:\n",
      "Level 1 inputs are quoted prices (unadjusted) in active markets for identical assets and liabilities that the Group can\n",
      "access at the measurement date.\n",
      "Level 2 inputs are inputs other than quoted prices included within level 1 that are observable for the assets or\n",
      "liabilities, either directly or indirectly.\n",
      "Level 3 inputs are unobservable inputs for the assets and liabilities.\n",
      "Assets and liabilities that are measured at fair value on a recurring basis\n",
      "The following table shows the fair value hierarchy for financial assets and liabilities that are measured at fair value\n",
      "on a recurring basis at September 30, 2020 and at December 31, 2019:\n",
      "At September 30, 2020\n",
      "Note Level 1 Level 2 Level 3 Total\n",
      "(€ thousand)\n",
      "Investments and other financial assets - Liberty Shares 16 6,131 — — 6,131\n",
      "Current financial assets 19 — 34,042 — 34,042\n",
      "Total assets 6,131 34,042 — 40,173\n",
      "Other financial liabilities 19 — 5,690 — 5,690\n",
      "Total liabilities — 5,690 — 5,690\n",
      "At December 31, 2019\n",
      "Note Level 1 Level 2 Level 3 Total\n",
      "(€ thousand)\n",
      "Investments and other financial assets - Liberty Shares 16 7,674 — — 7,674\n",
      "Current financial assets 19 — 9,423 — 9,423\n",
      "Total assets 7,674 9,423 — 17,097\n",
      "Other financial liabilities 19 — 14,791 — 14,791\n",
      "Total liabilities — 14,791 — 14,791\n",
      "There were no transfers between fair value hierarchy levels for the periods presented.\n",
      "The fair value of current financial assets and other financial liabilities relates to derivative financial instruments and\n",
      "is measured by taking into consideration market parameters at the balance sheet date, using widely accepted valuation\n",
      "techniques. In particular, the fair value of foreign currency derivatives (forward contracts, currency swaps and options) and\n",
      "interest rate caps is determined by taking the prevailing foreign currency exchange rates and interest rates, as applicable, at\n",
      "the balance sheet date.\n",
      "F-24\n",
      "\n",
      "The par value of cash and cash equivalents usually approximates fair value due to the short maturity of these\n",
      "instruments, which consist primarily of bank current accounts.\n",
      "Assets and liabilities not measured at fair value on a recurring basis\n",
      "For financial instruments represented by short-term receivables and payables, for which the present value of future\n",
      "cash flows does not differ significantly from carrying value, the Group assumes that carrying value is a reasonable\n",
      "approximation of the fair value. In particular, the carrying amount of current receivables and other current assets and of trade\n",
      "payables and other liabilities approximates their fair value.\n",
      "The following table represents carrying amount and fair value for the most relevant categories of financial assets and\n",
      "financial liabilities not measured at fair value on a recurring basis:\n",
      "At September 30, 2020 At December 31, 2019\n",
      "Carrying Fair Carrying Fair\n",
      "Note amount Value amount Value\n",
      "(€ thousand)\n",
      "Receivables from financing activities 18 967,837 967,837 966,448 966,448\n",
      "Total assets 967,837 967,837 966,448 966,448\n",
      "Debt 23 2,740,492 2,767,670 2,089,737 2,103,871\n",
      "Total liabilities 2,740,492 2,767,670 2,089,737 2,103,871\n",
      "27. RELATED PARTY TRANSACTIONS\n",
      "Pursuant to IAS 24, the related parties of the Group are entities and individuals capable of exercising control, joint\n",
      "control or significant influence over the Group and its subsidiaries, Fiat Chrysler Automobiles N.V. (“FCA”, and together\n",
      "with its subsidiaries the “FCA Group”), companies belonging to the FCA Group, Exor N.V. (“Exor”, and together with its\n",
      "subsidiaries, the “Exor Group”), companies belonging to the Exor Group, unconsolidated subsidiaries of the Group,\n",
      "associates and joint ventures. In addition, members of Ferrari Group Board of Directors, Board of Statutory Auditors and\n",
      "executives with strategic responsibilities and their families are also considered related parties.\n",
      "The Group carries out transactions with related parties on commercial terms that are normal in the respective\n",
      "markets, considering the characteristics of the goods or services involved. Transactions carried out by the Group with these\n",
      "related parties are primarily of a commercial nature and, in particular, these transactions relate to:\n",
      "Transactions with FCA Group companies\n",
      "• the sale of engines and car bodies to Maserati S.p.A. (“Maserati”) which is controlled by the FCA Group;\n",
      "• the purchase of engine components for the use in the production of Maserati engines from FCA US LLC, which\n",
      "is controlled by the FCA Group;\n",
      "• a technical cooperation, starting from November 2019, between the Group and FCA Group with the aim to\n",
      "enhance the quality and competitiveness of their respective products, while reducing costs and investments;\n",
      "• transactions with other FCA Group companies, mainly relating to the services provided by FCA Group\n",
      "companies, including human resources, payroll, tax, customs and procurement of insurance coverage and\n",
      "sponsorship revenues.\n",
      "Transactions with Exor Group companies\n",
      "• the Group incurs rental costs from Iveco Group companies related to the rental of trucks used by the Formula 1\n",
      "racing team;\n",
      "• the Group earns sponsorship revenue from Iveco S.p.A.\n",
      "F-25\n",
      "\n",
      "Transactions with other related parties\n",
      "• the purchase of components for Formula 1 racing cars from COXA S.p.A.;\n",
      "• consultancy services provided by HPE S.r.l.;\n",
      "• sponsorship agreement relating to Formula 1 activities with Ferretti S.p.A.;\n",
      "• sale of cars to certain members of the Board of Directors of Ferrari N.V. and Exor.\n",
      "In accordance with IAS 24, transactions with related parties also include compensation to Directors, the Audit\n",
      "Committee and managers with strategic responsibilities.\n",
      "The amounts of transactions with related parties recognized in the consolidated income statement are as follows:\n",
      "For the nine months ended September 30,\n",
      "2020 2019\n",
      "Net financial Net financial\n",
      "Net Costs (1) expenses/ Net Costs (1) expenses/\n",
      "revenues revenues\n",
      "(income) (income)\n",
      "(€ thousand)\n",
      "FCA Group companies\n",
      "Maserati 58,317 1,044 — 103,322 2,690 —\n",
      "FCA US LLC — 10,613 — — 12,518 —\n",
      "Magneti Marelli (2) — — — 352 10,444 —\n",
      "Other FCA Group companies 7,444 4,316 1,716 6,677 5,856 1,262\n",
      "Total FCA Group companies 65,761 15,973 1,716 110,351 31,508 1,262\n",
      "Exor Group companies (excluding the FCA Group) 169 1,183 1 212 303 (3)\n",
      "Other related parties 12 8,169 9 490 9,971 25\n",
      "Total transactions with related parties 65,942 25,325 1,726 111,053 41,782 1,284\n",
      "Total for the Group 2,390,980 1,424,330 37,785 2,838,974 1,626,222 32,093\n",
      "______________________________\n",
      "(1) Costs include cost of sales, selling, general and administrative costs and other expenses/(income), net.\n",
      "(2) FCA completed the sale of Magneti Marelli on May 2, 2019, following which Magneti Marelli (which subsequently operates under the name\n",
      "“Marelli”) is no longer a related party.\n",
      "Non-financial assets and liabilities originating from related party transactions are as follows:\n",
      "At September 30, 2020 At December 31, 2019\n",
      "Other Other\n",
      "Trade Trade Other Trade Trade Other\n",
      "current current\n",
      "receivables payables liabilities receivables payables liabilities\n",
      "assets assets\n",
      "(€ thousand)\n",
      "FCA Group companies\n",
      "Maserati 27,569 4,552 — 19,309 48,617 5,449 — 21,821\n",
      "FCA US LLC — 4,879 — — — 4,636 — —\n",
      "Other FCA Group companies 2,293 3,697 194 309 1,165 3,598 203 581\n",
      "Total FCA Group companies 29,862 13,128 194 19,618 49,782 13,683 203 22,402\n",
      "Exor Group companies (excluding the FCA\n",
      "169 373 134 143 350 9 237 207\n",
      "Group)\n",
      "Other related parties 104 2,238 1,934 2,213 147 2,565 1,295 1,835\n",
      "Total transactions with related parties 30,135 15,739 2,262 21,974 50,279 16,257 1,735 24,444\n",
      "Total for the Group 290,821 639,978 94,880 771,295 231,439 711,539 92,830 800,015\n",
      "F-26\n",
      "\n",
      "Current financial assets at September 30, 2020 included €1,699 thousand with the FCA Bank group (nil at December\n",
      "31, 2019). There were no other financial assets or financial liabilities originating from related party transactions at September\n",
      "30, 2020 or December 31, 2019.\n",
      "28. ENTITY-WIDE DISCLOSURES\n",
      "The following table presents an analysis of net revenues by geographic location of the Group’s customers for the\n",
      "three and nine months ended September 30, 2020 and 2019:\n",
      "For the three months ended For the nine months ended\n",
      "September 30, September 30,\n",
      "2020 2019 2020 2019\n",
      "(€ thousand)\n",
      "Italy 112,591 98,143 234,227 287,606\n",
      "Rest of EMEA 429,365 377,503 1,139,514 1,171,961\n",
      "Americas (1) 182,243 265,320 607,597 767,592\n",
      "Mainland China, Hong Kong and Taiwan 49,571 65,749 90,859 305,915\n",
      "Rest of APAC (2) 114,200 108,599 318,783 305,900\n",
      "Total net revenues 887,970 915,314 2,390,980 2,838,974\n",
      "______________________________\n",
      "(1) Americas includes the United States of America, Canada, Mexico, the Caribbean and Central and South America.\n",
      "(2) Rest of APAC mainly includes Japan, Australia, Singapore, Indonesia, South Korea, Thailand and Malaysia.\n",
      "The Group had an average number of employees of 4,410 and 4,130 for the nine months ended September 30, 2020\n",
      "and 2019, respectively, and 4,427 and 4,195 for the three months ended September 30, 2020 and 2019, respectively.\n",
      "Depreciation amounted to €158,831 thousand and €132,974 thousand for the nine months ended September 30, 2020\n",
      "and 2019, respectively, and €55,671 thousand and €46,302 thousand for the three months ended September 30, 2020 and\n",
      "2019, respectively.\n",
      "Amortization amounted to €147,521 thousand and €104,753 thousand for the nine months ended September 30,\n",
      "2020 and 2019, respectively, and €52,466 thousand and €37,266 thousand for the three months ended September 30, 2020\n",
      "and 2019, respectively.\n",
      "29. SUBSEQUENT EVENTS\n",
      "The Group evaluated subsequent events through November 3, 2020, which is the date the Interim Condensed\n",
      "Consolidated Financial Statements were authorized for issuance, and there were no events to report.\n",
      "F-27\n"
     ]
    }
   ],
   "source": [
    "filename = 'docs/ferrari_interim_report_at_and_for_the_three_and_nine_months_ended_september_30_2020.pdf'\n",
    "pdf = pdfplumber.open(filename)\n",
    "text = '\\n\\n'.join([page.extract_text() for page in pdf.pages])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41acfc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabula-py in /Users/nishithranjanbiswas/anaconda3/lib/python3.11/site-packages (2.9.0)\n",
      "Requirement already satisfied: pandas>=0.25.3 in /Users/nishithranjanbiswas/anaconda3/lib/python3.11/site-packages (from tabula-py) (2.2.0)\n",
      "Requirement already satisfied: numpy in /Users/nishithranjanbiswas/anaconda3/lib/python3.11/site-packages (from tabula-py) (1.26.4)\n",
      "Requirement already satisfied: distro in /Users/nishithranjanbiswas/anaconda3/lib/python3.11/site-packages (from tabula-py) (1.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/nishithranjanbiswas/anaconda3/lib/python3.11/site-packages (from pandas>=0.25.3->tabula-py) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/nishithranjanbiswas/anaconda3/lib/python3.11/site-packages (from pandas>=0.25.3->tabula-py) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/nishithranjanbiswas/anaconda3/lib/python3.11/site-packages (from pandas>=0.25.3->tabula-py) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/nishithranjanbiswas/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=0.25.3->tabula-py) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tabula-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91348cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error from tabula-java:\n",
      "The operation couldn’t be completed. Unable to locate a Java Runtime.\n",
      "Please visit http://www.java.com for information on installing Java.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['java', '-Djava.awt.headless=true', '-Dfile.encoding=UTF8', '-jar', '/Users/nishithranjanbiswas/anaconda3/lib/python3.11/site-packages/tabula/tabula-1.0.5-jar-with-dependencies.jar', '--pages', 'all', '--guess', '--format', 'JSON', 'docs/ferrari_interim_report_at_and_for_the_three_and_nine_months_ended_september_30_2020.pdf']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtabula\u001b[39;00m \n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m tables \u001b[38;5;241m=\u001b[39m tabula\u001b[38;5;241m.\u001b[39mread_pdf(filename, pages\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m, multiple_tables\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tabula/io.py:395\u001b[0m, in \u001b[0;36mread_pdf\u001b[0;34m(input_path, output_format, encoding, java_options, pandas_options, multiple_tables, user_agent, use_raw_url, pages, guess, area, relative_area, lattice, stream, password, silent, columns, relative_columns, format, batch, output_path, force_subprocess, options)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is empty. Check the file, or download it manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 395\u001b[0m     output \u001b[38;5;241m=\u001b[39m _run(\n\u001b[1;32m    396\u001b[0m         tabula_options,\n\u001b[1;32m    397\u001b[0m         java_options,\n\u001b[1;32m    398\u001b[0m         path,\n\u001b[1;32m    399\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m    400\u001b[0m         force_subprocess\u001b[38;5;241m=\u001b[39mforce_subprocess,\n\u001b[1;32m    401\u001b[0m     )\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m temporary:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tabula/io.py:82\u001b[0m, in \u001b[0;36m_run\u001b[0;34m(options, java_options, path, encoding, force_subprocess)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(java_options) \u001b[38;5;241m-\u001b[39m IGNORED_JAVA_OPTIONS:\n\u001b[1;32m     80\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjava_options is ignored until rebooting the Python process.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tabula_vm\u001b[38;5;241m.\u001b[39mcall_tabula_java(options, path)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tabula/backend.py:108\u001b[0m, in \u001b[0;36mSubprocessTabula.call_tabula_java\u001b[0;34m(self, options, path)\u001b[0m\n\u001b[1;32m    105\u001b[0m     args\u001b[38;5;241m.\u001b[39mappend(path)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 108\u001b[0m     result \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m    109\u001b[0m         args,\n\u001b[1;32m    110\u001b[0m         stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[1;32m    111\u001b[0m         stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[1;32m    112\u001b[0m         stdin\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mDEVNULL,\n\u001b[1;32m    113\u001b[0m         check\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    114\u001b[0m     )\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mstderr:\n\u001b[1;32m    116\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot stderr: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/subprocess.py:571\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[0;32m--> 571\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m    572\u001b[0m                                  output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr)\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process\u001b[38;5;241m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['java', '-Djava.awt.headless=true', '-Dfile.encoding=UTF8', '-jar', '/Users/nishithranjanbiswas/anaconda3/lib/python3.11/site-packages/tabula/tabula-1.0.5-jar-with-dependencies.jar', '--pages', 'all', '--guess', '--format', 'JSON', 'docs/ferrari_interim_report_at_and_for_the_three_and_nine_months_ended_september_30_2020.pdf']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "import tabula \n",
    "import pandas\n",
    "tables = tabula.read_pdf(filename, pages=\"all\", multiple_tables=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6212e42",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'table' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m table[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'table' is not defined"
     ]
    }
   ],
   "source": [
    "table[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de216ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472545a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
